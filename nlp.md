NLP
===
* [A curated list of speech and natural language processing resources](https://github.com/edobashira/speech-language-processing)
* [NLPK: 강승식 교수의 nlp 카페](http://cafe.daum.net/nlpk)
* [parserator - a framework for making parsers using natural language processing (NLP) methods](http://parserator.datamade.us/)
* [Keyword extraction in Java](http://www.vikasing.com/2013/09/keyword-extraction-in-java.html)
* [Extracting meaningful text from webpages](http://www.vikasing.com/2012/03/extracting-meaningful-text-from.html)
* [Extracting (meaningful) text from webpages - II](http://www.vikasing.com/2013/06/extracting-meaningful-text-from.html)
* [꼬꼬마 프로젝트!](http://kkma.snu.ac.kr/)
* [Free Term Extractors](https://termcoord.wordpress.com/about/testing-of-term-extraction-tools/free-term-extractors/)
* [‘시리’가 아직까지 말귀를 못 알아듣는 까닭](http://www.bloter.net/archives/227915)
* [Heteronym (linguistics)](https://en.wikipedia.org/wiki/Heteronym_(linguistics))
* [Pronounceable Anagrams](http://smithamilli.com/blog/anagrams/)
* [ROC Curve, AUC](http://digndig.net/blog/2013/06/01/312/)
* [Tf-idf 가중치](http://jeongsw.tistory.com/449)
* [입 개발자를 위한 TF-IDF](https://charsyam.wordpress.com/2017/04/08/%EC%9A%A9%EC%96%B4-%EC%A0%95%EB%A6%AC-%EC%9E%85-%EA%B0%9C%EB%B0%9C%EC%9E%90%EB%A5%BC-%EC%9C%84%ED%95%9C-tf-idf/)
* [What is TF-IDF? The 10 minute guide](http://michaelerasm.us/tf-idf-in-10-minutes/)
* [Part 1: For Beginners - Bag of Words 캐글뽀개기 6월 이상열](http://nbviewer.ipython.org/gist/syleeie2310/d720330f793203829e47)
* [Writers Choose Their Favorite Words](http://www.newyorker.com/culture/cultural-comment/writers-choose-their-favorite-words/) 쓰이는 단어의 종류를 통해 글 쓴 사람 예측?
* [Algorithms for text fingerprinting?](https://news.ycombinator.com/item?id=9716837)
* [하나의 차트로 이해하는 민주당과 공화당이 세계를 보는 다른 시각](http://newspeppermint.com/2015/06/15/worldview/)
* [Ask HN: What are the best tools for analyzing large bodies of text?](https://news.ycombinator.com/item?id=9733883)
* [Special Section: Reconceiving Text Analytics](http://dho.ie/sites/default/files/Toward_an_Algorithmic_Criticism.pdf)
* [ExoBrain](http://exobrain.kr/)
  * [인간-기계 지식소통을 위한 자연어 QA 워크샵 – 엑소브레인 인공지능](http://143.248.55.96/workshop/)
* [한자로](http://hanjaro.juntong.or.kr/)
* [Making Apps Understand Natural Language](http://yahoolabs.tumblr.com/post/123387824121/making-apps-understand-natural-language)
* [Automatically spotting interesting sentences in parliamentary debates](https://fullfact.org/blog/getting_closer_automated_factchecking)
* [Tone Analyzer](https://tone-analyzer-demo.mybluemix.net/)
* [Bag of Words Meet Bags of Popcorn - (1) Part 1: Bag of Words](http://khanrc.tistory.com/entry/kaggle-Bag-of-Words-Meet-Bags-of-Popcorn-1-Part-1)
* [Kaggle Solution: What’s Cooking ? (Text Mining Competition)](http://www.analyticsvidhya.com/blog/2015/12/kaggle-solution-cooking-text-mining-competition)
* [WHERE TECHNOLOGY MEETS BUSINESS. TYING TEXT ANALYTICS TO YOUR BUSINESS GOALS](http://www.incite-group.com/events/textwest/conference-agenda.php)
* [For 40 years, computer scientists looked for a solution that doesn’t exist](http://www.bostonglobe.com/ideas/2015/08/10/computer-scientists-have-looked-for-solution-that-doesn-exist/tXO0qNRnbKrClfUPmavifK/story.html) edit distance
* [CS224d: Deep Learning for Natural Language Processing](http://cs224d.stanford.edu/)
  * [DSBA CS224d](https://www.youtube.com/playlist?list=PLetSlH8YjIfUf3gBv1JpCLa9WVaL8yE0-)
* [CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/cs224n/)
  * [cs224n-winter17-notes](https://github.com/stanfordnlp/cs224n-winter17-notes)
  * [CS 224N: TensorFlow Tutorial](http://web.stanford.edu/class/cs224n/lectures/cs224n-2017-tensorflow.pdf)
  * [Lecture Collection | Natural Language Processing with Deep Learning (Winter 2017)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6)
* [CS224U: Natural Language Understanding](http://web.stanford.edu/class/cs224u/) 
  * [Distributional word representations](http://nbviewer.jupyter.org/github/cgpotts/cs224u/blob/master/vsm.ipynb)
* [Deep Learning for NLP](https://www.comp.nus.edu.sg/~kanmy/courses/6101_2016_2/)
* [Deep Learning for Natural Language Processing: 2016-2017](http://www.cs.ox.ac.uk/teaching/courses/2016-2017/dl/)
  * [Oxford Deep NLP 2017 course](https://github.com/oxford-cs-deepnlp-2017/lectures)
  * [Lecture 8 - Generating Language with Attention Chris Dyer](https://www.youtube.com/watch?v=ah7_mfl7LD0)
* [DAWG data structure in Word Judge](http://porcupineprogrammer.blogspot.kr/2012/03/dawg-data-structure-in-word-judge.html)
* [A Simple Artificial Intelligence Capable of Basic Reading Comprehension](http://blog.ayoungprogrammer.com/2015/09/a-simple-artificial-intelligence.html)
* [Extracting Structured Data From Recipes Using Conditional Random Fields](http://open.blogs.nytimes.com/2015/04/09/extracting-structured-data-from-recipes-using-conditional-random-fields/?_r=1)
* [The future of programmers](http://tcz.hu/the-future-of-programmers)
* [IBM ‘왓슨’, 인지컴퓨팅 서비스로 업그레이드](http://www.bloter.net/archives/239630)
* [Semantics, Representations and Grammars for Deep Learning](http://arxiv.org/abs/1509.08627)
* [A Primer on Neural Network Models for Natural Language Processing](http://u.cs.biu.ac.il/~yogo/nnlp.pdf)
* [politeness - Write in a more polite, friendly tone](https://labs.foxtype.com/politeness)
* [Understanding Natural Language with Deep Neural Networks Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch)
* [An Inside View of Language Technologies at Google](http://breakthroughanalysis.com/2015/10/28/an-inside-view-of-language-technologies-at-google/)
* [Google Cloud에서 Natural Language API 정리](https://jungwoon.github.io/google%20cloud/2017/11/13/Google-Natural-Language/)
* [Google Cloud 서비스 계정키 얻기 및 GCS 공유하기](https://jungwoon.github.io/google%20cloud/2017/11/17/Get-Service-Account-Key/)
* [Understanding Convolutional Neural Networks for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)
  * [자연어 처리 문제를 해결하는 CONVOLUTIONAL NEURAL NETWORKS 이해하기](http://blog.naver.com/rupy400/220776488979)
* **[Convolutional Methods for Text](https://medium.com/@TalPerry/convolutional-methods-for-text-d5260fd5675f)**
  * 텍스트 처리와 관련해서는 LSTM/GRU를 비롯한 RNNs 가 대세지만 CNN도 장점이 있는데 이를 잘 정리한 글
  * RNN이 순서에 영향을 받지만 CNN은 단어의 의미에 영향을 주는 데에 있어 조금 멀리 떨어져 있는 문장에서의 단어 등이 역할을 할 수 있음
  * 전체를 한꺼번에 보게 하는 데에는 더 유리
  * NLP 전반에 대한 이해와 DNN 종류들의 장단점 등도 잘 파악할 수 있는 매우 좋은 글
* [Convolutional Sequence-to-Sequence Learning (2017)](https://github.com/j-min/conv_s2s/blob/master/overview.ipynb)
  * [Convolutional Sequence-to-Sequence Learning (2017)](https://nbviewer.jupyter.org/github/j-min/conv_s2s/blob/master/overview.ipynb)
  * (NLP 처음 접하시는 분들을 위한)
  1. RNN enc-dec 부터 conv seq2seq 까지 간단한 흐름 정리
  2. conv s2s 이해를 위해 읽어야 할 논문 10+ 편
* [Learning Deep Structured Semantic Models for Web Search using Clickthrough Data](http://research.microsoft.com/pubs/198202/cikm2013_DSSM_fullversion.pdf)
* [시나브로 배우는 자연어처리](http://www.slideshare.net/shuraba1/ss-56479835)
  * **[시나브로 배우는 자연어처리 실습자료](http://nbviewer.ipython.org/github/babelPish/nlp/blob/master/part5/studybreak_zip/babel_zip.ipynb)**
* [collocations.de - Association Measures](http://collocations.de/AM/index.html)
* [Perpelxity](https://en.wikipedia.org/wiki/Perplexity)
  * [Perplexity in LM](http://hexists.tistory.com/215)
  * [Lecture 4: Evaluating language models](https://courses.engr.illinois.edu/cs498jh/Slides/Lecture04.pdf)
  * [speech recognition & LM](http://phonolog.tistory.com/entry/speech-recognition-LM)
  * [하이퍼망 분자컴퓨팅 기반 단어 재인 시뮬레이션](https://bi.snu.ac.kr/Publications/Theses/MS06f_KangYJ.pdf)
* [웹용 KorpuSQL 실행기](http://lab.bab2min.pe.kr/KorpuSQLWeb)
* [An Experimental Study on Open Source Korean Morphological Analyzers for Evaluating Noun Extraction](http://www.dbpia.co.kr/Journal/ArticleDetail/NODE06559147)
* [Episode 22: 자연언어처리 특집 1부 – 마이크로소프트 NLP연구실의 김용범님과 함께](https://iamprogrammer.io/post/9401)
* [Espresso - AIR LAB, Changwon National University](http://air.changwon.ac.kr/~airdemo/Espresso/)
* [악평생성기 (Bad Comment Generator using RNN) _ 송치성](http://www.slideshare.net/shuraba1/bad-comment-generator-using-rnn)
  * [Bad Comment Generator using RNN](http://nbviewer.jupyter.org/github/daydrill/BadCmtGenerator/blob/master/bad_cmt_generator_code.ipynb)
* [5—INTRO TO NLP AND RNNS](http://course.fast.ai/lessons/lesson5.html)
* [딥엘라스틱 - 검색 + 로봇 저널리즘 + 인지신경언어학 + 딥러닝NLP](http://babelpish.github.io/deep-elastic/)
* [PHP + MySQL 언어 식별기(Language Detection) 개발기](http://bab2min.tistory.com/503)
  * [언어 식별기 (Language Detection)](http://lab.bab2min.pe.kr/detectLang)
* [word-rnn - a fork of Andrej Karpathy's wonderful char-rnn](https://github.com/larspars/word-rnn)
* [컴퓨터가 소설을 써요](http://jamonglab.com/2015/11/11/computer-writer/)
* [Next Word Auto-Completion](https://kyucho.shinyapps.io/nextword/)
* [2015 자연어처리 및 정보검색 워크샵](https://sites.google.com/site/sighclt/haengsasogae/jayeon-eocheoli-mich-jeongbogeomsaeg-wokeusyab-1/jayeon-eocheoli-mich-jeongbogeomsaeg-wokeusyab)
* [“네이버에서 만나보셨나요? 인공지능 채팅 로봇”](http://www.bloter.net/archives/256278)
* [Introducing DeepText: Facebook's text understanding engine](https://code.facebook.com/posts/181565595577955/introducing-deeptext-facebook-s-text-understanding-engine/)
  * [페이스북, ‘사람 수준으로’ 내용을 이해하는 딥텍스트 A.I. 공개](http://www.itworld.co.kr/news/99613#csidxc8e244e28d7c435a8c8b8bbefd32f3e)
* [NLP 자연어처리](http://hub-ai.com/nlp)
* [니코니코동화의 공개코멘트 데이터를 Deep Learning로 해석하기](https://blog.umay.be/2016/06/02/niconico-nlp.html)
  * [わかるLSTM ～ 最近の動向と共に](http://qiita.com/t_Signull/items/21b82be280b46f467d1b)
* [Language Understanding for Text-based Games Using Deep Reinforcement Learning](http://arxiv.org/pdf/1506.08941v1.pdf)
* [Generative Models](https://openai.com/blog/generative-models)
* [온라인 한국어 POS 태거 만들기](https://www.youtube.com/watch?v=oSPVB9o6D50)
* [파이썬을 이용한 자연어처리 기초](https://www.facebook.com/notes/%EB%B0%94%EB%B2%A8%ED%94%BC%EC%89%AC/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC-%EA%B8%B0%EC%B4%88/784679931573503)
* [Introducing Cloud Natural Language API, Speech API open beta and our West Coast region expansion](https://cloudplatform.googleblog.com/2016/07/the-latest-for-Cloud-customers-machine-learning-and-west-coast-expansion.html)
* [구글, 자연어·음성인식 API 공개…한국어도 포함](http://www.bloter.net/archives/260492)
* [머신러닝의 자연어 처리기술(I)](http://www.slideshare.net/ssuser06e0c5/i-64267027)
* [영국의 19살 청년이 만든 인공지능 로봇변호사](http://www.venturesquare.net/728822)
* [ko_restoration - Module for restoring Korean text working with KomornaPy](https://github.com/lynn-hong/ko_restoration)
* [딥러닝을 이용한 자연어처리의 연구동향](http://www.slideshare.net/ssuser06e0c5/ss-64417928)
* [Exploring Session Context using Distributed Representations of Queries and Reformulations](https://www.microsoft.com/en-us/research/wp-content/uploads/2015/08/sigirfp093-mitra.pdf)
  * 사용자의 쿼리 세션데이터와, 문서클릭데이터로 CNN으로 쿼리의 word-embedding을 만듦
  * 쿼리와 관계를 벡터로 변환
  * 두 쿼리의 관계벡터는 단순히 두 쿼리벡터의 뺴기(차이?)로 간단하지만
  * 이러한 관계벡터들을 클러스터링하니, 쿼리 변환의 의도가 클러스터링 됨
    * 동일의도인데, 다른 모양의 쿼리변환
    * 검색 의도를 좁히는 쿼리변환
    * 의도를 아예 점프하는 쿼리변환
* [기계학습과 딥러닝의 응용](https://www.youtube.com/watch?v=7_6b8iSGj5g)
* [Universal Dependencies](http://universaldependencies.org/)
* [BabelNet](http://babelnet.org/)
  * [META prize 2015: BabelNet!](https://www.youtube.com/watch?v=lchI3AQbA9M)
* [An Intuitive Natural Language Understanding System](http://www.slideshare.net/inscit2006/an-intuitive-natural-language-understanding-system)
* [Korean Treebank Annotations Version 2.0](https://catalog.ldc.upenn.edu/LDC2006T09)
  * [sample](https://catalog.ldc.upenn.edu/desc/addenda/LDC2006T09.txt) EUC-KR encoded
* [An NLP Approach to Analyzing Twitter, Trump, and Profanity](http://blog.algorithmia.com/nlp-approach-twitter-trump-profanity)
* [Deep Learning Cases: Text and Image Processing](http://www.slideshare.net/grigorysapunov/deep-learning-cases-text-and-image-processing)
* [CS 124: From Languages to Information](http://web.stanford.edu/class/cs124/)
* [영문 복사만 하면…품사 알려드려요](http://techholic.co.kr/archives/51731)
* [PyData Paris 2016 - Statistical Topic Extraction](https://www.youtube.com/watch?v=Y4nE6diy72o)
* [brat rapid annotation tool](http://brat.nlplab.org/index.html) online environment for collaborative text annotation
  * [brat rapid annotation tool (brat) - for all your textual annotation needs](https://github.com/nlplab/brat)
* [28회 한글 및 한국어 정보처리 학술대회](https://sites.google.com/site/2016hclt)
  * [자료실](https://sites.google.com/site/2016hclt/jalyosil)
  * [개체명 인식 시스템 개발 및 적용](https://github.com/krikit/annie)
* [확률문법](https://www.facebook.com/saishiot/photos/a.1786185075003862.1073741835.1538273176461721/1786185115003858)
* [주니어 데이터 분석가의 걸그룹 데이터 분석하기](https://brunch.co.kr/@cloud09/97)
* [korean.abcthesaurus.com](http://korean.abcthesaurus.com/) 동의어 사전
* [Microsoft Concept Graph Preview For Short Text Understanding](https://concept.research.microsoft.com/Home/Introduction)
* [en.wikipedia.org/wiki/Precision_and_recall](https://en.wikipedia.org/wiki/Precision_and_recall)
  * **[accuracy, precision, recall의 차이](http://shine-ing.tistory.com/m/157)**
  * [정확도(accuracy)와 정밀도(precision)의 차이](http://www.withrobot.com/technicalreport19/)
  * [en.wikipedia.org/wiki/Sensitivity_and_specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)
  * [measure 상관관계](https://gist.github.com/hyunjun/aaa479bcf6485ea36e6add81c65d3e78)
  * [#2.6. Accuracy, Precision, Recall](https://www.youtube.com/watch?v=1jboC7nWnfM&feature=youtu.be&list=PL0oFI08O71gKEXITQ7OG2SCCXkrtid7Fq)
  * [입개발자를 위한 Accuracy, Precision, Recall](http://www.popit.kr/%EC%9A%A9%EC%96%B4-%EC%A0%95%EB%A6%AC-%EC%9E%85%EA%B0%9C%EB%B0%9C%EC%9E%90%EB%A5%BC-%EC%9C%84%ED%95%9C-accuracy-precision-recall/)
  * [Classification 모델 평가 기준 1편](https://brunch.co.kr/@chris-song/54)
  * **[Classification & Clustering 모델 평가](http://bcho.tistory.com/1206)**
* [Natural Language Understanding with Distributed Representation](https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf)
* [Natural language processing in 10 lines of code: Part 1](http://www.cytora.com/insights/2016/11/30/natural-language-processing-in-10-lines-of-code-part-1)
* [Deep Learning the Stock Market](https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02)
* [NLP: Everyday, Analytical & Unusual Uses](http://www.allanalytics.com/document.asp?doc_id=260387)
* [Welcome to Railroad Diagram Generator!](http://bottlecaps.de/rr/ui) BNF rule to diagram
* [Awesome-Korean-NLP](https://github.com/datanada/Awesome-Korean-NLP)
* [Awesome-korean-nlp](https://insikk.github.io/awesome-korean-nlp/)
* [Is Google Hyping it? Why Deep Learning cannot be Applied to Natural Languages Easily](https://www.linkedin.com/pulse/google-hyping-why-deep-learning-cannot-applied-easily-berkan-ph-d)
* [100 Must-Read NLProc Papers](http://masatohagiwara.net/100-nlp-papers/)
* [NLP papers](https://docs.google.com/spreadsheets/d/1phOGpz-ckF48yLHIW98cuqzR9cas5wXpEqoZi9r4WDI/edit#gid=0)
* [ratsgo.github.io/blog/categories](https://ratsgo.github.io/blog/categories/)
  * [딥러닝 기반 자연어처리 기법의 최근 연구 동향](https://ratsgo.github.io/natural%20language%20processing/2017/08/16/deepNLP/)
* [NLP를 위한 딥러닝 가이드](http://docs.likejazz.com/deep-learning-for-nlp/)
* [Information Extraction with Reinforcement Learning](https://github.com/karthikncode/DeepRL-InformationExtraction)
* [Linguistic Knowledge as Memory for Recurrent Neural Networks](https://arxiv.org/abs/1703.02620)
* [Last Words: Computational Linguistics and Deep Learning](http://mitp.nautil.us/article/170/last-words-computational-linguistics-and-deep-learning)
  * PDP(연결주의)쪽 룸멜허트나 맥클랜드의 연구들 - 신경망 기반 의미론 모형
  * 인간 언어와 관련한 인지과학적 연구 - 어떻게 언어를 학습하고 개념들이 조직화되는가라는 관점
* [Computational Linguistics and Deep Learning](http://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00239)
* [4 APPROACHES TO NATURAL LANGUAGE PROCESSING & UNDERSTANDING](http://www.topbots.com/4-different-approaches-natural-language-processing-understanding)
  * Distributional: 최근 유행하는 ML. 폭은 넓힐 수 있지만, 깊이는 잡지 못함
  * Frame-based: 마빈 민스키. 논리적 semantics에 강점. 확고한 supervision이 존재해야 한다는 단점
  * Model-theoretical: Q/A와 rich semantics의 장점. (프레임 기반보다 더한) labor-intensive and narrow in scope 
  * Interactive learning: language as a cooperative game between speaker and listener
    * Syntax – what is grammatical? : “no compiler errors”
    * Semantics – what is the meaning?: “no implementation bugs”
    * Pragmatics – what is the purpose or goal?: “implemented the right algorithm.”
* [Deep Learning for Text Understanding from Scratch](http://www.kdnuggets.com/2015/03/deep-learning-text-understanding-from-scratch.html)
* **[Mining English and Korean text with Python](https://www.lucypark.kr/courses/2015-ba/text-mining.html)**
* [How to get started in NLP](https://medium.com/towards-data-science/how-to-get-started-in-nlp-6a62aa4eaeff)
* [NATURAL LANGUAGE GENERATION](http://www.inf.ed.ac.uk/teaching/courses/nlg/)
* [CS 20SI: Tensorflow for Deep Learning Research](https://web.stanford.edu/class/cs20si/)
* [NLP for Korean](https://github.com/bage79/nlp4kor)
  * [nlp4kor](https://www.youtube.com/playlist?list=PLE_yleP-KQefhFSNh16hJKnq6stIG05fu)
  * [CNN for MNIST](http://nbviewer.jupyter.org/github/bage79/nlp4kor/blob/master/ipynb/CNN_for_MNIST.ipynb)
  * [CNN for MNIST #1](https://www.youtube.com/watch?v=0UkAV3DnpPk&list=PLE_yleP-KQefhFSNh16hJKnq6stIG05fu&index=1)
  * [CNN for MNIST #2](https://www.youtube.com/watch?v=3-Rvh-lgUcU&list=PLE_yleP-KQefhFSNh16hJKnq6stIG05fu&index=2)
  * [FFNN for 한글 띄어쓰기](https://github.com/bage79/nlp4kor/blob/master/ipynb/FFNN_for_word-spaceing.ipynb)
  * [DAE for 철자 오류 교정](http://nbviewer.jupyter.org/github/bage79/nlp4kor/blob/master/ipynb/DAE_for_spelling_error_correction.ipynb)
* [Building A Gigaword Corpus Lessons on Data Ingestion, Management, and Processing for NLP](https://www.youtube.com/watch?v=j1DdGX2d9BE)
* [Teaching Machines to Describe Images with Natural Language Feedback](http://www.cs.toronto.edu/~linghuan/feedbackImageCaption/)
* [Sang-Kil Park's Jupyter Notebooks](https://github.com/likejazz/jupyter-notebooks)
* [An Adversarial Review of “Adversarial Generation of Natural Language”](https://medium.com/@yoav.goldberg/an-adversarial-review-of-adversarial-generation-of-natural-language-409ac3378bd7)
* [Deep Learning for Speech and Language](https://www.facebook.com/nextobe1/posts/339301146505887)
* [deep learning nlp best practices](http://ruder.io/deep-learning-nlp-best-practices/index.html)
* [Recent Trends in Deep Learning Based Natural Language Processing](https://arxiv.org/abs/1708.02709)
* [Natural Language Processing in Artificial Intelligence is almost human-level accurate. Worse yet, it gets smart!](https://sigmoidal.io/boosting-your-solutions-with-nlp/)
* [Language Emergence](https://github.com/batra-mlp-lab/lang-emerge)
* [Speech and Language Processing (3rd ed. draft)](http://web.stanford.edu/~jurafsky/slp3/)
* [Memory Augmented Neural Networks for Natural Language Processing](https://drive.google.com/file/d/0B9dqzboiV5u-UmxJQlJqcUl6anM/view)
* [EMNLP 2017](http://mogren.one/blog/2017/09/13/emnlp.html)
  * [EMNLP 2017](https://ku.cloud.panopto.eu/Panopto/Pages/Sessions/List.aspx)
* [Natural Language Processing Tasks and Selected References](https://github.com/Kyubyong/nlp_tasks)
* 자연언어처리(NLP)를 위한 언어학 기초
  * [담화분석](http://blog.naver.com/bcj1210/221147187757)
  * [화용론](http://blog.naver.com/bcj1210/221147166747)
  * [의미론](http://blog.naver.com/bcj1210/221147150551)
  * [통사론](http://blog.naver.com/bcj1210/221147134955)
  * [구와 문장](http://blog.naver.com/bcj1210/221145989566)
  * [형태론](http://blog.naver.com/bcj1210/221145151867)
  * [단어의 형성](http://blog.naver.com/bcj1210/221144651784)
  * [언어의 기원](http://blog.naver.com/bcj1210/221144574548)
* [Deep Learning for NLP, advancements and trends in 2017](https://tryolabs.com/blog/2017/12/12/deep-learning-for-nlp-advancements-and-trends-in-2017/)
* [Deep NLP: 딥러닝을 이용한 자연어처리](https://speakerdeck.com/vcnc/deep-nlp-dibreoningeul-iyonghan-jayeoneoceori)
* [AI: NLP](https://www.youtube.com/playlist?list=PLsFtzQAC8dDdIqSY3o5XF_IBIgSLcyzTd)
* [ML/NLP PUBLICATIONS IN 2017](http://www.marekrei.com/blog/ml-nlp-publications-in-2017/)
* [Experiments Codes for Bi-directional Block Self-attention](https://github.com/code4review/BiBloSA)
  * [Bi-Directional Block Self-Attention for Fast and Memory-Efficient Sequence Modeling](https://openreview.net/forum?id=H1cWzoxA-)
  * 주어진 시퀀스를 여러 개의 Block 으로 나누고 intra-block SAN으로 local context 를 모델링한 뒤, inter-block SAN으로 long-range dependency 를 모델링
  * 기존의 Self-Attention Network (SAN) 이 너무 메모리를 많이 쓰는 점을 개선
  * 많은 NLP 분야에서 Self-attention 기법들이 (특히 번역 분야에서는) 표준으로 자리잡고 후속 연구가 활발히 이루어지고 있는 걸로 보임
    * (ex. Non-autoregressive transformer, Masked self-attention, Directional self-attention)
* [How to solve 90% of NLP problems: a step-by-step guide](https://blog.insightdatascience.com/how-to-solve-90-of-nlp-problems-a-step-by-step-guide-fda605278e4e)
* [파이썬자연어처리](https://www.youtube.com/playlist?list=PLaTc2c6yEwmrmRyBUMsDDFk_FyLF9fARR)
* [IMDB 영화리뷰 감정 분석](https://www.youtube.com/playlist?list=PLaTc2c6yEwmocBySMhvBttyAD8eZelFIP)

# 띄어쓰기
* [기계학습을 이용한 한글 자동 띄어쓰기](http://hub-ai.com/nlp/767)
* [어절 uni-gram을 이용한 띄어쓰기 모델](http://sonsworld.tistory.com/85)
* [Sentence boundary disambiguation](https://en.wikipedia.org/wiki/Sentence_boundary_disambiguation)
* [python-crfsuite를 사용해서 한국어 자동 띄어쓰기를 학습해보자](http://blog.theeluwin.kr/post/147587579528/python-crfsuite%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%B4%EC%84%9C-%ED%95%9C%EA%B5%AD%EC%96%B4-%EC%9E%90%EB%8F%99-%EB%9D%84%EC%96%B4%EC%93%B0%EA%B8%B0%EB%A5%BC-%ED%95%99%EC%8A%B5%ED%95%B4%EB%B3%B4%EC%9E%90)
* [RNN을 이용한 한글 자동 띄어쓰기](http://freesearch.pe.kr/archives/4617)
* [딥러닝 기반 한글 자동 띄어쓰기 API 공개](http://freesearch.pe.kr/archives/4647)
  * [딥러닝 한글 자동띄어쓰기 모형 성능 향상 및 API 업데이트](http://freesearch.pe.kr/archives/4674)

# Book
* [Neural Network Methods for Natural Language Processing](http://www.morganclaypoolpublishers.com/catalog_Orig/samples/9781627052955_sample.pdf)
  * [A Primer on Neural Network Models for Natural Language Processing](http://u.cs.biu.ac.il/~yogo/nnlp.pdf)
* [Quantitative corpus linguistics with R: a practical introduction](http://www.linguistics.ucsb.edu/faculty/stgries/research/qclwr/qclwr.html)

# Category
* text categorization; 예를 들어 100만개의 상품 description이 있고, 이걸 supervised를 위한 document로 사용해, 나중에 들어오는 상품 description을 통해 cateogory 판별
  * naive bayes
  * gensim, model.docvecs e.g. model.docvecs.most_similar([1,2,3]) -> 문서 태그가 '10000'이면 model.docvecs['10000']으로 해당 docvec을 가져옴
    * most_similar 호출 시 파라미터로써 벡터(numpy array)의 리스트 혹은, 문서의 태그들이 담긴 리스트 전달 가능
    * 결과 값으로 문서의 태그 및 유사도를 반환
  * doc2vec
    * 낮은 정확도
    * 기본적으로 word co-occurrence 에 기반하고 있고 각 word 는 word embedding 에 의한 vector 사용
    * 이 vector들의 단순 합은 ambiquity 문제가 경험적으로 발생
    * document 단위가 짧으면 짧은 대로 , 쿼리 스트링이 짧으면 짧은 대로 또 ambiguity 문제가 발생
  * word2vec
    * doc2vec과 유사
    * 전체 corpus 에 대해 모델을 만든 후, predict 할 때 description 보다 제목 같이 짧으면서 컨텍스트를 담고 있는 것으로
  입력을 주면 좀 나음
  * 이미 카테고리 도메인이 결정된 경우 LDA/LSI 가 더 좋은 방법일 수 있음
    * LDA / LSI 는 각각의 카테고리를 반영하는 토큰의 기여도를(weight) 확률분포로 표현
    * LDA 경우 더 많이 기여하고 있는 워드 순
    * LSI 의 경우 positive 기여도 뿐만 아니라 negative 기여도 확률을 결과로 반환
    * 그러므로 쿼리스트링이 있을 때 가장 확률 높은 카테고리 계산 가능
  * TFIDF
    * feature 수가 많다 해도 document similarity 를 계산하는 게 아니라 카테고리를 분류하기 위함이기 때문에 dimension
  문제가 크지 않을 수 있음
    * TFIDF 로 weighting 한 벡터들을 가지고 클러스터링
    * 실제 label 가지고 TFIDF weight 가 각 label 을 얼마나 잘 구분하고 있는지 feasibility 를 판단할 수도 있음
    * 혹은 각 카테고리별로 모델을 만들어서 dictionary 를 작게 만들어 feature 수를 줄일 수도 있음
    * 각각의 dictionary 셋과 워드에 대한 TFIDF weight 를 가지고 카테고리별로 representing 한 워드들을 뽑아볼 수도 있음
* [나누고 분류해야 세상이 보인다](http://ppss.kr/archives/32926)
* [Category Theory for Programmers: The Preface](http://bartoszmilewski.com/2014/10/28/category-theory-for-programmers-the-preface/)
* [Category Theory for Scientists (Old Version)](http://ocw.mit.edu/courses/mathematics/18-s996-category-theory-for-scientists-spring-2013/textbook/MIT18_S996S13_textbook.pdf)
* [분류 문제에서 앙상블 방법](http://freesearch.pe.kr/archives/1071)
* [Logic, Languages, Compilation, and Verification](http://www.cs.uoregon.edu/research/summerschool/summer12/curriculum.html)
* [‘뉴욕타임스’, 머신러닝 기반 자동 태그 시스템 개발](http://www.bloter.net/archives/234850)
* [Categories for Programmers](http://bartoszmilewski.com/2015/09/01/the-yoneda-lemma/)
* [Fast & easy baseline text categorization with vw](http://nlpers.blogspot.com/2016/08/fast-easy-baseline-text-categorization.html)

# ChatBot
* [HipChat을 이용한 ChatBot 만들기](https://opentutorials.org/module/2260/12797)
* [DEEP LEARNING FOR CHATBOTS, PART 1 – INTRODUCTION](http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/)
  * [딥러닝 챗봇, PART 1 – INTRODUCTION (한글번역)](http://blog.naver.com/rupy400/220781279491)
* [DEEP LEARNING FOR CHATBOTS, PART 2 – IMPLEMENTING A RETRIEVAL-BASED MODEL IN TENSORFLOW](http://www.wildml.com/2016/07/deep-learning-for-chatbots-2-retrieval-based-model-tensorflow/)
  * [딥러닝 챗봇 , PART 2 – IMPLEMENTING A RETRIEVAL-BASED MODEL IN TENSORFLOW(한글번역)](http://blog.naver.com/rupy400/220781859893)
* [Deep leaning for Chatbot Developers](https://github.com/j-min/DL-for-Chatbot/)
  * [DL-for-Chatbot](https://github.com/j-min/DL-for-Chatbot/tree/master/slides)
* [Clippy’s Back: The Future of Microsoft Is Chatbots](http://www.bloomberg.com/features/2016-microsoft-future-ai-chatbots/)
* [Build a bot without coding - Launch a full-featured chatbot in 7 minutes](https://chatfuel.com/)
* [Microsoft Bot Framework](https://dev.botframework.com/)
* [사람이 챗봇을 만듭니다](https://www.youtube.com/playlist?list=PLqkITFr6P-oRQu0OJCIqHuff-ubbCkWlL)
  * [Microsoft Bot Framework 관련 강좌](https://github.com/koreaEva/Bot)
  * [20180120Hands_on_Lab](https://drive.google.com/drive/folders/14d7Maw7nCkvey_BoVFVCv_I_B3IqAFp7)
* [Botkit - Building Blocks for Building Slack Bots](https://howdy.ai/botkit/)
* [AWS Lambda와 API Gateway로 Slack Bot 만들기](http://www.usefulparadigm.com/2016/04/06/creating-a-slack-bot-with-aws-lambda-and-api-gateway/)
* [Your next shopping experience starts with a text](https://operator.com/)
* [x.ai is a personal assistant who schedules meetings for you](https://x.ai/)
* [Kino - My Personal Assistant (개인용 Slack Bot을 통한 Quantified Self 프로젝트)](https://www.slideshare.net/DongJunLee6/kino-my-personal-assistant-slack-bot-quantified-self)
* [slacker로 slack bot 만들기](https://hyesun03.github.io/2016/10/08/slackbot/)
* [wit.ai](https://wit.ai/)
  * [Wit.ai stories/conversational app demo](https://youtu.be/yLAHVPaHWFA)
* [AWS 서버리스 챗봇 경진대회에 참여하세요!](https://aws.amazon.com/ko/blogs/korea/enter-the-aws-serverless-chatbot-competition/)
* [The White House's New Facebook Messenger Bot Makes It Easy To Send A Message To Obama](http://www.fastcompany.com/3062674/the-white-houses-new-facebook-messenger-bot-makes-it-easy-to-send-a-message-to-obama)
* [Wonder is a bot that will remember anything for you](https://techcrunch.com/2016/08/12/wonder-is-a-bot-that-will-remember-anything-for-you/)
* [Introducing the Bots Landscape: 170+ companies, $4 billion in funding, thousands of bots](http://venturebeat.com/2016/08/11/introducing-the-bots-landscape-170-companies-4-billion-in-funding-thousands-of-bots/)
* [지적 대화를 위한 깊고 넓은 딥러닝 Pycon APAC 2016](http://www.slideshare.net/carpedm20/pycon-korea-2016)
  * [PyCon 2016’s TensorFlow 자료](https://tensorflowkorea.wordpress.com/2016/08/16/pycon-2016s-tensorflow-%EC%9E%90%EB%A3%8C/)
  * 1. 이미지(사람의 얼굴 사진)을 이해하고 스스로 만드는 모델
    * [carpedm20.github.io/faces](http://carpedm20.github.io/faces/)
  * [github.com/carpedm20/DCGAN-tensorflow](https://github.com/carpedm20/DCGAN-tensorflow)
  * [뉴럴 네트워크로 만든 튜링 머신](https://github.com/carpedm20/NTM-tensorflow)
  * [Question Answering, Language Model](https://github.com/carpedm20/MemN2N-tensorflow)
  * [Teaching Machines to Read and Comprehend](https://github.com/carpedm20/attentive-reader-tensorflow)
  * [Neural Variational Inference for Text Processing](https://github.com/carpedm20/variational-text-tensorflow)
* [Stanfy Blog](https://stanfy.com/blog/)
  * [Advanced Natural Language Processing Tools for Bot Makers – LUIS, Wit.ai, Api.ai and others](https://stanfy.com/blog/advanced-natural-language-processing-tools-for-bot-makers/)
  * [The Rise of Chat Bots: Useful Links, Articles, Libraries and Platforms](https://stanfy.com/blog/the-rise-of-chat-bots-useful-links-articles-libraries-and-platforms/)
  * [Know Your Bot, Part II: Slack, The Bot Paradise](https://stanfy.com/blog/know-your-bot-part-ii-slack-the-bot-paradise/)
  * [Know Your Bot, Part I: Telegram And Twitter](https://stanfy.com/blog/know-your-bot-part-i-telegram-and-twitter/)
  * [s2 lab1-1: API.ai concept and terms](https://www.youtube.com/watch?v=jF70X0tUzV8)
  * [s2 lab1-2: API.ai making bot demo](https://youtu.be/jBnzfLGcn5o)
* [Multi-domain Neural Network Language Generation for Spoken Dialogue Systems(NAACL-HLT 2016)](https://arxiv.org/abs/1603.01232)
  * [code](https://github.com/shawnwun/RNNLG/tree/master)
* [facebook 맞춤법 검사기 봇](https://www.facebook.com/groups/botgroup/permalink/504923836371451/)
* [코딩없이 만드는 채팅봇](http://www.closer.co.kr/)
* [Dialog System - http://nlp.postech.ac.kr/research/dialog_system/](http://nlp.postech.ac.kr/research/dialog_system/)
* [Do-it-yourself NLP for bot developers](https://medium.com/lastmile-conversations/do-it-yourself-nlp-for-bot-developers-2e2da2817f3d)
* [Making Friends With Artificial Intelligence: Eric Horvitz at TEDxAustin](https://www.youtube.com/watch?v=dpoVh9xwdD4)
* [4차 산업혁명 특별기획 ‘기계와의 대결’ 2부](http://news.kbs.co.kr/news/view.do?ncd=3336967)
* [Facebook steps in to prove the value of chatbots with Tommy Hilfiger](https://techcrunch.com/2016/09/09/botty-hilfiger)
* [The rise of bots... acquisitions!](https://www.linkedin.com/pulse/rise-bots-acquisitions-luigi-congedo)
* **[라이크 어 Poncho: JiveScript 날씨 챗봇](http://www.popit.kr/how-to-make-korean-chatbot/)**
* [Heek is a chatbot that can build you a website](https://techcrunch.com/2016/10/03/heek-is-a-chatbot-that-can-build-you-a-website/)
* [bots.duolingo.com](http://bots.duolingo.com/)
* [혼자 힘으로 한국어 챗봇 개발하기](http://exagen.tistory.com/notice/63)
* [챗봇 개발 프레임워크 ChatFlow, 베타버전 출시](http://etinow.me/104)
* [Build a restaurant reservation Messenger bot using IBM Watson with no code](https://blog.stamplay.com/build-a-restaurant-reservation-messenger-bot-using-ibm-watson-with-no-code-912745bafa7)
* [DeepQA - My tensorflow implementation of "A neural conversational model", a Deep learning based chatbot](https://github.com/Conchylicultor/DeepQA)
  * [Deep Q&A](https://github.com/hunkimForks/DeepQA)
* [챗봇 시작해보기](http://www.slideshare.net/ssusercf5d12/ss-69518853)
* [대화형 챗봇 설계의 과제](https://gist.github.com/haje01/7fc9d1b1fc1b6c8c9b7918abf5407a86)
* [A developer's guide to chatbots](http://www.ibm.com/developerworks/library/cc-cognitive-chatbot-guide/index.html)
* [UX 북마크#10. 챗봇(Chatbot) A-Z](https://brunch.co.kr/@ebprux/197)
* [TF-KR Conf 2 강의 2: 조재민, Developing Korean chatbot 101](https://www.youtube.com/watch?v=i0sQB1DRh84&index=3&list=PLlMkM4tgfjnLHjEoaRKLdbpSIDJhiLtZE&t=372s)
* [Developing Korean Chatbot 101](http://www.slideshare.net/JaeminCho6/developing-korean-chatbot-101-71013451)
* [Retrieval-Based Conversational Model in Tensorflow (Ubuntu Dialog Corpus)](https://github.com/dennybritz/chatbot-retrieval/)
* [20170121 한국인공지능협회 - 제7차 오픈세미나 - 챗봇 (1/5)](https://www.youtube.com/watch?v=UdQBegtyFsw&feature=youtu.be)
* [20170121 한국인공지능협회 - 제7차 오픈세미나 - 챗봇 (2/5)](https://www.youtube.com/watch?v=5tb-BMMp0Zc&feature=youtu.be)
* [20170121 한국인공지능협회 - 제7차 오픈세미나 - 챗봇 (3/5)](https://www.youtube.com/watch?v=DXgHy8YUbZs&feature=youtu.be)
* [20170121 한국인공지능협회 - 제7차 오픈세미나 - 챗봇 (4/5)](https://www.youtube.com/watch?v=Tq2otY_SQQQ&feature=youtu.be)
* [20170121 한국인공지능협회 - 제7차 오픈세미나 - 챗봇 (5/5)](https://www.youtube.com/watch?v=n-IXuLpqkGo&feature=youtu.be)
* [세계 챗봇 생태계 분석](https://brunch.co.kr/@pilsogood/2)
* [20170227 파이썬으로 챗봇_만들기](https://www.slideshare.net/KimSungdong1/20170227-72644192)
* [KahWee Teng: Coding Chat Bots - JSConf.Asia 2016](https://www.youtube.com/watch?v=c_hmwFwvO0U&feature=youtu.be)
* [Node.JS로 카카오봇 만들기](https://cheese10yun.github.io/kakao-bot-node)
* [카카오톡 자동응답 API로 학식봇 구현](http://throughkim.kr/2016/07/11/kakao-haksik/)
* [카카오톡 자동응답 API를 이용하여 카카오톡 봇 만들기](http://secumaster2.tistory.com/3)
* [The Conversational Intelligence Challenge](https://deeppavlov.github.io/convai/)
* [Visual Dialog - a novel task that requires an AI agent to hold a meaningful dialog with humans in natural, conversational language about visual content](https://visualdialog.org/)
* [Natural Language Pipeline for Chatbots](https://hackernoon.com/natural-language-pipeline-for-chatbots-897bda41482)
* [Contextual Chat-bots with Tensorflow](https://chatbotsmagazine.com/contextual-chat-bots-with-tensorflow-4391749d0077)
* [How To Build an Interactive Chatbot for Twitter Direct Messages](https://chatbotsmagazine.com/how-to-create-a-twitter-direct-message-bot-with-quick-replies-3ee6a2af4a12)
* [슬랙봇, 어디까지 만들어봤니?](https://spoqa.github.io/2017/05/22/slackbot.html)
* [왓슨으로 쉽게 개발하는 카카오톡 챗봇 1. Watson Conversation 서비스로 인공지능 대화 서비스 만들기](https://developer.ibm.com/kr/cloud/bluemix/watsonservice/2017/01/13/watsonchatbot-1-watson-conversation/)
* [Node.js Facebook 챗봇 빠른시작: 369봇 만들기](https://gist.github.com/kidkkr/b8e0f18b9274bb66b2630e38c9c5211b)
* [www.luis.ai](https://www.luis.ai)
* [Stephanie - YOUR VIRTUAL ASSISTANT!](https://slapbot.github.io/)
  * [Stephanie Virtual Assistant](https://www.youtube.com/watch?v=Pp5dXbWZsrU)
  * [Stephanie - an open-source platform built specifically for voice-controlled applications as well as to automate daily tasks imitating much of a virtual assistant's work](https://github.com/slapbot/stephanie-va)
  * [SOUNDER ALGORITHM](https://slapbot.github.io/documentation/resources/algorithm/)
  * [Sounder API - the Sounder Library API, which is an abstraction of the Sounder Algorithm](https://github.com/slapbot/sounder)
  * [USAGE](https://slapbot.github.io/documentation/usage/)
* [챗봇을 만들기 위한 두 가지 AI 모델](https://brunch.co.kr/@gentlepie/18)
* [Deal or no deal? Training AI bots to negotiate](https://code.facebook.com/posts/1686672014972296)
  * [Deal or No Deal? End-to-End Learning for Negotiation Dialogues](https://github.com/facebookresearch/end-to-end-negotiator) pytorch
* [대화시스템 개발을 위한 자연어처리 기술](http://blog.naver.com/naver_search/221027662050)
* [Creating AnswerBot with Keras and TensorFlow (TensorBeat)](https://www.slideshare.net/Avkashslide/creating-answerbot-with-keras-and-tensorflow-tensorbeat)
  * [tensorbeat-answerbot](https://github.com/Avkash/mldl/tree/master/tensorbeat-answerbot)
* [30분 안에 챗봇 만들기 1편](https://brunch.co.kr/@chris-song/28)
* [30분 안에 챗봇 만들기 2편](https://brunch.co.kr/@chris-song/29)
* [Own ChatBot Based on Recurrent Neural Network](https://blog.kovalevskyi.com/rnn-based-chatbot-for-6-hours-b847d2d92c43)
* [Chatbots: Theory and Practice](https://www.linkedin.com/pulse/chatbots-theory-practice-jonathan-mugan)
* [Let Android dream electric sheep: Making emotion model for chat-bot with Python3, NLTK and TensorFlow](https://www.slideshare.net/inureyes/let-android-dream-electric-sheep-making-emotion-model-for-chatbot-with-python3-nltk-and-tensorflow)
* [Show me red! – feat. 서울 시립 미술관 데이터를 사용한 챗봇 만들기](http://khg423.dothome.co.kr/index.php/2017/08/21/show-me-red-feat-%EC%84%9C%EC%9A%B8-%EC%8B%9C%EB%A6%BD-%EB%AF%B8%EC%88%A0%EA%B4%80-%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%9C-%EC%B1%97%EB%B4%87-%EB%A7%8C%EB%93%A4%EA%B8%B0/)
* [Python과 Tensorflow를 활용한 AI Chatbot 개발 및 실무 적용](https://www.slideshare.net/healess/python-tensorflow-ai-chatbot)
* [“인공지능” 리테일 챗봇 만들기](https://developer.ibm.com/kr/journey/create-cognitive-retail-chatbot/)
* [Seq2Seq Chatbot](https://github.com/zsdonghao/seq2seq-chatbot)
* [페이스북 챗봇 만들기](https://korchris.github.io/2017/06/29/FB_chatbot/)
* [Neural Network Dialog System Papers](https://github.com/snakeztc/NeuralDialogPapers)
* [강화학습 챗봇 Dialogue Policy Optimization](https://drive.google.com/file/d/1thN7vY8QnWtVM1X5myoxnhRi8DK1tlpG/view)

## Python
* [Slacker를 이용한 Slack Bot 만들기](https://corikachu.github.io/articles/2016-08/python-slack-bot-slacker)
* [Building AI Chat bot using Python 3 & TensorFlow](https://speakerdeck.com/inureyes/building-ai-chat-bot-using-python-3-and-tensorflow)
  * [Chat bot making process using Python 3 & TensorFlow](http://pt.slideshare.net/inureyes/chat-bot-making-process-using-python-3-tensorflow)
  * [신정규 : Creating AI chat bot with Python 3 and Tensorflow - PyCon APAC 2016](https://www.youtube.com/watch?v=q44fefORi1k)
  * [Scripts used for preparing PyCON APAC 2016 presentation https://speakerdeck.com/inureyes/building-ai-chat-bot-using-python-3-and-tensorflow](https://github.com/inureyes/pycon-apac-2016)
* [Create a Chatbot for Telegram in Python to Summarize Text](http://blog.algorithmia.com/create-a-chatbot-telegram-python-summarize-text/)
* [python에서 telegram bot 사용하기](https://blog.psangwoo.com/2016/12/08/python%EC%97%90%EC%84%9C-telegram-bot-%EC%82%AC%EC%9A%A9%ED%95%98%EA%B8%B0/)
* python으로 telegram bot 활용하기
  * [1 기본 설정편](https://blog.psangwoo.com/coding/2016/12/08/python-telegram-bot-1.html)
  * [2 채널편](https://blog.psangwoo.com/coding/2016/12/11/python-telegram-bot-2.html)
  * [3 챗봇편](https://blog.psangwoo.com/coding/2018/01/09/python-telegram-bot-3.html)
* [카카오톡 대화 생성기(http://jsideas.net/python/2017/04/05/kakao_rnn.html)
* [Building a botnet on PyPi](https://hackernoon.com/building-a-botnet-on-pypi-be1ad280b8d6)
* [ChatOps with PowerShell - Matthew Hodgkins](https://www.youtube.com/watch?v=XIMOFnfdOx0)

# Classification
* [Bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model)
* [Implementing a CNN for Text Classification in TensorFlow](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)
  * [Convolutional Neural Network for Text Classification in Tensorflow](https://github.com/dennybritz/cnn-text-classification-tf)
  * [IMPLEMENTING A CNN FOR TEXT CLASSIFICATION IN TENSORFLOW (한글 번역)](http://blog.naver.com/rupy400/220777178142)
  * [CNNs for sentence classification](https://github.com/yoonkim/CNN_sentence)
  * [합성곱 신경망(CNN) 딥러닝을 이용한 한국어 문장 분류](http://docs.likejazz.com/cnn-text-classification-tf/)
  * [MIT 6.S191 Lecture 2: Sequence Modeling with Neural Networks](https://www.youtube.com/watch?v=zQxm3Upr3_I&feature=youtu.be)
* [Free Code Friday - Better and Faster Machine Learning Classifiers in Python](https://www.youtube.com/watch?v=AA-bJmZak9Q)
* [Time series classification](http://www.slideshare.net/hunkim/time-series-classification)
* [“What is Relevant in a Text Document?”](https://arxiv.org/pdf/1612.07843v1.pdf)
  * 예를 들어, 카테고리가 있는 뉴스문서 학습데이터가 있는 경우 문서를 분류하는 분류기를 만들 때
  * 문서에서 어떤 단어가 어떤 클래스로 분류하는데 얼만큼의 영향이 있었는지 역으로 추적하기가 쉽지 않음(Maximum Entropy 같은 걸 사용하는 것이 아니라면)
  * 이를 역으로 추적하는 방법에 대한 논문
* [Text Classification using Neural Networks](https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6)
* [Text Classification using Algorithms](https://chatbotslife.com/text-classification-using-algorithms-e4d50dcba45)
* [Text Classifier Algorithms in Machine Learning](https://blog.statsbot.co/text-classifier-algorithms-in-machine-learning-acc115293278)
* [Tensorflow Text Classification – Python Deep Learning](https://sourcedexter.com/tensorflow-text-classification/)
* [lime](https://github.com/marcotcr/lime)
* [On Building a “Fake News” Classification Model \*update](https://opendatascience.com/blog/how-to-build-a-fake-news-classification-model/)
* [scalawox fakenews](https://www.facebook.com/scalawox/posts/483538445375414)
* [Automated Text Classification Using Machine Learning](https://www.kdnuggets.com/2018/01/automated-text-classification-machine-learning.html)
* [TRAIN ONCE, TEST ANYWHERE: ZERO-SHOT LEARNING FOR TEXT CLASSIFICATION](https://paralleldots.xyz/Zero-Shot-Learning-for-Text-Classification)
  * Zero Shot Learning : 학습 데이터없이 텍스트 분류 모델 만들기
    * Zero Shot Learning은 학습을 하지 않고 데이터세트의 구성원을 추론할 수 있는 방법
    * 대부분 하나의 데이터 세트에서 습득한 지식을 다른 학습 세트에 적용 할 수 있는 일부 형태의 transfer learning에 의해 성취됩니다
  * 지금까지 imagenet 데이터세트의 지식을 새로운 것에 사용할 수 있는 비전 작업을 위해 여러 개의 Zero Shot Learning 방법을 제안했지만 텍스트 분류를 위한 건 최초
    * 큰 노이즈의 데이터세트에서 문장과 해당 범주 간의 관계를 학습하여 새로운 범주 또는 새 데이터세트로 일반화
  * [TRY OUR CUSTOM CLASSIFIER DEMO](https://www.paralleldots.com/custom-classifier)
* [Alisa Dammer - Baby steps in short-text classification with python](https://www.youtube.com/watch?v=5ExfLYdYzQE)
* [Actionable and Political Text Classification Using Word Embeddings and LSTM](https://www.youtube.com/watch?v=NOUMgThZ5UE)
* [Pycon Ireland 2017: Text Classification with Word Vectors & Recurrent Neural Networks - Shane Lynn](https://www.youtube.com/watch?v=KcS6nVUT3Gc)
* [Machine Learning - Text Classification with Python, nltk, Scikit & Pandas](https://www.youtube.com/watch?v=5xDE06RRMFk)
* [Introduction to Natural Language Processing with Python - Asyncjs](https://www.youtube.com/watch?v=IMKweOTFjXw)
* [Patrick Harrison | Modern NLP in Python](https://www.youtube.com/watch?v=6zm9NC9uRkk)
* [Advanced Python 2: Advanced Text Processing](https://www.youtube.com/watch?v=kVnGH0i4Kbs)

# Clustering
* dbscan
* [Finding Topics in Harry Potter using K-Means Clustering](http://dogdogfish.com/2015/05/11/finding-topics-in-harry-potter-using-k-means-clustering/)
* 언론사가 알아야 할 알고리즘
  * [① k-means 클러스터링](http://www.bloter.net/archives/263023)
  * [② 협업 필터링 추천](http://www.bloter.net/archives/263722)
* [Comparing different clustering algorithms on toy datasets](http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html#sphx-glr-auto-examples-cluster-plot-cluster-comparison-py)
* [Density-Based Clustering](https://blog.dominodatalab.com/topology-and-density-based-clustering/)
* [Text Clustering : Get quick insights from Unstructured Data 1](https://machinelearningblogs.com/2017/01/26/text-clustering-get-quick-insights-from-unstructured-data/)
* [Text Clustering : Get quick insights from Unstructured Data 2](https://machinelearningblogs.com/2017/06/23/text-clustering-get-quick-insights-unstructured-data-2/)
* [14 Great Articles and Tutorials on Clustering](https://www.datasciencecentral.com/profiles/blogs/14-great-articles-and-tutorials-on-clustering)
* [The 5 Clustering Algorithms Data Scientists Need to Know](https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68)

# Corpus
* [CORPORA AND OTHER LANGUAGE AND SPEECH DATA UNDER DICE](http://www.inf.ed.ac.uk/resources/corpora/)
* [UTagger + KorpuSQL을 이용해서 코퍼스 구축하기](http://bab2min.tistory.com/474)
* [KorpuSQL 클릭만으로 간편하게 코퍼스 구축하기](http://bab2min.tistory.com/529)
* [PHP, MySQL 코퍼스를 통해 관련어 추출](http://bab2min.tistory.com/533)
* [인공지능 씨앗 한글 말뭉치, 2007년 멈춰선 까닭](http://www.bloter.net/archives/260569)
* [④ 송철의 국립국어원장 "한국어 AI 시대의 기초는 말뭉치..제2의 세종계획 추진해야"](http://media.daum.net/digital/others/newsview?newsid=20161009102004061)
* [언제까지 포털 영어사전만 쓸 건가요? – 말뭉치(코퍼스)를 활용한 영어 글쓰기 기초 편](http://slownews.kr/58742)
* [NIA(National Information Society Agency) Dictionary](https://github.com/haven-jeon/NIADic)
  * [신조어 포함된 형태소사전 공개..빅데이터 분석 정확도↑](http://v.media.daum.net/v/20170221141203629)
  * [우리말샘 사전 현황](https://htmlpreview.github.io/?https://github.com/haven-jeon/NIADic/blob/master/NIADic/vignettes/woorimalsam-dic.html)
  * [한글형태소 사전 NIADic](https://kbig.kr/index.php?page=1&sv=title%E2%80%8B%E2%80%8B&sw&q=knowledge%2Fpds_&tgt=view&idx=16451)
* [개체명 인식용 말뭉치](https://ithub.korean.go.kr/user/total/referenceView.do?boardSeq=5&articleSeq=118&boardGb=T&isInsUpd&boardType=CORPUS)
* [국어사전 데이터](http://www.korean.go.kr/front/onlineQna/onlineQnaView.do;front=C010D1C318C9E310B1EAB32905185327?mn_id=60&qna_seq=123958&pageIndex=1)
* [Korean Parallel corpora (of https://sites.google.com/site/koreanparalleldata/)](https://github.com/jungyeul/korean-parallel-corpora)
* [표준국어대사전.csv](https://github.com/mrchypark/stdkor)

# Disambiguation
* [Automatic disambiguation of English puns](https://www.ukp.tu-darmstadt.de/fileadmin/user_upload/Group_UKP/publikationen/2015/2015_Miller_Disambiguation_of_English_puns.pdf)
* [Discovering Types for Entity Disambiguation](https://blog.openai.com/discovering-types-for-entity-disambiguation/)

# Doc2Vec
* [REDDIT 2 VEC - Use Doc2Vec to get SubReddit Suggestions](http://www.reddit2vec.com/)

# Filtering
* [집단지성프로그래밍 ch6. 문서 필터링](http://www.slideshare.net/icristi/ch6-48743141)

# Knowledge
* [국가생물종지식정보시스템](http://www.nature.go.kr/)
* [:BaseKB Gold Ultimate is now available in AWS](https://groups.google.com/forum/#!topic/infovore-basekb/1YR9Zl5ANDQ)
  * [:BaseKB Gold Ultimate](http://basekb.com/gold/)
  * [:BaseKB Gold Ultimate](https://aws.amazon.com/marketplace/pp/B010RA39G4/)
* [Knowledge-Based Trust: Estimating the Trustworthiness of Web Sources](http://www.vldb.org/pvldb/vol8/p938-dong.pdf)

# Language Model LM
* [Language modeling a billion words](http://torch.ch/blog/2016/07/25/nce.html)
* [확률론적 언어 모형](https://www.datascienceschool.net/view-notebook/a0c848e1e2d343d685e6077c35c4203b/)
* [Perplexed by Game of Thrones. A Song of N-Grams and Language Models](http://nlp.yvespeirsman.be/blog/song-of-ngrams-and-lms/)
* [Character-Aware Neural Language Models](https://arxiv.org/abs/1508.06615)
  * [Character-Aware Neural Language Models](https://github.com/yoonkim/lstm-char-cnn)
  * CNN과 Highway Network를 사용 (입력은 LSTM)해서 State-of-Art의 성과
  * 기존보다 크게 감소된 Parameter로 높은 성능을 내어, 휴대폰과 같은 Model Size가 중요한 영향을 미치는 곳에 적합
  * Word Embedding 시 형태소 tagging 필요하지 않음
  * 형태소 정보들이 많은 언어에서 기존보다 높은 성능 (언어 종속성 낮음)
* [14. 텐서플로우(TensorFlow)를 이용해서 언어 모델(Language Model) 만들기 – Recurrent Neural Networks(RNNs) 예제 2 – PTB(Penn Tree Bank) 데이터셋](http://solarisailab.com/archives/1925)
* **[How to Develop a Word Embedding Model for Predicting Movie Review Sentiment](https://machinelearningmastery.com/develop-word-embedding-model-predicting-movie-review-sentiment/)** keras, word2vec
* [MUSE: Multilingual Unsupervised and Supervised Embeddings](https://github.com/facebookresearch/MUSE)

# LDA [Latent Dirichlet Allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)
* [Yes24 책 추천 알고리즘, 어떻게 구현했나](http://hyunje.com/data%20analysis/2016/02/02/yes24-recommendation-2/)
* [Latent Dirichlet Allocation (LDA) with Python](https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html#what-is-lda)
* [Latent Dirichlet Allocation, LDA](http://parkcu.com/blog/latent-dirichlet-allocation/)
* [word2vec, LDA, and introducing a new hybrid algorithm: lda2vec](http://www.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994)
* [Spectral LDA on Spark](https://github.com/FurongHuang/spectrallda-tensorspark)

# Library
* 날개셋
  * [다음 버전 개발 근황](http://moogi.new21.org/tc/1360)
* [오픈 한글](http://openhangul.com/)
* [은전한닢 프로젝트 - 검색에서 쓸만한 오픈소스 한국어 형태소 분석기를 만들자!](http://eunjeon.blogspot.kr/)
  * [elasticsearch-analysis-seunjeon 5.0.0.0 배포합니다](http://eunjeon.blogspot.com/2016/11/elasticsearch-analysis-seunjeon-5000.html)
* [academictorrents.com](http://academictorrents.com/)
* [Adapt Intent Parser - an open source software library for converting natural language into machine readable data structures](https://adapt.mycroft.ai)
* [AllenNLP - An open-source NLP research library, built on PyTorch](http://allennlp.org/)
* [Babelpish.github.io](http://babelpish.github.io)
* [Compact Language Detector 2](https://github.com/CLD2Owners/cld2)
* [Daon 형태소 분석기](https://github.com/rasoio/daon)
* [fastText is a library for efficient learning of word representations and sentence classification](https://github.com/facebookresearch/fastText)
  * C++, 추가적인 의존 라이브러리 없음
  * Deep Learning 기반의 분류기와 정확도는 비슷하면서도 속도가 빠름
  * multi-core CPU 상에서 10억개 이상의 단어를 10분 내로 학습하고, 50만개의 문장을 1분안에 312k개의 클래스로 분류 가능
  * [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759)
    * our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation
    * We can train fastText on more than one billion words in less than ten minutes using a standard multicore CPU, and classify half a million sentences among 312K classes in less than a minute.
  * [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)
  * [Facebook’s Artificial Intelligence Research lab releases open source fastText on GitHub](https://techcrunch.com/2016/08/18/facebooks-artificial-intelligence-research-lab-releases-open-source-fasttext-on-github)
  * **[Introduction to Natural Language Processing with fastText](https://github.com/miguelgfierro/sciblog_support/blob/master/Intro_to_NLP_with_fastText/Intro_to_NLP.ipynb)**
  * [FastText.zip: Compressing text classification models](https://arxiv.org/abs/1612.03651)
  * [Pre-trained word vectors](https://github.com/facebookresearch/fastText/blob/master/pretrained-vectors.md)
  * [Aligning the fastText vectors of 78 languages](https://github.com/Babylonpartners/fastText_multilingual)
  * [Introduction to Natural Language Processing with fastText](https://github.com/miguelgfierro/sciblog_support/blob/master/Intro_to_NLP_with_fastText/Intro_to_NLP.ipynb)
  * [fastText_doc2vec](https://github.com/Skarface-/fastText_doc2vec)
  * [fastText4j - Java port of C++ version of Facebook Research fastText](https://github.com/linkfluence/fastText4j)
  * [scikit-learn wrappers for Python fastText](https://github.com/shaypal5/skift)
  * [FastText Tutorial - How to Classify Text with FastText](https://www.youtube.com/watch?v=4l_At3oalzk)
* [go-freeling - Golang Natural Language Processing](https://github.com/advancedlogic/go-freeling)
* [hangul-toolkit - 한글 자모 분해, 조합(오토마타), 조사 붙이기, 초/중/종 분해조합, 한글/한자/영문 여부 체크 등을 지원](https://github.com/bluedisk/hangul-toolkit)
* [InferSent - semantic sentence 표현을 제공하는 sentence embedding 방법](https://www.facebook.com/nextobe1/posts/341533606282641)
* [Kanji recognition - implementation of Nei Kato's directional feature extraction algorithm](https://github.com/bitbanger/gogaku)
* [Kiwi - 지능형 한국어 형태소 분석기(Korean Intelligent Word Identifier)](https://github.com/bab2min/Kiwi)
  * [좋아, 형태소 분석기를 만들어봅시다. - 0](http://bab2min.tistory.com/560)
  * [좋아, 형태소 분석기를 만들어봅시다. - 1](http://bab2min.tistory.com/561)
  * [좋아, 형태소 분석기를 만들어봅시다. - 2](http://bab2min.tistory.com/562)
  * [좋아, 형태소 분석기를 만들어봅시다. - 3](http://bab2min.tistory.com/563)
  * [지능형 한국어 형태소 분석기 ver 0.2](http://bab2min.tistory.com/571)
  * [지능형 한국어 형태소 분석기 ver 0.3 - 알고리즘 최적화 & 메모리 풀](http://bab2min.tistory.com/572)
  * [지능형 한국어 형태소 분석기 0.4버전 업데이트](http://bab2min.tistory.com/580)
* [knwl - A Javascript Natural Language Parser](http://loadfive.com/os/knwl/)
* [KoalaNLP = Korean + Scala + NLP. 한국어 형태소 및 구문 분석기의 모음입니다](https://github.com/nearbydelta/KoalaNLP)
* Mecab
  * [Taku Kudo - Mecab developer](http://chasen.org/~taku/index.html.en)
  * [mecab-ko 윈도우에서 빌드하기](http://legendfinger.tistory.com/625)
* [Memory Networks](https://github.com/facebook/MemNN)
* [mit-nlp](https://github.com/mit-nlp)
* [NGT - Neighborhood Graph and Tree for Indexing High-dimensional Data](https://github.com/yahoojapan/NGT/blob/master/README.md)
  * word embeddings와 같은 고차원 데이터에서 k nearest item을 근사적으로 빠르게 찾는 라이브러리
  * annoy와 비슷하지만 graph tree 기반 indexing
* [nlg-eval - Evaluation code for various unsupervised automated metrics for Natural Language Generation](https://github.com/Maluuba/nlg-eval)
* [Rouzeta - 유한 상태 기반의 한국어 형태소 분석기](https://shleekr.github.io/)
  * [유한 상태 기반의 한국어 형태소 분석기](http://readme.skplanet.com/?p=13166)
  * [유한 상태 기반의 한국어 형태소 분석기](https://shleekr.github.io/2016/06/30/introducing-rouzeta/)
* [SentencePiece - an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training](https://github.com/google/sentencepiece)
* [Simplenlg - a simple Java API designed to facilitate the generation of Natural Language](https://github.com/delver/simplenlg)
* [Stanford Natural Language Processing Group](http://nlp.stanford.edu/software/corenlp.shtml)
  * [corenlp](http://corenlp.run/)
  * [Stanford CoreNLP – a suite of core NLP tools](http://stanfordnlp.github.io/CoreNLP/)
  * [nlp.stanford.edu/teaching](https://nlp.stanford.edu/teaching/)
* [StarSpace - Learning embeddings for classification, retrieval and ranking](https://github.com/facebookresearch/Starspace)
  * [Embed All The Things](https://www.facebook.com/groups/PyTorchKR/permalink/967337433405938/)
* [tacit - Text Analysis,Collection and Interpretation Tool](http://tacit.usc.edu/)
* [Text Understanding from Scratch](http://arxiv.org/abs/1502.01710)

## Java
* [lucene-Korean-Analyzer Lucene Analyzer For Korean](https://github.com/need4spd/lucene-Korean-Analyzer)
  * [03. Solr 5.0.0 - 아리랑(arirang) 한글 형태소 분석기 적용](http://juncon.tistory.com/8)
* [한글 받침에따라서 '을/를' 구분하기](http://gun0912.tistory.com/65)
* [VWL 텍스트 분석기 0.9](http://www.vw-lab.com/31)
* [Autocomplete words with spring boot and redis](https://github.com/okihouse/spring-boot-redis-auto-complete) 자동완성

## JavaScript
* [TajaJS is a simple Hangul library in JavaScript](https://github.com/linterpreteur/taja.js)

## Python
* **[13 Deep Learning Frameworks for Natural Language Processing in Python](https://medium.com/@datamonsters/13-deep-learning-frameworks-for-natural-language-processing-in-python-2b84a6b6cd98)**
* [Annoy (Approximate Nearest Neighbors Oh Yeah) - a C++ library with Python bindings to search for points in space that are close to a given query point](https://github.com/spotify/annoy)
  * [Approximate nearest neighbor methods and vector models – NYC ML meetup](http://www.slideshare.net/erikbern/approximate-nearest-neighbor-methods-and-vector-models-nyc-ml-meetup)
  * [Approximate Nearest Neighbors](https://brunch.co.kr/@goodvc78/15)
* [Document Clustering with Python](http://brandonrose.org/clustering)
* [Mining English and Korean text with Python](http://www.lucypark.kr/courses/2015-ba/text-mining.html)
* [NLTK](http://www.nltk.org)
  * [book](http://www.nltk.org/book/)
  * [한국어와 NLTK, Gensim의 만남](http://www.slideshare.net/lucypark/nltk-gensim)
  * [NLP with NLTK – Part 1](https://opendatascience.com/blog/nlp-with-nltk-part-1/)
  * [python_nltk](https://github.com/zerosum99/python_nltk)
  * [github.com/zerosum99/python_nltk](https://github.com/zerosum99/python_nltk)
  * [NLTK로 배우는 자연언어처리](http://m.blog.naver.com/PostList.nhn?blogId=bcj1210&categoryNo=10&listStyle=style1)
  * [Tutorial 5: Analyzing text using Python NLTK](https://www.youtube.com/watch?v=Flpj_D8b1Vg)
  * [NLTK Basic Text Analytics](https://www.youtube.com/watch?v=AKcxEfz-EoI)
  * [NLTK with Python 3 for Natural Language Processing](https://www.youtube.com/playlist?list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL)
  * [22 Python NLTK Corpus](https://www.youtube.com/watch?v=pMv-rxqBhsY)
  * [NLTK Text Processing Tutorial Series](https://www.youtube.com/playlist?list=PLcTXcpndN-Sl9eYrKM6jtcOTgC52EJnqH)
  * [Computing Document Similarity with NLTK (March 2014)](https://www.youtube.com/watch?v=FfLo5OHBwTo)
	* [Tokenizing Words Sentences with Python NLTK](https://www.youtube.com/watch?v=A5n7tsZctwM)
* [Hangulize - 외래어 자동 한글 변환 모듈](https://github.com/sublee/hangulize)
* [How to create a text mining algorithm with Python](http://breakoutroom.co/v/641)
* [keystroke practice](https://github.com/hyunjun/practice/tree/master/python/keystroke)
* [Keyword finder: automatic keyword extraction from text](http://blog.urx.com/urx-blog/2015/10/13/keyword-finder-automatic-keyword-extraction-from-text)
* [KoNLPy: Korean NLP in Python](http://konlpy.org/)
  * [github.com/konlpy/konlpy](https://github.com/konlpy/konlpy)
  * [자바, 미안하다! 파이썬 한국어 NLP](http://www.slideshare.net/lucypark/py-con-2014-38531830)
  * [자바, 미안하다! Korean NLP with Python](https://www.lucypark.kr/slides/2014-pyconkr)
  * [word2vec을 하기 앞서 형태소 분석을 해보자](http://blog.naver.com/pjt3591oo/220967117096)
  * [Pycon2017 koreannlp](https://www.slideshare.net/kimhyunjoonglovit/pycon2017-koreannlp)
  * [customized KoNLPy](https://github.com/lovit/customized_konlpy)
* [krtpy - Korean Romanization/Hangulization utility written in python](https://github.com/danrasband/krtpy)
* [ParlAI (pronounced “par-lay”) - a framework for dialog AI research, implemented in Python](https://github.com/facebookresearch/ParlAI/blob/master/README.md)
  * [ParlAI: A new software platform for dialog research](https://code.facebook.com/posts/266433647155520/parlai-a-new-software-platform-for-dialog-research)
* [py-hanspell - 파이썬 한글 맞춤법 검사 라이브러리. (네이버 맞춤법 검사기 사용)](https://github.com/ssut/py-hanspell)
* [PyStruct - Structured Learning in Python](https://pystruct.github.io)
* [Python-jamo is a Python Hangul syllable decomposition and synthesis library for working with Hangul characters and jamo](https://github.com/JDongian/python-jamo)
* [spaCy is a library for industrial-strength natural language processing in Python and Cython](http://spacy.io/)
  * [spaCy: Industrial-strength NLP](https://github.com/explosion/spaCy)
  * [dependency parse tree visualization](http://spacy.io/displacy/)
  * [Neural coref - State-of-the-art coreference resolution based on neural nets and spaCy](https://github.com/huggingface/neuralcoref)
    * 신경망과 spaCy를 이용한 coreference resolution library
    * [State-of-the-art neural coreference resolution for chatbots](https://medium.com/huggingface/state-of-the-art-neural-coreference-resolution-for-chatbots-3302365dcf30)
  * [Prodigy: A new tool for radically efficient machine teaching](https://explosion.ai/blog/prodigy-annotation-tool-active-learning)
* [TextBlob Sentiment: Calculating Polarity and Subjectivity](http://planspace.org/20150607-textblob_sentiment/) python
  * [Natural Language Basics with TextBlob](http://rwet.decontextualize.com/book/textblob/)
* [Text Generation With LSTM Recurrent Neural Networks in Python with Keras](http://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)
* [textgenrnn - Python module to easily generate text using a pretrained character-based recurrent neural network](https://github.com/minimaxir/textgenrnn)
* [Vowpal Wabbit is a machine learning system which pushes the frontier of machine learning with techniques such as online, hashing, allreduce, reductions, learning2search, active, and interactive learning. http://hunch.net/~vw/](https://github.com/JohnLangford/vowpal_wabbit)
  * [vwnlp - Solving NLP problems with Vowpal Wabbit: Tutorial and more](https://github.com/hal3/vwnlp)

## R
* [KoNLP - R package for Korean NLP http://cran.r-project.org/web/packages/KoNLP/index.html](https://github.com/haven-jeon/KoNLP)
  * [KoNLP v.0.80.0 버전 업(on CRAN now)](http://freesearch.pe.kr/archives/4520)
  * [한글 언어 자원과 R: KoNLP 개선과 활용](http://www.slideshare.net/r-kor/r-konlp)
  * [KoNLPer - KoNLP 결과를 보내주는 flask with r 서버 dockerize http://konlper.duckdns.org/list](https://github.com/mrchypark/KoNLPer)

## Scala
* [Open Korean Text Processor - An Open-source Korean Text Processor](https://github.com/open-korean-text/open-korean-text)
* [twitter-korean-text - 트위터에서 만든 한국어 처리기](https://github.com/twitter/twitter-korean-text)

# LSA
* [잠재 디리클레 할당](http://ko.wikipedia.org/wiki/%EC%9E%A0%EC%9E%AC_%EB%94%94%EB%A6%AC%ED%81%B4%EB%A0%88_%ED%95%A0%EB%8B%B9)
* [A Solution to Plato's Problem: The Latent Semantic Analysis Theory of Acquisition, Induction and Representation of Knowledge](http://lsa.colorado.edu/papers/plato/plato.annote.html)
* [Latent Semantic Variable Models](http://videolectures.net/slsfs05_hofmann_lsvm/)
* [Word vectors using LSA, Part - 2](http://www.vikasing.com/2015/05/word-vectors-using-lsa-part-2.html)
* [Sentence Embedding](https://medium.com/towards-data-science/sentence-embedding-3053db22ea77)

# LSH
* [LSH( Locality sensitive hashing )](http://m.blog.daum.net/_blog/_m/articleView.do?blogid=02ONK&articleno=13627840)

# Named Entity
* [Named Entity Recognition: Examining the Stanford NER Tagger](http://blog.urx.com/urx-blog/2015/7/28/named-entity-recognition-examining-the-stanford-ner-tagger)
* [한국어 개체명 인식 기술(Named Entity Recognition)](http://swguru.kr/48)
* [K-ICT 빅데이터센터](https://kbig.kr/)
* [NeuroNER - A Named-Entity Recognition Program based on Neural Networks and Easy to Use](http://neuroner.com/)

# News
* [“포털 야구 중계, 로봇 저널리즘이 대체 가능해“](http://www.bloter.net/archives/227030)
  * [이 기사는 로봇이 썼을까, 기자가 썼을까](http://www.bloter.net/archives/227482)
* [③로봇, 저널리즘을 넘보다](http://www.bloter.net/archives/232289)
* [마커, “뉴스, 다 읽지 마세요. 형광펜 처리된 중요한 부분만 보세요”](http://besuccess.com/2015/05/%EB%B9%84%EA%B8%80%EB%A1%9C%EB%B2%8C-%EC%8A%A4%ED%83%80%ED%8A%B8%EC%97%85-%EB%B0%B0%ED%8B%80-%EB%A7%88%EC%BB%A4-%EB%89%B4%EC%8A%A4-%EB%8B%A4-%EC%9D%BD%EC%A7%80-%EB%A7%88%EC%84%B8%EC%9A%94/)
* [“수 없이 쏟아지는 읽을거리, 중요한 것만 밑줄 쳐 드립니다”, 마커 정철현 대표](http://besuccess.com/2015/07/marker-2/)
* [‘뉴욕타임스’, 머신러닝 기반 자동 태그 시스템 개발](http://www.bloter.net/archives/234850)
* [지난 26년간 언론에서 가장 중요한 정보원은 누구였을까?](http://slownews.kr/53460)
* [세월호 참사 1년 동안의 언론보도를 통해 드러난 언론매체의 정치적 경도](http://jhp.snu.ac.kr/sewol.html)
* [세월호 참사 1년 동안의 언론보도를 통해 드러난 언론매체의 정치적 경도](http://jhp.snu.ac.kr/sewol.html)
* [왜 언론사는 채팅봇에 흥분하는가](http://www.bloter.net/archives/254532)
* [뉴스 빅데이터 분석 시스템 ‘빅카인즈’ 공식 출범](http://www.bloter.net/archives/254773)
* [네이버 뉴스 댓글 ‘남성’ 많고 ’10대·여성’ 적고](http://www.bloter.net/archives/260384)
* [뉴스를 재미있게 만드는 방법; 뉴스잼](https://www.pycon.kr/2016apac/program/1)
  * [김경훈: 뉴스를 재미있게 만드는 방법; 뉴스잼 - PyCon APAC 2016](https://www.youtube.com/watch?v=Txj4MzqL_Mk&feature=youtu.be)
  * [20160813, PyCon2016APAC 뉴스를 재미있게 만드는 방법; 뉴스잼](http://www.slideshare.net/koorukuroo/20160813-pycon2016apac)
* [‘2억9천만원 아파트’ 기사에 달린 댓글로 본 사회학](http://www.vw-lab.com/23)
* [Google starts highlighting fact-checks in News](https://techcrunch.com/2016/10/13/google-helps-speak-truth-to-power-adds-prominence-to-fact-checks-in-news/)
* [Extract News In Three Words Using Triples](http://www.cytora.com/insights/2016/11/21/extracting-information-from-natural-language-using-triples)
* [factcheck.snu.ac.kr](http://factcheck.snu.ac.kr/)
* [컴퓨테이셔널 저널리즘](http://www.bloter.net/archives/276095)

# Ontology
* [Protege Ontology Library](http://protegewiki.stanford.edu/wiki/Protege_Ontology_Library)
* [Disease Ontology](http://www.disease-ontology.org/)
* [SNOMED CT](http://en.wikipedia.org/wiki/SNOMED_CT)
* [jena Ontology API와 sparQL을 사용하여 검색시스템 만들기](http://cholol.tistory.com/225)

# Paper
* [57 SUMMARIES OF MACHINE LEARNING AND NLP RESEARCH](http://www.marekrei.com/blog/paper-summaries/)

# Parser
* [Announcing SyntaxNet: The World’s Most Accurate Parser Goes Open Source](http://googleresearch.blogspot.kr/2016/05/announcing-syntaxnet-worlds-most.html?m=1)
  * [SyntaxNet: 텐서플로우 NLP](https://tensorflow.blog/2016/05/13/syntaxnet-%ED%85%90%EC%84%9C%ED%94%8C%EB%A1%9C%EC%9A%B0-nlp/)
  * [Google 자연어 처리 오픈소스 SyntaxNet 공개](http://cpuu.postype.com/post/166917/)
  * [Google SyntaxNet 설치하기(Ubuntu / Mac)](http://cpuu.postype.com/post/197684/)
  * [SyntaxNet in context: Understanding Google's new TensorFlow NLP model](https://spacy.io/blog/syntaxnet-in-context)
  * [Structured Training for Neural Network Transition-Based Parsing](http://www.petrovi.de/data/acl15.pdf)
  * [github.com/dsindex/syntaxnet](https://github.com/dsindex/syntaxnet)
  * [github.com/dsindex/parsing-syntaxnet](https://github.com/dsindex/blog/wiki/%5Bparsing%5D-SyntaxNet)
  * [github.com/krikit/syntaxnet](https://github.com/krikit/syntaxnet)
  * [An Upgrade to SyntaxNet, New Models and a Parsing Competition](https://research.googleblog.com/2017/03/an-upgrade-to-syntaxnet-new-models-and.html)
* [Grammatical Framework - A programming language for multilingual grammar applications](http://www.grammaticalframework.org/)
* [Syntactic Parsing of Web Queries with Question Intent](https://research.yahoo.com/publications/8709/syntactic-parsing-web-queries-question-intent)
  * [Novel Modeling of Syntactic Parsing for Web Queries](https://yahooresearch.tumblr.com/post/145926804326/novel-modeling-of-syntactic-parsing-for-web)
  * [The Yahoo Query Treebank, V. 1.0](http://arxiv.org/pdf/1605.02945v2.pdf)
  * [Language Data - Yahoo Answers Query Treebank, version 1.0](http://webscope.sandbox.yahoo.com/catalog.php?datatype=l&did=79)
* [Phoenix Server - a Galaxy-wrapped version of the Phoenix robust semantic CFG parser](http://wiki.speech.cs.cmu.edu/olympus/index.php/Phoenix)
* [SLING: A Natural Language Frame Semantic Parser](https://research.googleblog.com/2017/11/sling-natural-language-frame-semantic.html)

# QA Question Answer
* [SQuAD - The Stanford Question Answering Dataset](https://stanford-qa.com/)
  * [BiDAF - Bi-Directional Attention Flow for Machine Comprehension](https://allenai.github.io/bi-att-flow/)
  * [SQuAD - The Stanford Question Answering Dataset](https://rajpurkar.github.io/SQuAD-explorer/)
* [www.facebook.com/groups/AIKoreaOpen/permalink/1207284209305687](https://www.facebook.com/groups/AIKoreaOpen/permalink/1207284209305687)
  * [Query-Regression Networks](https://github.com/seominjoon/qrn)
* [carpedm20.github.io](http://carpedm20.github.io/)
* [Implementation of Dynamic memory networks by Kumar et al. http://arxiv.org/abs/1506.07285](https://github.com/YerevaNN/Dynamic-memory-networks-in-Theano)
* [Implementation of the Convolution Neural Network for factoid QA on the answer sentence selection task](https://github.com/aseveryn/deep-qa)
* [Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus](https://arxiv.org/abs/1603.06807)
* [Deep Language Modeling for Question Answering using Keras](http://benjaminbolte.com/blog/2016/keras-language-modeling.html)
* [Deep Language Modeling for Question Answering using Keras](http://ben.bolte.cc/blog/2016/keras-language-modeling.html)
* [FRDF Frame Semantic-based QA system](https://machinereading.github.io/FRDF/)
  * [FRDF: Frame-semantic-based QA system](https://github.com/machinereading/FRDF)
* [gotquestions.org](http://www.gotquestions.org/)
* [OKBQA Home](http://www.okbqa.org/)
* [KBQA: An Online Template Based Question Answering System over Freebase](https://www.ijcai.org/Proceedings/16/Papers/640.pdf)
* [KBQA: Learning Question Answering over QA Corpora and Knowledge Bases](http://www.vldb.org/pvldb/vol10/p565-cui.pdf)
* [Question Answering System using Multiple Information Source and Open Type](http://www.aclweb.org/anthology/N15-3023) Answer Merge
* [qald.sebastianwalter.org](http://qald.sebastianwalter.org/)
* [SearchQA](https://github.com/nyu-dl/SearchQA)
* [START - Natural Language Question Answering System](http://start.csail.mit.edu/)
* [TriviaQA: A Large Scale Dataset for Reading Comprehension and Question Answering](http://nlp.cs.washington.edu/triviaqa/)
* [Reading Wikipedia to Answer Open-Domain Questions](https://research.fb.com/publications/reading-wikipedia-to-answer-open-domain-questions/)
* [SIGIR2017에서 발표한 RNN을 이용한 자연어 질의 변환](http://blog.naver.com/naver_search/221066184768)
* [PR-037: Ask me anything: Dynamic memory networks for natural language processing](https://www.youtube.com/watch?v=oxSrjuspQEs&feature=youtu.be)
* [Learning to reason by reading text and answering questions](https://www.youtube.com/watch?v=r0veZ_WV0sA&feature=youtu.be)
  * [Learning to reason by reading text and answering questions](https://www.slideshare.net/NaverEngineering/learning-to-reason-by-reading-text-and-answering-questions/1)
* [강화학습 기반 QA 시스템 - 김영삼](https://www.youtube.com/watch?v=jw-Am-sPTsY&feature=youtu.be)
* [MRQA 2018: Machine Reading for Question Answering](https://mrqa2018.github.io/)

# Sentiment
* [A comparison of open source tools for sentiment analysis](http://fotiad.is/blog/sentiment-analysis-comparison/)
* [감정어휘 평가사전과 의미마디 연산을 이용한 영화평 등급화 시스템](http://clab.snu.ac.kr/arssa/lib/exe/fetch.php?media=ks_sa_2010_1.pdf)
  * [감정어휘 평가사전 1.0](http://clab.snu.ac.kr/arssa/doku.php?id=app_dict_1.0)
* [TextBlob Sentiment: Calculating Polarity and Subjectivity](http://planspace.org/20150607-textblob_sentiment/) python
  * [Natural Language Basics with TextBlob](http://rwet.decontextualize.com/book/textblob/)
* [Modern Methods for Sentiment Analysis](https://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis)
* [LSTM Networks for Sentiment Analysis](http://deeplearning.net/tutorial/lstm.html)
* [Sentiment Analysis using LSTM network](https://codeburst.io/sentiment-analysis-using-lstm-network-a9a28682d3e6)
* [KOrean Sentiment Analysis Corpus, KOSAC](http://word.snu.ac.kr/kosac/)
* [Naver sentiment movie corpus v1.0](https://github.com/e9t/nsmc)
* [The emotional arcs of stories are dominated by six basic shapes](http://arxiv.org/abs/1606.07772)
  * [컴퓨터가 분석한 6가지 이야기 유형](http://newspeppermint.com/2016/07/24/m-stories/)
    * [The emotional arcs of stories are dominated by six basic shapes](http://arxiv.org/abs/1606.07772)
* [dracula.sentimentron.co.uk/sentiment-demo](http://dracula.sentimentron.co.uk/sentiment-demo/)
* [Sentiment Analysis and Aspect classification for Hotel Reviews](https://github.com/monkeylearn/hotel-review-analysis)
* [Exploring Sentiment in Literature with Deep Learning](https://medium.com/@awjuliani/exploring-sentiment-in-literature-with-deep-learning-30366aff578e)
* [Learning when to skim and when to read](https://metamind.io/research/learning-when-to-skim-and-when-to-read)
* [감성분석 API](http://api.openhangul.com/overview?m=sentiment)
* [Sentiment analysis on Twitter using word2vec and keras](http://ahmedbesbes.com/sentiment-analysis-on-twitter-using-word2vec-and-keras.html)
  * [Sentiment analysis on forum articles using word2vec and Keras](https://nbviewer.jupyter.org/github/likejazz/jupyter-notebooks/blob/master/sentimental-analysis-word2vec-keras.ipynb)
* [TWITTER SENTIMENT ANALYSIS USING COMBINED LSTM-CNN MODELS](http://konukoii.com/blog/2018/02/19/twitter-sentiment-analysis-using-combined-lstm-cnn-models/)
* [한국어 감성 분석기](https://github.com/mrlee23/KoreanSentimentAnalyzer)
* [How to Develop an N-gram Multichannel Convolutional Neural Network for Sentiment Analysis](https://machinelearningmastery.com/develop-n-gram-multichannel-convolutional-neural-network-sentiment-analysis)

# Similarity
* [Analyzing stylistic similarity amongst authors A quantitative comparison of writing styles in 12,590 books from Project Gutenberg](http://markallenthornton.com/blog/stylistic-similarity/)
* [Correlation and dependence](https://en.wikipedia.org/wiki/Correlation_and_dependence)
* [faiss - A library for efficient similarity search and clustering of dense vectors](https://github.com/facebookresearch/faiss)
* [Fuzzy string matching using cosine similarity](http://blog.nishtahir.com/2015/09/19/fuzzy-string-matching-using-cosine-similarity/)
* [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index)
* [Most frequent k characters](https://en.wikipedia.org/wiki/Most_frequent_k_characters)
* [Mutual information](https://en.wikipedia.org/wiki/Mutual_information)
* [Similarity measure](https://en.wikipedia.org/wiki/Similarity_measure)
* [Simple matching coefficient](https://en.wikipedia.org/wiki/Simple_matching_coefficient)
* [Sørensen–Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient)
* [Tversky index](https://en.wikipedia.org/wiki/Tversky_index)
* [FIVE MOST POPULAR SIMILARITY MEASURES IMPLEMENTATION IN PYTHON](https://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/)
* **[Vector_Similarity - Python, Java implementation of TS-SS called from "A Hybrid Geometric Approach for Measuring Similarity Level Among Documents and Document Clustering"](https://github.com/taki0112/Vector_Similarity)**
* [MinHash Tutorial with Python Code](http://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/)
* [Vector_Similarity](https://github.com/taki0112/Vector_Similarity/blob/master/README.md)
* [NMF 알고리즘을 이용한 유사한 문서 검색과 구현(1/2)](http://bcho.tistory.com/1216) matrix factorization
* [NMF 알고리즘을 이용한 유사 문서 검색과 구현(2/2) sklearn을 이용한 구현](http://bcho.tistory.com/1220)
* [String Matching and Database Merging](https://medium.com/@javiergb_com/string-matching-and-database-merging-9ef9b4f7fea4) Machine Learning to compare and join heterogeneous data from heterogeneous sources
* [Brain's Pick: 단어 간 유사도 파악 방법](https://brunch.co.kr/@kakao-it/189)
  * [ling.kakaobrain.com/wordweb](http://ling.kakaobrain.com/wordweb)

# Summarize
* [Automatic summarization](https://en.wikipedia.org/wiki/Automatic_summarization)
* [summariz3](http://theeluwin.tumblr.com/post/146188165713/summariz3)
* [Text summarization with TensorFlow](https://tensorflowkorea.wordpress.com/2016/08/25/text-summarization-with-tensorflow/)
* [How to Run Text Summarization with TensorFlow](https://medium.com/@surmenok/how-to-run-text-summarization-with-tensorflow-d4472587602d)
* [Text summarization with TensorFlow](https://research.googleblog.com/2016/08/text-summarization-with-tensorflow.html)
  * [github.com/tensorflow/models/textsum](https://github.com/tensorflow/models/tree/master/textsum)
  * [sequence-to-sequence Learning](http://arxiv.org/abs/1409.3215)
  * dataset [A Neural Attention Model for Abstractive Sentence Summarization](https://arxiv.org/abs/1509.00685)
* [NDC 2017 마이크로토크 - 프로그래머가 뉴스 읽는 법](https://www.slideshare.net/suminb/ndc-2017-how-programmers-read-news)
* [tldr - Text summarization service](https://github.com/suminb/tldr)
* [24 A Serious NLP Application Text Auto Summarization using Python](https://www.youtube.com/watch?v=NDPyjZkblJc)

# Spark
* [Natural Language Processing With Apache Spark](https://dzone.com/articles/in-progress-natural-language-processing)
* [Introducing the Natural Language Processing Library for Apache Spark](https://databricks.com/blog/2017/10/19/introducing-natural-language-processing-library-apache-spark.html)
  * [spark-nlp - Natural Language Understanding Library for Apache Spark](https://github.com/johnsnowlabs/spark-nlp)
* [Deep learning text NLP and Spark Collaboration . 한글 딥러닝 Text NLP & Spark](https://www.slideshare.net/ssusere94328/deep-learning-text-nlp-and-spark-collaboration-text-nlp-spark)
* [Deep Learning and NLP with Spark by Andy Petrella and Melanie Warrick](https://www.youtube.com/watch?v=vaiE2yGdJSg)
* [Classifying Text in Money Transfers with Apache Spark - Jose A. Rodriguez-Serrano](https://www.youtube.com/watch?v=sqKQdEE2cvY)
* [Deep Learning and NLP with Spark - by Andy Petrella](https://www.youtube.com/watch?v=kHgttIwWcaE)
* [SF Scala, David Hall, ScalaNLP Epic](https://www.youtube.com/watch?v=rpfVtRqQ4_o)
* [Natural Language Processing with CNTK and Apache Spark - Ali Zaidi](https://www.youtube.com/watch?v=vArImMkvlGM)
* [Text By the Bay 2015: Marek Kolodziej, Unsupervised NLP Tutorial using Apache Spark](https://www.youtube.com/watch?v=pIMs946Eu2U)

# Speller
* [How to Write a Spelling Corrector](http://norvig.com/spell-correct.html)
  * [철자 교정기 작성하기](http://theyearlyprophet.com/spell-correct.html)
* [Deep Spelling](https://medium.com/@majortal/deep-spelling-9ffef96a24f6)
* [How to Strike a Match](http://www.catalysoft.com/articles/strikeamatch.html)
* [파이썬으로 네이버 맞춤법 검사하기](https://festi.kr/blog/924384d4-1059-4093-acc0-ff48fbacf7f1/)
* [한글 검색 질의어 오타 패턴 분석과 사용자 로그를 이용한 질의어 오타 교정 시스템 구축](http://www.slideshare.net/gogamza/ss-6265729)
* [사쿠라 훈민정음](http://terms.naver.com/list.nhn?cid=41818&categoryId=41818&so=st4.asc)
* **[Word Prediction using Convolutional Neural Networks](https://github.com/Kyubyong/word_prediction)**

# TextRank
* **[TextRank를 이용한 문서요약](http://excelsior-cjh.tistory.com/entry/TextRank%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EB%AC%B8%EC%84%9C%EC%9A%94%EC%95%BD)**
* [TextRank for Korean](https://github.com/theeluwin/textrankr)
* [LexRank for Korean](https://github.com/theeluwin/lexrankr)
* [NDC 2017 마이크로토크 - 프로그래머가 뉴스 읽는 법](https://www.slideshare.net/suminb/ndc-2017-how-programmers-read-news)
* [Text Summarization with Gensim](https://rare-technologies.com/text-summarization-with-gensim/) gensim의 textrank
* [textacy: higher-level NLP built on spaCy](https://github.com/chartbeat-labs/textacy) text analysis based on spaCy
* [python-rake](https://pypi.python.org/pypi/python-rake) 키워드 추출 패키지

# TFIDF
* [TFIDF In Java](https://github.com/taki0112/TFIDF_Java)
* [The fastest way to identify keywords in news articles — TFIDF with Wikipedia (Python version)](https://medium.com/@adam.chin/the-fastest-way-to-identify-keywords-in-news-articles-tfidf-with-wikipedia-python-version-baf874d7eb16)
* [Machine Learning with Text - TFIDF Vectorizer MultinomialNB Sklearn (Spam Filtering example Part 2)](https://www.youtube.com/watch?v=bPYJi1E9xeM)

# Translation
* [Introduction to Neural Machine Translation with GPUs (Part 1)](http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-with-gpus/)
* [Introduction to Neural Machine Translation with GPUs (Part 2)](http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-2/)
* [Introduction to Neural Machine Translation with GPUs (part 3)](http://devblogs.nvidia.com/parallelforall/introduction-neural-machine-translation-gpus-part-3/)
* [문자 단위의 Neural Machine Translation](http://www.slideshare.net/naver-labs/27-hclt-neural-machine-traslation)
* [Jointly Modeling Embedding and Translation to Bridge Video and Language](https://drive.google.com/file/d/0B-ZSzubU1u02TDZKU1Q0akxUM0k/view)
* [Tips on Building Neural Machine Translation Systems](https://github.com/neubig/nmt-tips/blob/master/README.md)
  * [Subword Neural Machine Translation](https://github.com/rsennrich/subword-nmt)
* [Machine Learning is Fun Part 5: Language Translation with Deep Learning and the Magic of Sequences](https://medium.com/@ageitgey/machine-learning-is-fun-part-5-language-translation-with-deep-learning-and-the-magic-of-sequences-2ace0acca0aa)
* [Google's Neural Machine Translation System](https://norman3.github.io/papers/docs/google_neural_machine_translation)
* [Peeking into the neural network architecture used for Google's Neural Machine Translation](http://smerity.com/articles/2016/google_nmt_arch.html)
* [Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation](https://arxiv.org/abs/1611.04558) 여러 언어를 동시에 번역하도록 학습했더니 한번도 학습에 사용한 적이 없는 언어쌍에 대해서도 번역이 가능
* [ZERO-SHOT LEARNING FOR VISION AND MULTIMEDIA](https://staff.fnwi.uva.nl/t.e.j.mensink/zsl2016/)
* [gtbot - 구글 번역 API를 이용한 슬랙 번역 봇입니다](https://github.com/qodot/gtbot)
* [GNMT로 알아보는 신경망 기반 기계번역](https://drive.google.com/file/d/0B_Ekt7icI0htZGlpWVpwODJSOVU/view)
* [OpenNMT - a industrial-strength, open-source (MIT) neural machine translation system utilizing the Torch mathematical toolkit](http://opennmt.net/)
  * [Open-Source Neural Machine Translation in PyTorch http://opennmt.net/](https://github.com/OpenNMT/OpenNMT-py)
* [nmtpy - a suite of Python tools, primarily based on the starter code provided in github.com/nyu-dl/dl4mt-tutorial for training neural machine translation networks using Theano.](https://github.com/lium-lst/nmtpy)
* [py-googletrans - (unofficial) Googletrans: Free and Unlimited Google translate API for Python. Translates totally free of charge](https://github.com/ssut/py-googletrans)
* [onlinedoctranslator.com](https://www.onlinedoctranslator.com/) 구글 api를 사용해 만든 번역 서비스
* [Deep Learning Takes on Translation](https://cacm.acm.org/magazines/2017/6/217734-deep-learning-takes-on-translation/fulltext)
* [TensorFlow에서 나만의 신경 기계 번역 시스템 구축](https://www.facebook.com/nextobe1/posts/345585482544120)
* [Learned in translation: contextualized word vectors](https://einstein.ai/research/learned-in-translation-contextualized-word-vectors)
* [OpenSubtitles2016](http://opus.lingfil.uu.se/OpenSubtitles2016.php)
* [카카오번역기가 양질의 대규모 학습 데이터를 확보하는 방법](https://brunch.co.kr/@kakao-it/154)
* [신경망 번역 모델의 진화 과정](https://brunch.co.kr/@kakao-it/155)
* [Machine Translation Without the Data](https://buzzrobot.com/machine-translation-without-the-data-21846fecc4c0)
* [How to Configure an Encoder-Decoder Model for Neural Machine Translation](https://machinelearningmastery.com/configure-encoder-decoder-model-neural-machine-translation)
* [Neural Korean to English Machine Translater with Gluon](https://github.com/haven-jeon/ko_en_neural_machine_translation)
* [신경망 한영 번역기 코드 공개](http://freesearch.pe.kr/archives/4741)

# Twitter
* [Analyzing Twitter Part 1](http://rohankshir.github.io/2015/06/30/analyzing-twitter-part-1/)
* [Analyzing Twitter Part 2](http://rohankshir.github.io/2015/10/30/analyzing-twitter-part-2/)
* [Analyzing Twitter Part 3](http://rohankshir.github.io/2015/12/25/analyzing-twitter-part-3/)

# Voice
* [THE COMPUTERS ARE LISTENING HOW THE NSA CONVERTS SPOKEN WORDS INTO SEARCHABLE TEXT](https://firstlook.org/theintercept/2015/05/05/nsa-speech-recognition-snowden-searchable-text/)
* [Hound Internal Demo](https://www.youtube.com/watch?v=M1ONXea0mXg)
  * [숨쉬기 힘들 때까지 말해도…놀라운 음성인식엔진](http://techholic.co.kr/archives/35360)
* [“음성인식 기술로 만화 주인공과 대화 나눠요”](http://www.bloter.net/archives/234697)
* [Google voice search: faster and more accurate](https://research.googleblog.com/2015/09/google-voice-search-faster-and-more.html)
* [WaveNet: A Generative Model for Raw Audio](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)
  * [인간처럼 톤․억양 재현한 음성을…](http://techholic.co.kr/archives/60333)
* [Baidu Deep Voice explained Part 2 — Training](https://medium.com/athelas/baidu-deep-voice-explained-part-2-training-810e87d20047)
* [Neural Voice Cloning with a Few Samples](http://research.baidu.com/neural-voice-cloning-samples/)
* [Kaldi Speech Recognition Toolkit](https://github.com/kaldi-asr/kaldi)
  * [Kaldi asr(automatic speech recognition) 음성인식 오픈소스 라이브러리 사용법 및 예제 정리](https://github.com/Gyubin/TIL/blob/master/data_science/kaldi_asr.md)
* [Tutorial: Asynchronous Speech Recognition in Python](https://medium.com/towards-data-science/tutorial-asynchronous-speech-recognition-in-python-b1215d501c64)
* [책 읽어주는 딥러닝: 배우 유인나가 해리포터를 읽어준다면 DEVIEW 2017](https://www.slideshare.net/carpedm20/deview-2017-80824162)
  * [Multi-speaker Tacotron in TensorFlow. 오픈소스 딥러닝 다중 화자 음성 합성 엔진. http://carpedm20.github.io/tacotron](https://github.com/devsisters/multi-speaker-tacotron-tensorflow)
* [카카오미니는 목소리를 어떻게 인식할까?](https://brunch.co.kr/@kakao-it/135)
* [AudioSet - A massive dataset of manually annotated audio events](https://research.google.com/audioset/)
* [SPEECH TO TEXT(STT) 라이브러리와 프로세싱을 이용하여 음성인식 테스트하기](http://kocoafab.cc/tutorial/view/608)
* [Getting robots to understand speech: Using Watson’s Natural Language Classifier service](https://www.ibm.com/blogs/watson/2016/08/getting-robots-understand-speech-using-watsons-natural-language-classifier-service/)
* [Mozilla, 음성데이터세트 ‘딥스피치(DeepSpeech)’ 공개](http://www.aitimes.kr/news/articleView.html?idxno=11072)
* [wav2letter - a simple and efficient end-to-end Automatic Speech Recognition (ASR) system from Facebook AI Research](https://github.com/facebookresearch/wav2letter)

# Wikipedia
* **[practice](https://gist.github.com/hyunjun/3f628bbf2a4ccdcd11f4)**
* [A Multilingual Corpus of Automatically Extracted Relations from Wikipedia](http://googleresearch.blogspot.kr/2015/06/a-multilingual-corpus-of-automatically.html)
* [Exploring Wikipedia with Gremlin Graph Traversals](http://markorodriguez.com/2012/03/07/exploring-wikipedia-with-gremlin-graph-traversals/)
* **[Fact Extraction from Wikipedia Text](https://github.com/dbpedia/fact-extractor)**
* [LSA-ing Wikipedia with Apache Spark](http://www.slideshare.net/cloudera/lsaing-wikipedia-with-apache-spark)
* [wiki - Command line tool to fetch summaries from mediawiki wikis, like Wikipedia](https://github.com/walle/wiki)

# Word2Vec
* [Modern Methods for Sentiment Analysis](https://districtdatalabs.silvrback.com/modern-methods-for-sentiment-analysis)
* [Word vectors (word2vec) on named entities and phrases - I](http://www.vikasing.com/2015/03/word-vectors-word2vec-on-named-entities.html)
* [w.elnn.kr](http://w.elnn.kr)
* [Five crazy abstractions my Deep Learning word2vec model just did](http://byterot.blogspot.kr/2015/06/five-crazy-abstractions-my-deep-learning-word2doc-model-just-did-NLP-gensim.html)
* [Neural Language Model and Word2Vec](https://github.com/dsindex/blog/wiki/%5BWord2Vec%5D-Neural-Language-Model-and-Word2Vec)
* [2015 py con word2vec이 추천시스템을 만났을 때](http://www.slideshare.net/ssuser2fe594/2015-py-con-word2vec)
* [한국어와 NLTK, Gensim의 만남](http://www.slideshare.net/lucypark/nltk-gensim)
* [한국어와 NLTK, Gensim의 만남](https://www.lucypark.kr/slides/2015-pyconkr/)
* **[Word2Vec Vector Algebra Comparison - Python(Gensim) VS Scala(Spark)](http://hoondongkim.blogspot.com/2016/07/word2vec-vector-algebra-comparison.html)**
* [FastText and Gensim word embeddings](http://rare-technologies.com/fasttext-and-gensim-word-embeddings/)
* [word2vec with gensim](http://m.blog.naver.com/pdc222/220693024820)
* [단어 임베딩의 원리와 gensim.word2vec 사용법](https://www.datascienceschool.net/view-notebook/6927b0906f884a67b0da9310d3a581ee/)
* [models.word2vec – Deep learning with word2vec](https://radimrehurek.com/gensim/models/word2vec.html)
* [Word2vec with Gensim - Python](https://www.youtube.com/watch?v=thLzt3D-A10)
* [word2vec tutorial](https://github.com/krikit/word2vec_tutorial)
  * [word2vec_tutorial.ipynb](https://github.com/krikit/word2vec_tutorial/blob/master/word2vec_tutorial.ipynb)
  * [doc2vec_tutorial.ipynb](https://github.com/krikit/word2vec_tutorial/blob/master/doc2vec_tutorial.ipynb)
* [Word2Vec Tutorial - The Skip-Gram Model](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)
* [word2vec, LDA, and introducing a new hybrid algorithm: lda2vec](http://www.slideshare.net/ChristopherMoody3/word2vec-lda-and-introducing-a-new-hybrid-algorithm-lda2vec-57135994)
* [Bag of Words Meets Bags of Popcorn](https://www.kaggle.com/c/word2vec-nlp-tutorial)
* [Vector Representations of Words](https://www.tensorflow.org/versions/r0.7/tutorials/word2vec/index.html)
  * [단어의 벡터 표현 (Vector Representations of Words)](https://codeonweb.com/entry/dcc9ef10-5d8f-47c4-bd6e-27878a9a8b62)
* [브런치 작가 추천과 Word2Vec](https://brunch.co.kr/@goodvc78/7)
* [word2vec_basic.ipynb](https://github.com/sjchoi86/tensorflow-tutorials/blob/master/notebooks/word2vec_basic.ipynb)
* [The Amazing Power of Word Vectors](http://www.kdnuggets.com/2016/05/amazing-power-word-vectors.html)
* [Audio Word2Vec: Unsupervised Learning of Audio Segment Representations using Sequence-to-sequence Autoencoder](http://arxiv.org/abs/1603.00982)
* [word2vec](https://code.google.com/archive/p/word2vec/)
* [How to giving a specific word to word2vec model in tensorflow](http://stackoverflow.com/questions/38027289/how-to-giving-a-specific-word-to-word2vec-model-in-tensorflow)
* [한국어 Word2Vec](http://blog.theeluwin.kr/post/146591096133/%ED%95%9C%EA%B5%AD%EC%96%B4-word2vec)
* [tag2vec - 인스타그램 태그를 Word2vec으로 학습시킨 태그 벡터 공간입니다. https://tag2vec.herokuapp.com](https://github.com/muik/tag2vec)
* [Making Sense of Everything with words2map](http://blog.yhat.com/posts/words2map.html)
* [github.com/leeyonghwan92/news_clustering](https://github.com/leeyonghwan92/news_clustering) 동국대학교 4학년 학생 졸업 프로젝트
* [한글을 이용한 데이터마이닝및 word2vec이용한 유사도 분석](http://blog.naver.com/2feelus/220384206922)
* [5-1. 텐서플로우(TensorFlow)를 이용해 자연어를 처리하기(NLP) – Word Embedding(Word2vec)](http://solarisailab.com/archives/374)
* [On word embeddings - Part 3: The secret ingredients of word2vec](http://sebastianruder.com/secret-word2vec/index.html)
* [Ali Ghodsi, Lec [3,1]: Deep Learning, Word2vec](https://www.youtube.com/watch?v=TsEGsdVJjuA&spfreload=10)
* [Pre-trained word vectors of 30+ languages](https://github.com/Kyubyong/wordvectors)
* [Play with word embeddings in your browser](https://medium.com/@awjuliani/play-with-word-embeddings-in-your-browser-fc904a009058)
* [NLP Research part 1. Vector Representations of Words](https://www.facebook.com/notes/enjoydl/nlp-research-part-1-vector-representations-of-words/1237465276335840)
* [Word2Vec 그리고 추천 시스템의 Item2Vec](https://brunch.co.kr/@goodvc78/16)
* [박근혜 탄핵 결정문 전문 Word2Vec Visualization w/Tensorflow](http://visionigniter.blogspot.com/2017/03/word2vec-visualization-wtensorflow.html)
* [단어를 숫자로! Google의 Word2Vec](https://medium.com/@deepvalidation/%EB%8B%A8%EC%96%B4%EB%A5%BC-%EC%88%AB%EC%9E%90%EB%A1%9C-google%EC%9D%98-word2vec-18a4b14f8730)
* [Word2Vec In Java](https://github.com/taki0112/Word2VecJava)
* [code.google.com/archive/p/word2vec](https://code.google.com/archive/p/word2vec/)
* [Sample code for vectorizing emotion words, visualize emotion word vectors, and find most similar words for "angry"](https://gist.github.com/kendemu/6df1722bed1d5148cf43385c992c8414)
* [Word2GM (Word to Gaussian Mixture)](https://github.com/benathi/word2gm)
* [Simple NN with Keras](https://nbviewer.jupyter.org/github/likejazz/jupyter-notebooks/blob/master/vector-representation-of-words.ipynb)
* [Deep Learning #4: Why You Need to Start Using Embedding Layers](https://medium.com/towards-data-science/deep-learning-4-embedding-layers-f9a02d55ac12)
* [A non-NLP application of Word2Vec](https://medium.com/towards-data-science/a-non-nlp-application-of-word2vec-c637e35d3668)
* [PR-027:GloVe - Global vectors for word representation](https://www.youtube.com/watch?v=uZ2GtEe-50E)
* [Lecture 2 | Word Vector Representations: word2vec](https://www.youtube.com/watch?v=ERibwqs9p38&index=2&list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6)
* [번역에서 배우기 : 문맥화된 단어 벡터(contextualized word vector)](https://www.facebook.com/nextobe1/posts/375503076219027)
* [쉽게 씌어진 word2vec](https://dreamgonfly.github.io/machine/learning,/natural/language/processing/2017/08/16/word2vec_explained.html)
* [Stop Using word2vec](http://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/)
* [Word Tensors](http://multithreaded.stitchfix.com/blog/2017/10/25/word-tensors/)
* [Word embeddings in 2017: Trends and future directions](http://ruder.io/word-embeddings-2017/)
* [Aerin Kim - Phrase2Vec In Practice #AIWTB 2016](https://www.youtube.com/watch?v=kGGA1Wm8_x0)
* [Using Word2vec for Music Recommendations](https://towardsdatascience.com/using-word2vec-for-music-recommendations-bb9649ac2484)
* [Use Neural Networks to Find the Best Words to Title Your eBook](https://www.datasciencecentral.com/profiles/blogs/use-neural-networks-to-find-the-best-words-to-title-your-ebook)
* [Playing with word vectors](https://medium.com/@martinkonicek/playing-with-word-vectors-308ab2faa519)
* [Transform anything into a vector; entity2vec: Using cooperative learning approaches to generate entity vectors](https://blog.insightdatascience.com/entity2vec-dad368c5b830)
* [Learning meaningful location embeddings from unlabeled visits](http://www.sentiance.com/2018/01/29/learning-meaningful-location-embeddings-from-unlabeled-visits/)
* [Mapping Medium’s Tags](https://medium.engineering/mapping-mediums-tags-1b9a78d77cf0)
* [bilm-tf](https://github.com/allenai/bilm-tf)
  * word2vec, glove 등의 lookup 기반 embedding 기법과는 다르게 context word embedding을 사용해서 downstream task의 성능 향상
  * 1. 대용량 corpus를 이용해서 2-layer bilstm lm 모델을 만들고
  * 2. 각 timestep에 있는 h값에 대한 linear combination 결과를 현재 timestep의 word embedding으로 사용
  * 3. combination weight는 downstream task의 cost function을 통해서 조정

# Topic Modeling
* [Topic Modeling with LDA Introduction](https://opendatascience.com/blog/topic-modeling-with-lda-introduction/)
* [Text Mining 101: Topic Modeling](http://www.kdnuggets.com/2016/07/text-mining-101-topic-modeling.html)
* [Topic Modeling in Multi-Aspect Reviews](http://nbviewer.ipython.org/gist/benjamincohen1/d7caaa3d07bbb89cd39a)
* [Topic Modeling of Twitter Followers](http://alexperrier.github.io/jekyll/update/2015/09/04/topic-modeling-of-twitter-followers.html)
* [Topic Modeling in Multi-Aspect Reviews](http://nbviewer.ipython.org/gist/benjamincohen1/d7caaa3d07bbb89cd39a)
* [Topic Modeling With Python](https://www.youtube.com/watch?v=XVrZSXuYliI)
