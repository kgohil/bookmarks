[Spark](https://spark.apache.org)
=====
* [Spark Documentation](https://spark.apache.org/documentation.html)
* [Databricks Spark Knowledge Base](https://www.gitbook.com/book/databricks/databricks-spark-knowledge-base/details)
* [Spark Programming Guide](https://spark.apache.org/docs/latest/programming-guide.html)
* [Tuning Spark](http://spark.apache.org/docs/latest/tuning.html)
* [advanced dependency management](http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management)
* [Custom API Examples For Apache Spark - The examples are basic and only for newbies in Scala and Spark.](https://github.com/HyukjinKwon/spark-custom-api)
* [Welcome to Spark Python API Docs!](https://spark.apache.org/docs/latest/api/python/index.html)
* [github.com/apache/spark](https://github.com/apache/spark)
* **[SparkTutorian.net - Apache Spark For the Common * Man!](http://sparktutorials.net/)**
* [sparktutorials.github.io](https://sparktutorials.github.io/)
* [learn hadoop spark by examples](https://www.java-success.com/category/tutorial/hadoop-tutorials/learn-hadoop-spark-by-examples/)
* [Spark 시작하기 (유용한 사이트 링크)](https://www.facebook.com/notes/%EC%8A%A4%EC%82%AC%EB%AA%A8-%ED%95%9C%EA%B5%AD-%EC%8A%A4%ED%8C%8C%ED%81%AC-%EC%82%AC%EC%9A%A9%EC%9E%90-%EB%AA%A8%EC%9E%84-%EC%9D%B8%EB%A9%94%EB%AA%A8%EB%A6%AC-%EC%BB%B4%ED%93%A8%ED%8C%85/spark-%EC%8B%9C%EC%9E%91%ED%95%98%EA%B8%B0-%EC%9C%A0%EC%9A%A9%ED%95%9C-%EC%82%AC%EC%9D%B4%ED%8A%B8-%EB%A7%81%ED%81%AC/775214279207144?hc_location=ufi)
* [Learning Spark With Scala](https://maheshkndpl.wordpress.com/2017/09/01/introduction-to-spark/)
* [Spark Internals](https://github.com/JerryLead/SparkInternals)
* [pubdata.tistory.com/category/Lecture_SPARK](http://pubdata.tistory.com/category/Lecture_SPARK)
* [Apache Spark - Executive Summary](https://www.linkedin.com/pulse/apache-spark-executive-summary-alan-brown)
* [Teach yourself Apache Spark – Guide for nerds!](https://www.linkedin.com/pulse/teach-yourself-apache-spark-guide-nerds-shrinath-parikh)
* [Apache Spark - cyber.dbguide.net](http://cyber.dbguide.net/lecture.php?action=view&no=154)
* [Stanford CS347 Guest Lecture: Apache Spark](http://www.slideshare.net/rxin/stanford-cs347-guest-lecture-apache-spark)
* [BerkeleyX: CS100.1x Introduction to Big Data with Apache Spark](https://courses.edx.org/courses/BerkeleyX/CS100.1x/1T2015/)
  * [mooc-setup](https://github.com/spark-mooc/mooc-setup)
  * [Spark로 빅데이터 입문, 1-2주차 노트](http://seoh.github.io/blog/2015/06/10/big-data-with-spark-1-2-week/)
  * [Spark로 빅데이터 입문, 3주차 노트](http://seoh.github.io/blog/2015/06/14/big-data-with-spark-3-week/)
* [BerkeleyX: CS190.1x Scalable Machine Learning](https://courses.edx.org/courses/BerkeleyX/CS190.1x/1T2015/)
  * [Spark: Cluster Computing with Working Sets](http://people.csail.mit.edu/matei/papers/2010/hotcloud_spark.pdf)
  * [Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing](https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf)
* [bigdatauniversity.com](http://bigdatauniversity.com)
  * [Spark Fundamentals I](http://bigdatauniversity.com/bdu-wp/bdu-course/spark-fundamentals/)
  * [Spark Fundamentals II](http://bigdatauniversity.com/bdu-wp/bdu-course/spark-fundamentals-ii/)
* [Introduction to Spark](https://www.dataquest.io/mission/123/introduction-to-spark/)
* [Spark Programming](http://www.slideshare.net/taewook/spark-programming)
* [Introduction to Spark Internals](http://www.slideshare.net/michiard/introduction-to-spark-internals)
* [Intro to Apache Spark Training - Part 1](https://www.youtube.com/watch?v=VWeWViFCzzg)
* Cloudera
  * [Cloudera Engineering Blog · Spark Posts](http://blog.cloudera.com/blog/category/spark/)
  * **[How-to: Tune Your Apache Spark Jobs (Part 1)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-1/)**
  * **[How-to: Tune Your Apache Spark Jobs (Part 2)](http://blog.cloudera.com/blog/2015/03/how-to-tune-your-apache-spark-jobs-part-2/)**
  * [LSA-ing Wikipedia with Apache Spark](http://www.slideshare.net/cloudera/lsaing-wikipedia-with-apache-spark)
  * [Making Apache Spark Testing Easy with Spark Testing Base](http://blog.cloudera.com/blog/2015/09/making-apache-spark-testing-easy-with-spark-testing-base/)
  * [Getting Apache Spark Customers to Production](http://www.slideshare.net/cloudera/getting-apache-spark-customers-to-production)
  * [Why Your Apache Spark Job is Failing](http://www.slideshare.net/cloudera/why-your-apache-spark-job-is-failing)
* [The Apache Spark @youtube](https://www.youtube.com/user/TheApacheSpark)
* [Apache spark 소개 및 실습](http://www.slideshare.net/KangDognhyun/apache-spark-70360736)
* [Spark 소개 1부](http://www.slideshare.net/brotherjinho/spark-1-48694544)
* [Spark 소개 2부](http://www.slideshare.net/brotherjinho/spark-2-52028665)
* [RE: ShootingStar TV 1회 - 아파치 스파크와 RDD](https://www.youtube.com/watch?v=nuZecG90tLs)
  * [스터디용 아파치 스파크 환경구성 - 윈도우](https://www.youtube.com/watch?v=Rh62AHznlnc)
  * [스터디용 아파치 스파크 환경구성 - 인텔리J](https://www.youtube.com/watch?v=SWCf2A9xZgs)
* [databricks](https://databricks.com/)
  * [sparkhub.databricks.com](http://sparkhub.databricks.com/)
  * [Examples for Learning Spark](https://github.com/databricks/learning-spark)
  * [Project Tungsten: Bringing Spark Closer to Bare Metal](https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html)
    * [Project Tungsten: Apache Spark](http://www.infoobjects.com/project-tungsten-apache-spark/)
    * [Deep Dive into Project Tungsten: Bringing Spark Closer to Bare Metal-(Josh Rosen, Databricks)](http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen)
    * [SPARK 성능의 핵심 PROJECT TUNGSTEN 톺아보기](https://younggyuchun.wordpress.com/2017/01/31/spark-%EC%84%B1%EB%8A%A5%EC%9D%98-%ED%95%B5%EC%8B%AC-project-tungsten-%ED%86%BA%EC%95%84%EB%B3%B4%EA%B8%B0/)
  * [Simplifying Big Data Analytics with Apache Spark](http://www.slideshare.net/databricks/bdtc2?ref=http%3A%2F%2Fwww.slideshare.net%2Fdatabricks%2Fslideshelf)
  * [Databricks Announces General Availability of Its Cloud Platform](http://insidebigdata.com/2015/06/15/databricks-announces-general-availability-of-its-cloud-platform/)
  * [A Deeper Understanding of Spark Internals - Aaron Davidson (Databricks)](https://www.youtube.com/watch?v=dmL0N3qfSc8)
  * [DEVOPS ADVANCED CLASS](http://training.databricks.com/devops.pdf)
  * [스파크의 사용 환경 내용 - data bricks](http://knight76.tistory.com/entry/%ED%8E%8C-%EC%8A%A4%ED%8C%8C%ED%81%AC%EC%9D%98-%EC%82%AC%EC%9A%A9-%ED%99%98%EA%B2%BD-%EB%B0%9C%ED%91%9C)
* [What is shuffle read & shuffle write in Apache Spark](http://stackoverflow.com/questions/27276884/what-is-shuffle-read-shuffle-write-in-apache-spark)
* [Scrap your MapReduce! (Or, Introduction to Apache Spark)](http://rahulkavale.github.io/blog/2014/11/16/scrap-your-map-reduce/)
* [Learning Spark](https://www.safaribooksonline.com/library/view/learning-spark/9781449359034/)
* [Introduction to Data Science with Apache Spark](http://ko.hortonworks.com/blog/introduction-to-data-science-with-apache-spark/)
* [HPC is dying, and MPI is killing it](http://www.dursi.ca/hpc-is-dying-and-mpi-is-killing-it/)
* [Spark은 왜 이렇게 유명해지고 있을까?](http://www.slideshare.net/KSLUG/ss-47355270)
* [Analytics With Apache Spark Is Coming](http://www.vidyasource.com/blog/Data/Hadoop/Analytics/Programming/Scala/Java/Python/Architecture/2015/04/22/analytics-with-apache-spark-is-coming)
* [Interactive Analytics using Apache Spark](http://www.slideshare.net/differentsachin/interactive-analytics-using-apache-spark)
* [bicdata]()
  * 고급 분석을 '현실'로 만드는 스파크 -> 머신런닝 알고리즘이 포함 있지만, 고급분석가의 관점으로는 기초적인 알고리즘만 포함
  * 모든 것을 더 편하게 만들어주는 스파크 -> M/R 형식의 프로그램은 많이 편해짐. MPI 방식은 지원하지 않음
  * 하나 이상의 언어를 말하는 스파크 -> scala, java, python을 지원하지만, scala에 최적화되어 있고 나머지 언어는 좀 불편
  * 더 빨리 결과를 도출하는 스파크 -> 성능 테스트를 해보면, SparkStream은 storm보다 느리고, SparkSQL은 Hive보다 느림. 일반적인 Spark 프로그램이 성능이 좋음
  * 하둡 개발업체를 가리지 않는 스파크 -> 오픈소스는 대부분 업체를 가리지 않고, 용도와 장단점이 다름
  * 실시간 고급 분석 -> 기존(하둡)보다는 빠른 고급분석(??)이기 하지만, 준실시간
* [VCNC가 Hadoop대신 Spark를 선택한 이유](http://engineering.vcnc.co.kr/2015/05/data-analysis-with-spark/)
* [(25) 라인플러스 게임보안개발실...스파크+메소스로 10분 당 15TB 처리](https://www.imaso.co.kr/news/article_view.php?article_idx=20150519094003)
* [bcho.tistory.com/tag/Apache Spark](http://bcho.tistory.com/tag/Apache%20Spark)
  * [Spark 노트](http://bcho.tistory.com/983)
  * [Apache Spark이 왜 인기가 있을까?](http://bcho.tistory.com/1023)
  * [Apache Spark 설치 하기](http://bcho.tistory.com/1024)
  * [Apache Spark 소개 - 스파크 스택 구조](http://bcho.tistory.com/1026)
  * [Apache Spark 클러스터 구조](http://bcho.tistory.com/1025)
  * [Apache Spark - RDD (Resilient Distributed DataSet) 이해하기 - #1/2](http://bcho.tistory.com/1027)
  * [Apache Spark RDD 이해하기 #2 - 스파크에서 함수 넘기기 (Passing function to Spark)](http://bcho.tistory.com/1028)
  * [Apache Spark(스파크) - RDD Persistence (스토리지 옵션에 대해서)](http://bcho.tistory.com/1029)
  * [Apache Spark - Key/Value Paris (Pair RDD)](http://bcho.tistory.com/1030)
  * [Apache Spark-Python vs Scala 성능 비교](http://bcho.tistory.com/1031)
* **[blog.madhukaraphatak.com](http://blog.madhukaraphatak.com/)**
  * [Introduction to Spark Data Source API - Part 1](http://blog.madhukaraphatak.com/introduction-to-spark-data-source-api-part-1/)
* [Spark Summit](https://spark-summit.org/)
  * [Using Cascading to Build Data-centric Applications on Spark](https://spark-summit.org/2014/talk/using-cascading-to-build-data-centric-applications-on-spark)
  * [spark-summit.org/2015](https://spark-summit.org/2015/)
    * [Spark Summit 2015- Track A](http://livestream.com/fourstream/sparksummit2015-tracka)
    * [Spark Summit 2015- Track B](http://livestream.com/fourstream/sparksummit2015-trackb)
    * [Spark Summit 2015- Track C](http://livestream.com/fourstream/sparksummit2015-trackc)
  * [spark-summit.org/east-2016/schedule](https://spark-summit.org/east-2016/schedule/)
    * [Spark Summit East 2016 첫 날 덤프](https://www.facebook.com/notes/jong-wook-kim/spark-summit-east-2016-%EC%B2%AB-%EB%82%A0-%EB%8D%A4%ED%94%84/1004799559567616)
    * [Spark Summit East 2016 둘째 날 덤프](https://www.facebook.com/notes/jong-wook-kim/spark-summit-east-2016-%EB%91%98%EC%A7%B8-%EB%82%A0-%EB%8D%A4%ED%94%84/1006965639351008)
  * [spark-summit.org/2016/schedule](https://spark-summit.org/2016/)
    * [A Deep Dive into Structured Streaming](http://www.slideshare.net/databricks/a-deep-dive-into-structured-streaming)
    * [Apache Spark 2.0 Preview: Machine Learning Model Persistence by Databricks](https://databricks.com/blog/2016/05/31/apache-spark-2-0-preview-machine-learning-model-persistence.html)
    * [Meson: Netflix's framework for executing machine learning workflows](http://techblog.netflix.com/2016/05/meson_31.html)
    * [How-to: Analyze Fantasy Sports using Apache Spark and SQL](http://blog.cloudera.com/blog/2016/06/how-to-analyze-fantasy-sports-using-apache-spark-and-sql/)
  * [Spark Summit 2016 West Training](https://www.youtube.com/playlist?list=PLK3eYwzuIEnUwvKo8ssbWMGippbMLyAxR)
    * [2016-06-06 Spark Summit West](https://drive.google.com/folderview?id=0B09cDg18tuRhMFg3cmtseC1KQ0U&usp=drive_web)
    * [Training Apache Spark Essentials](https://www.youtube.com/watch?v=OheiUl_uXwo)
      * [Class Notes - SSW 2016 Spark Essentials](http://tinyurl.com/Spark-Essentials-TE1)
    * [Training Continues: Apache Spark Essentials](https://www.youtube.com/watch?v=fROnFlD3Isw)
  * **[Spark Summit Europe 2016 참관기](http://d2.naver.com/helloworld/8852387)**
  * [OrderedRDD: A Distributed Time Series Analysis Framework for Spark (Larisa Sawyer)](https://www.youtube.com/watch?v=x2iM5he2gAU)
  * [Just Enough Scala for Spark (Dean Wampler)](https://www.youtube.com/watch?v=LBoSgiLV_NQ)
  * [TensorFrames: Deep Learning with TensorFlow on Apache Spark (Tim Hunter)](https://www.youtube.com/watch?v=gXItObf-qaI)
  * [SPARK SUMMIT EAST 2017](https://spark-summit.org/east-2017/schedule/)
  * [SPARK SUMMIT 2017 DATA SCIENCE AND ENGINEERING AT SCALE](https://spark-summit.org/2017/schedule/)
  * [비트윈 데이터팀의 Spark Summit EU 2017 참가기](http://engineering.vcnc.co.kr/2017/12/spark-summit-eu-2017/)
* [Tuning Java Garbage Collection for Spark Applications](https://databricks.com/blog/2015/05/28/tuning-java-garbage-collection-for-spark-applications.html)
* [Spark(1.2.1 -> 1.3.1) 을 위한 Mesos(0.18 -> 0.22.rc) - Upgrade](http://hoondongkim.blogspot.kr/2015/05/spark121-131-mesos018-021-upgrade.html)
* [RDDS ARE THE NEW BYTECODE OF APACHE SPARK](https://ogirardot.wordpress.com/2015/05/29/rdds-are-the-new-bytecode-of-apache-spark/)
* [Spark RDD Operations-Transformation & Action with Example](https://data-flair.training/blogs/spark-rdd-operations-transformations-actions/)
* [Apache Spark on Docker](https://github.com/sequenceiq/docker-spark)
* [Microbenchmarking Big Data Solutions on the JVM – Part 1](http://www.autoletics.com/posts/microbenchmarking-big-data-solutions-on-the-jvm-part-1)
* [Spark, Mesos, Zeppelin, HDFS를 활용한 대용량 보안 데이터 분석](http://developers.linecorp.com/blog/ko/?p=123)
* [(Berkeley CS186 guest lecture) Big Data Analytics Systems: What Goes Around Comes Around](http://www.slideshare.net/rxin/2015-0409-cs186guestlecture)
* [IBM, 오픈소스 커뮤니티에 머신러닝 기술 기증](http://www.bloter.net/archives/230353)
* [Productionizing Spark and the Spark Job Server](http://www.slideshare.net/EvanChan2/productionizing-spark-and-the-spark-job-server)
* [is Hadoop dead and is it time to move to Spark](http://www.quora.com/Is-Hadoop-dead-and-is-it-time-to-move-to-Spark)
* [Spark + S3 + R3 을 이용한 데이터 분석 시스템 만들기 by VCNC](https://speakerdeck.com/vcnc/spark-plus-s3-plus-r3-eul-iyonghan-deiteo-bunseog-siseutem-mandeulgi)
* [Parallel Programming with Spark (Part 1 & 2) - Matei Zaharia](https://www.youtube.com/watch?v=7k4yDKBYOcw)
* [Running multiple Spark Streaming jobs of different DStreams in parallel](https://stackoverflow.com/questions/43167592/running-multiple-spark-streaming-jobs-of-different-dstreams-in-parallel)
* [Arbitrary Stateful Processing in Apache Spark’s Structured Streaming](https://databricks.com/blog/2017/10/17/arbitrary-stateful-processing-in-apache-sparks-structured-streaming.html)
  * 'exactly once' 주제에서 Apache Spark의 Structured Streaming으로 중복 제거를 구현하는 방법에 대해 설명
  * 워터마크 기반으로 한 중복 제거 외에도 mapGroupsWithState를 사용하여 상태 저장 집계에 사용자 정의 로직을 추가 할 수 있는 방법에 대해 간략하게 설명
* [Stream All the Things! Architectures for Data Sets that Never End](https://deanwampler.github.io/polyglotprogramming/papers/StreamAllTheThings.pdf)
  * 스트리밍 중심 응용 프로그램 및 데이터 플랫폼 구축
  * 서비스를 함께 연결하는 단순성을 보여줌으로써 이벤트 소싱 아키텍처에 대해 동기를 부여
  * 실시간 및 분석 사례에 대한 다양한 시스템(Akka, Spark, Flink 및 기타)의 절충에 대해 설명
* [Petabyte-Scale Text Processing with Spark](http://tech.grammarly.com/blog/posts/Petabyte-Scale-Text-Processing-with-Spark.html)
* [Combining Druid and Spark: Interactive and Flexible Analytics at Scale](https://www.linkedin.com/pulse/combining-druid-spark-interactive-flexible-analytics-scale-butani)
* [Interactive Audience Analytics With Spark and HyperLogLog](http://eugenezhulenev.com/blog/2015/07/15/interactive-audience-analytics-with-spark-and-hyperloglog/)
* [Apache Spark Creator Matei Zaharia Interview](http://softwareengineeringdaily.com/2015/08/03/apache-spark-creator-matei-zaharia-interview/)
* [New Developments in Spark](http://www.slideshare.net/databricks/new-developments-in-spark)
* [Spark와 Hadoop, 완벽한 조합 (한국어)](http://www.slideshare.net/pudidic/spark-hadoop)
* [Spark Architecture: Shuffle](http://0x0fff.com/spark-architecture-shuffle/)
* [Naytev Wants To Bring A Buzzfeed-Style Social Tool To Every Publisher With Spark](http://techcrunch.com/2015/09/30/naytev-wants-to-bring-a-buzzfeed-style-social-tool-to-every-publisher-with-spark/)
* [Spinning up a Spark Cluster on Spot Instances: Step by Step](http://insightdataengineering.com/blog/sparkdevops/)
* [Spark Meetup at Uber](http://www.slideshare.net/databricks/spark-meetup-at-uber)
* [Bay Area Apache Spark Meetup @ Intel](https://www.youtube.com/watch?v=5UGkJzMBrQU)
  * [Easy, scalable, fault tolerant stream processing with structured streaming - spark meetup at intel in santa clara](https://www.slideshare.net/julesdamji/easy-scalable-fault-tolerant-stream-processing-with-structured-streaming-spark-meetup-at-intel-in-santa-clara-73743007)
* [Can Apache Spark process 100 terabytes of data in interactive mode?](http://fullstackml.com/2015/10/12/can-apache-spark-process-100-terabytes-of-data-in-interactive-mode/)
* [넷플릭스 빅데이터 플랫폼 아파치 스팍 통합 경험기](http://www.slideshare.net/deview/262-netflix)
* [Succinct Spark from AMPLab: Queries on Compressed RDDs](https://databricks.com/blog/2015/11/10/succinct-spark-from-amplab-queries-on-compressed-rdds.html)
* [How-to: Build a Complex Event Processing App on Apache Spark and Drools](http://blog.cloudera.com/blog/2015/11/how-to-build-a-complex-event-processing-app-on-apache-spark-and-drools/)
* [Improving Spark application performance](http://chapeau.freevariable.com/2014/09/improving-spark-application-performance.html)
* [“Fast food” and tips for RDD](http://pl.postech.ac.kr/~maidinh/blog/?p=61)
* [스칼라ML - 스칼라를 이용한 기계학습 기초(+Spark)](http://psygrammer.github.io/ScalaML/)
* [Secondary Sorting in Spark](http://codingjunkie.net/spark-secondary-sort/)
* [Distributed computing with spark](http://www.slideshare.net/javiersantospaniego/distributed-computing-with-spark)
* [Comparing the Dataflow/Beam and Spark Programming Models](https://cloud.google.com/blog/big-data/2016/02/comparing-the-dataflowbeam-and-spark-programming-models#closeImage)
* [Apache Spark Architecture](http://www.slideshare.net/AGrishchenko/apache-spark-architecture)
* [Scala vs. Python for Apache Spark](https://www.dezyre.com/article/scala-vs-python-for-apache-spark/213)
* [Natural Language Processing With Apache Spark](https://dzone.com/articles/in-progress-natural-language-processing)
* [맵알, ‘아파치 스파크’ 교육 과정 무료로 공개](http://www.bloter.net/archives/248982)
* [Spark HDFS Integration](http://0x0fff.com/spark-hdfs-integration/)
* [spark textfile load file instead of lines](http://stackoverflow.com/questions/29643348/spark-textfile-load-file-instead-of-lines)
* [Reading Text Files by Lines](https://wiki.ufal.ms.mff.cuni.cz/spark:recipes:reading-text-files)
* [Evening w/ Martin Odersky! (Scala in 2016) +Spark Approximates +Twitter Algebird](https://www.youtube.com/watch?v=_-I_X-k3D8A&feature=youtu.be)
* [ScalaJVMBigData-SparkLessons.pdf](deanwampler.github.io/polyglotprogramming/papers/ScalaJVMBigData-SparkLessons.pdf)
* [Introduction to Spark 2.0 : A Sneak Peek At Next Generation Spark](http://blog.madhukaraphatak.com/introduction-to-spark-2.0/)
  * [Spark Release 2.0.0](https://spark.apache.org/releases/spark-release-2-0-0.html)
  * [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html#dataframe-data-readerwriter-interface)
  * [A Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets - When to use them and why](https://databricks.com/blog/2016/07/14/a-tale-of-three-apache-spark-apis-rdds-dataframes-and-datasets.html)
  * [Introducing Apache Spark 2.0](https://databricks.com/blog/2016/07/26/introducing-apache-spark-2-0.html)
  * [Spark 2.0 Technical Preview: Easier, Faster, and Smarter](https://databricks.com/blog/2016/05/11/spark-2-0-technical-preview-easier-faster-and-smarter.html)
  * [Apache Spark 2.0 presented by Databricks co-founder Reynold Xin](https://www.brighttalk.com/webcast/12891/202021)
  * [APACHE SPARK 2.0 API IMPROVEMENTS: RDD, DATAFRAME, DATASET AND SQL](http://www.agildata.com/apache-spark-2-0-api-improvements-rdd-dataframe-dataset-sql/)
  * [Spark 2.0 – Datasets and case classes](https://blog.codecentric.de/en/2016/07/spark-2-0-datasets-case-classes/)
  * [Apache Spark 2.0 Performance Improvements Investigated With Flame Graphs](http://db-blog.web.cern.ch/blog/luca-canali/2016-09-spark-20-performance-improvements-investigated-flame-graphs)
  * [Generating Flame Graphs for Apache Spark](https://gist.github.com/kayousterhout/7008a8ebf2babeedc7ce6f8723fd1bf4)
  * [Apache Spark 2.0 Tuning Guide](http://www.slideshare.net/jcmia1/apache-spark-20-tuning-guide)
  * [Using Apache Spark 2.0 to Analyze the City of San Francisco's Open Data](https://www.youtube.com/watch?v=K14plpZgy_c)
  * [Modern Spark DataFrame & Dataset | Apache Spark 2.0 Tutorial](https://www.youtube.com/watch?v=_1byVWTEK1s)
  * [Structuring Apache Spark 2.0: SQL, DataFrames, Datasets And Streaming - by Michael Armbrust](https://www.youtube.com/watch?v=1a4pgYzeFwE)
  * [Apache Spark 2.0: A Deep Dive Into Structured Streaming - by Tathagata Das](https://www.youtube.com/watch?v=rl8dIzTpxrI)
  * [Spark 2.0 - by Matei Zaharia](https://www.youtube.com/watch?v=RUTeY4E2MoQ)
  * [Spark 2.x Troubleshooting Guide](https://www.slideshare.net/jcmia1/a-beginners-guide-on-troubleshooting-spark-applications)
* [Introducing Apache Spark 2.1 Now available on Databricks](https://databricks.com/blog/2016/12/29/introducing-apache-spark-2-1.html)
* [What's New in the Upcoming Apache Spark 2.3 Release?](http://go.databricks.com/databricks-runtime-4-with-apache-spark2-3)
* [The easiest way to run Spark in production](https://dcos.io/)
* [Spark tuning for Enterprise System Administrators](http://techsuppdiva.github.io/spark1.6.html)
* [Structuring Spark: DataFrames, Datasets, and Streaming by Michael Armbrust](http://www.slideshare.net/SparkSummit/structuring-spark-dataframes-datasets-and-streaming-by-michael-armbrust?from_m_app=android)
* [Spark Takes On Dataflow in Benchmark Test](http://www.datanami.com/2016/05/02/dataflow-tops-spark-benchmark-test/)
* [Stock inference engine using Spring XD, Apache Geode / GemFire and Spark ML Lib. http://pivotal-open-source-hub.github.io/StockInference-Spark](https://github.com/Pivotal-Open-Source-Hub/StockInference-Spark)
* [Learning Spark - 아키텍트를 꿈꾸는 사람들](http://d2.naver.com/news/8818403)
  * [2015_LearningSpark](https://github.com/andstudy/afternoon/wiki/2015_LearningSpark)
* [Tutorial: Spark-GPU Cluster Dev in a Notebook A tutorial on ad-hoc, distributed GPU development on any Macbook Pro](https://iamtrask.github.io/2014/11/22/spark-gpu/)
* [GPU Acceleration on Apache Spark™](http://www.spark.tc/gpu-acceleration-on-apache-spark-2/)
* [Spark에서 GPU를 사용해야하는 이유는 무엇입니까?](http://aitimes.org/archives/264)
* [Cluster - spark](http://www.slideshare.net/HyeonSeokChoi/cluster-spark)
* [Apache Spark Key Terms, Explained](https://databricks.com/blog/2016/06/22/apache-spark-key-terms-explained.html)
* [스파크 클라우데라 하둡 클러스터 원격 입출력 예제](http://blog.naver.com/hancury/220744753944)
* [이렇게 코딩 하면 안된다](https://github.com/jaeho-kang/deep-learning/blob/master/SPARK/Introduction%20to%20Apache%20Spark_%EC%A0%95%EB%A6%AC.md)
* [spark를 이용한 hadoop cluster 원격 입출력](http://blog.naver.com/hancury/220744753944)
* [Best Practices for Using Apache Spark on AWS](http://www.slideshare.net/AmazonWebServices/best-practices-for-using-apache-spark-on-aws)
* [Build a Prediction Engine Using Spark, Kudu, and Impala](https://dzone.com/articles/how-to-build-a-prediction-engine-using-spark-kudu)
* [Deep Dive: Apache Spark Memory Management](https://www.youtube.com/watch?v=dPHrykZL8Cg&feature=youtu.be&t=25m18s)
* [Deep Dive: Apache Spark Memory Management](http://go.databricks.com/deep-dive-apache-spark-memory-management)
* option
  * spark.executor.cores; node의 코어수
  * spark.cores.max 전체 갯수
  * e.g.
    * worker node가 2개이고 각 node당 8core cpu인데 spark.cores.max를 8로 주면 1개의 노드만 동작
    * 두개의 node에서 동작하게 하려면 spark.cores.max를 16으로
* [Apache Spark @Scale: A 60 TB+ production use case](https://code.facebook.com/posts/1671373793181703)
* [How Do In-Memory Data Grids Differ from Spark?](https://www.scaleoutsoftware.com/technology/how-do-in-memory-data-grids-differ-from-spark/)
* [Spark에서의 Data Skew 문제](http://eminency.github.io/techinal/spark/2016/10/08/data-skew.html)
* [처음해보는 스파크(spark)로 24시간안에 부동산 과열 분석해보기](http://angeliot.blogspot.com/2016/11/24-spark.html)
* [Intro to Apache Spark for Java and Scala Developers - Ted Malaska (Cloudera)](https://www.youtube.com/watch?v=x8xXXqvhZq8)
* **[Achieving a 300% speedup in ETL with Apache Spark](http://blog.cloudera.com/blog/2016/12/achieving-a-300-speedup-in-etl-with-spark/)**
  * Spark의 CSV 파일 작업에 대한 스니펫 소개
  * non-distributed version에 비해 Spark는 뛰어난 속도 향상 기능을 제공하며 Parquet과 같은 최적화된 형식으로 변환 할 수 있는 기능을 제공
* [Diving into Spark and Parquet Workloads, by Example](https://db-blog.web.cern.ch/blog/luca-canali/2017-06-diving-spark-and-parquet-workloads-example)
* [How to install and run Spark 2.0 on HDP 2.5 Sandbox](https://community.hortonworks.com/articles/53029/how-to-install-and-run-spark-20-on-hdp-25-sandbox.html)
* [Experimenting with Neo4j and Apache Zeppelin (Neo4j)-\[:LOVES\]-(Zeppelin)](https://medium.com/apache-zeppelin-stories/experimenting-with-neo4j-and-apache-zeppelin-d80b7bec8fd2)
* [Time-Series Missing Data Imputation In Apache Spark](http://www.jowanza.com/post/154094307399/time-series-missing-data-imputation-in-apache)
* **[Data Science How-To: Using Apache Spark for Sports Analytics](https://content.pivotal.io/blog/how-data-science-assists-sports)**
  * [Using Spark To Analyze the NBA and the 3-point Shot](https://github.com/crawles/spark-nba-analytics)
* [Hive on Spark: Getting Started](https://cwiki.apache.org/confluence/display/Hive/Hive+on+Spark%3A+Getting+Started)
* [Working with UDFs in Apache Spark](http://blog.cloudera.com/blog/2017/02/working-with-udfs-in-apache-spark/)
  * Python, Java, Scala에서 Apache Spark의 UDF, UDAF를 사용하는 간단한 예제
* [Apache Spark은 어떻게 가장 활발한 빅데이터 프로젝트가 되었나](http://readme.skplanet.com/wp-content/uploads/%ED%8A%B8%EB%9E%991-3Apache-Spark%EC%9D%80-%EC%96%B4%EB%96%BB%EA%B2%8C-%EA%B0%80%EC%9E%A5-%ED%99%9C%EB%B0%9C%ED%95%9C-%EB%B9%85%EB%8D%B0%EC%9D%B4%ED%84%B0-%ED%94%84%EB%A1%9C%EC%A0%9D%ED%8A%B8%EA%B0%80-%EB%90%98%EC%97%88%EB%82%98.pdf)
* [Using Apache Spark for large-scale language model training](https://code.facebook.com/posts/678403995666478/using-apache-spark-for-large-scale-language-model-training/)
  * Facebook에서 ngram 모델의 traing pipeline을 Apach Hive에서 Apache Spark으로 전환 시도 중
  * 두 가지 솔루션에 대한 설명과 Spark DSL 과 Hive QL의 유연성 비교 및 성능 수치
* [WRITING TO A DATABASE FROM SPARK](http://bigdatums.net/2016/10/16/writing-to-a-database-from-spark/)
* [Processing Solr data with Apache Spark SQL in IBM IOP 4.3](https://developer.ibm.com/hadoop/2017/03/21/processing-solr-data-apache-spark-sql-ibm-iop-4-3/)
  * Apache Spark을 Apach Solr로 연결하는 방법 소개
* [Blacklisting in Apache Spark](https://blog.cloudera.com/blog/2017/04/blacklisting-in-apache-spark/)
* [Tracking the Money — Scaling Financial Reporting at Airbnb](https://medium.com/airbnb-engineering/tracking-the-money-scaling-financial-reporting-at-airbnb-6d742b80f040)
* [The Benefits of Migrating HPC Workloads To Apache Spark](https://hortonworks.com/blog/recent-improvements-apache-zeppelin-livy-integration/)
  * Spark 작업을 실행하기위한 Apache Zeppelin과 Livy 작업 서버 간의 통합에 대한 최근 개선 사항 설명
* [Spark StandAlone 설치부터 예제 테스트까지](http://hellowuniverse.com/2017/03/08/spark-standalone-%EC%84%A4%EC%B9%98%EB%B6%80%ED%84%B0-%EC%98%88%EC%A0%9C-%ED%85%8C%EC%8A%A4%ED%8A%B8%EA%B9%8C%EC%A7%80/)
* [데이터분석 인프라 구축기 (1/4)](https://medium.com/@gamzabaw/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EC%9D%B8%ED%94%84%EB%9D%BC-%EA%B5%AC%EC%B6%95%EA%B8%B0-1-4-fc1ff841dae9)
* [데이터분석 인프라 구축기 (2/4)](https://medium.com/@gamzabaw/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EC%9D%B8%ED%94%84%EB%9D%BC-%EA%B5%AC%EC%B6%95%EA%B8%B0-2-4-616df0d52ac3)
* [데이터분석 인프라 구축기 (3/4)](https://medium.com/@gamzabaw/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EC%9D%B8%ED%94%84%EB%9D%BC-%EA%B5%AC%EC%B6%95%EA%B8%B0-3-4-bb2326089ba5)
* [데이터분석 인프라 구축기 (4/4)](https://medium.com/@gamzabaw/%EB%8D%B0%EC%9D%B4%ED%84%B0%EB%B6%84%EC%84%9D-%EC%9D%B8%ED%94%84%EB%9D%BC-%EA%B5%AC%EC%B6%95%EA%B8%B0-4-4-b74ba73c426c)
* [parquet 사용 예제](http://knight76.tistory.com/entry/spark-parquet-%ED%85%8C%EC%8A%A4%ED%8A%B8)
* [zipWithIndex, for-yield 예제](http://knight76.tistory.com/entry/spark-zipWithIndex-foryield-%EC%98%88%EC%A0%9C)
* [Apache Spark installation on Windows 10](https://hernandezpaul.wordpress.com/2016/01/24/apache-spark-installation-on-windows-10/)
* [Cloudera session seoul - Spark bootcamp](https://www.slideshare.net/SangbaeLim/cloudera-sessions-seoul-spark-bootcamp)
* [Benchmarking Big Data SQL Platforms in the Cloud](https://databricks.com/blog/2017/07/12/benchmarking-big-data-sql-platforms-in-the-cloud.html)
  * Vanilla Spark, Presto, Impala 보다 DataBricks 플랫폼이 더 빠르다는 주장
* [Building QDS: AIR Infrastructure](https://www.qubole.com/blog/building-qdsair-infrastructure/)
  * Qubole이 Data Platforms 2017 conference 발표한 Air라는 플랫폼에 대한 내용입니다.
* [스파크 스터디 ParkS](https://github.com/psygrammer/ParkS/)
  * [ParkS](https://docs.google.com/spreadsheets/d/1XSbyjPibkJiQPpi29y9oLlMD9ihHvazb7L0uIS5wGA8/edit#gid=959319708)
* [Cost Based Optimizer in Apache Spark 2.2](https://databricks.com/blog/2017/08/31/cost-based-optimizer-in-apache-spark-2-2.html)
  * Apache Spark 2.2의 Cost Based Optimizer와 TPC-DS benchmark에서 CBO 사용 여부에 관계없이 쿼리 수행 시간을 비교한 결과와 통계 정보 수집 방법 등에 대해 설명
* [spark 프레임워크를 활용해 자바 기반 웹 애플리케이션 개발 맛보기](https://slipp.net/questions/548)
* [Bay Area Apache Spark Meetup at HPE/Aruba Networks Summary](https://databricks.com/blog/2017/09/22/bay-area-apache-spark-meetup-at-hpearuba-networks-summary.html)
  * Aruba에서 PySpark 및 GraphFrames의 Databricks를 사용한 데이터 상관 관계에 관한 프레젠테이션
* [Apache Spark Professional Training with Hands On Lab](http://www.hadoopexam.com/spark/training/Apache_Spark_professional_training_developer_certification_exam_dumps.html)
* [IBM Cloud 환경에서 DSX Spark를 사용한 데이터 분석 시작하기](https://developer.ibm.com/kr/developer-%EA%B8%B0%EC%88%A0-%ED%8F%AC%EB%9F%BC/2017/11/08/ibm-cloud-dsx-spark/)
* [Spark-overflow - A collection of Spark related information, solutions, debugging tips and tricks, etc. PR are always welcome! Share what you know about Apache Spark](https://github.com/AllenFang/spark-overflow)
* [Introduction to Spark on Kubernetes](https://banzaicloud.github.io/blog/spark-k8s/)
* [A Deeper Understanding of Spark Internals - Aaron Davidson (Databricks)](https://www.youtube.com/watch?v=dmL0N3qfSc8)
* [Using Apache Spark to Analyze Large Neuroimaging Datasets](https://blog.dominodatalab.com/pca-on-very-large-neuroimaging-datasets-using-pyspark/)
* [Goal Based Data Production: The Spark of a Revolution - Sim Simeonov](https://www.youtube.com/watch?v=VR2lAMVD4_4)
* [Spark Job On Mesos - Log Handling](http://hoondongkim.blogspot.com/2016/02/spark-job-on-mesos-log-handling.html) programtic하게 log level별로 원하는 장소에 로그 남기기
* [Top 5 Mistakes to Avoid When Writing Apache Spark Applications](https://intellipaat.com/blog/top-5-mistakes-writing-apache-spark-applications/)
* [Deep Dive into Monitoring Spark Applications Using Web UI and SparkListeners (Jacek Laskowski)](https://www.youtube.com/watch?v=mVP9sZ6K__Y&feature=youtu.be)
* [Extreme Apache Spark: how in 3 months we created a pipeline that can process 2.5 billion rows a day](https://www.slideshare.net/jozefhabdank/extreme-apache-spark-how-in-3-months-we-created-a-pipeline-that-can-process-25-billion-rows-a-day)

# API
* [Spark Programming Model : Resilient Distributed Dataset (RDD) - 2015](http://www.bogotobogo.com/Hadoop/BigData_hadoop_Apache_Spark_Programming_Model_RDD.php)
* [Apache Spark: Examples Of Transformations](https://www.supergloo.com/fieldnotes/apache-spark-examples-of-transformations/)
* [The RDD API By Example](http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html)
* [backtobazics.com/category/big-data/spark](http://backtobazics.com/category/big-data/spark/) example of API
* [APACHE SPARK: RDD, DATAFRAME OR DATASET?](http://www.agildata.com/apache-spark-rdd-vs-dataframe-vs-dataset/)
* [Apache Spark’s Hidden REST API](http://arturmkrtchyan.com/apache-spark-hidden-rest-api)
* aggregate

  ```
  scala> val rdd = sc.parallelize(List(1, 2, 3, 3))
  rdd: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[2] at parallelize at <console>:21

  scala> rdd.aggregate((0, 0))((x, y) => (x._1 + y, x._2 - y), (x, y) => (x._1 + y._1, x._2 + y._2))
  res10: (Int, Int) = (9,-9)

  scala> rdd.map(t => (t, -t)).reduce((a, b) => (a._1 + b._1, a._2 + b._2))
  res11: (Int, Int) = (9,-9)
  ```
* aggregateByKey
  * [AggregateByKey implements Collect_list in Spark 1.4](http://alvincjin.blogspot.kr/2015/09/aggregatebykey-implements-collectlist.html)
* combineByKey
  * [Using combineByKey in Apache-Spark](http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/)
  * [Spark PairRDDFunctions: CombineByKey](http://codingjunkie.net/spark-combine-by-key/)
  * [Apache Spark combineByKey Explained](http://www.edureka.co/blog/apache-spark-combinebykey-explained)
* DataFrames
  * [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)
  * [spark2.0 dataframe의 filter,where,isin,select,contains,col,between,withColumn, 예제](http://knight76.tistory.com/entry/spark20-dataframe%EC%9D%98-filterwhereisinselect)
  * [Spark: Connecting to a jdbc data-source using dataframes](http://www.infoobjects.com/spark-connecting-to-a-jdbc-data-source-using-dataframes/)
  * [where과 filter의 차이](http://knight76.tistory.com/entry/spark-where%EA%B3%BC-filter%EC%9D%98-%EC%B0%A8%EC%9D%B4)
  * [Using spark data frame for sql](https://www.slideshare.net/charsyam2/using-spark-data-frame-for-sql)
  * [Selecting Dynamic Columns In Spark DataFrames (aka Excluding Columns)](http://bailiwick.io/2017/08/08/selecting-dynamic-columns-in-spark-dataframes/)
* Datasets
  * [Introducing Spark Datasets](https://databricks.com/blog/2016/01/04/introducing-spark-datasets.html)
  * [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)
  * [RDDs, DataFrames and Datasets in Apache Spark - NE Scala 2016](https://www.youtube.com/watch?v=pZQsDloGB4w&t=740s)
  * [Spark2.0 New Features(1) DataSet](http://www.popit.kr/spark2-0-new-features1-dataset/)
* distinct
  * [동일성, 동등성, Spark의 distinct](https://leeyh0216.github.io/dev/lang/spark/2017/04/30/dev-spark-equals.html)
* groupByKey
  * [Avoid GroupByKey](https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html)
* HashPartitioner
  * [Apache Spark - HashPartitioner : How does it work?](http://stackoverflow.com/questions/31424396/apache-spark-hashpartitioner-how-does-it-work)
  * [Partition by Hash on Keys](https://bzhangusc.wordpress.com/2014/06/17/partition-by-hash-on-keys/)
* join
  * [RDD join 예제](http://knight76.tistory.com/entry/spark-RDD-join-%EC%98%88%EC%A0%9C)
  * [join 예제](http://knight76.tistory.com/entry/spark-join-%EC%98%88%EC%A0%9C)
* persist
  * [RDD persist() or cache() 시 주의사항](http://tomining.tistory.com/84)
* SQL
  * [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)
    * [Column](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Column)
    * [Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
    * [Row](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Row)
  * [spark-csv - CSV Data Source for Apache Spark 1.x](https://github.com/databricks/spark-csv/)
    * [TextFileSuite.scala](https://github.com/databricks/spark-csv/blob/master/src/test/scala/com/databricks/spark/csv/util/TextFileSuite.scala)
  * [Spark SQL CSV Examples](https://www.supergloo.com/fieldnotes/spark-sql-csv-examples/)
  * [github.com/yhuai/spark/tree/eb77ee39b8616cb367541503baf7c07695ef1ec0/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv](https://github.com/yhuai/spark/tree/eb77ee39b8616cb367541503baf7c07695ef1ec0/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/csv)
  * [Dataframes from CSV files in Spark 1.5: automatic schema extraction, neat summary statistics, & elementary data exploration](http://www.nodalpoint.com/spark-dataframes-from-csv-files/)
  * [Spark 2.0 read csv number of partitions (PySpark)](http://stackoverflow.com/questions/38128233/spark-2-0-read-csv-number-of-partitions-pyspark)
  * [How to read csv file as DataFrame?](http://stackoverflow.com/questions/29704333/how-to-read-csv-file-as-dataframe)
  * [How to change column types in Spark SQL's DataFrame?](http://stackoverflow.com/questions/29383107/how-to-change-column-types-in-spark-sqls-dataframe)
  * [Working with Nested Data Using Higher Order Functions in SQL on Databricks](https://databricks.com/blog/2017/05/24/working-with-nested-data-using-higher-order-functions-in-sql-on-databricks.html)
    * Hadoop과 Spark은 nested structs, array, map 등과 같은 복잡하고 다양한 데이터를 처리하는 훌륭한 도구이지만 SQL에서 사용하는 건 어려움
    * Databricks 3.0에 추가된 TRANSFORM 연산과 Spark SQL에 추가된 "Higher Order Functions"를 소개(SPARK-19480)
  * [Spark SQL under the hood – part I](http://virtuslab.com/blog/spark-sql-hood-part-i/)
  * [Five Spark SQL Utility Functions to Extract and Explore Complex Data Types - Tutorial on how to do ETL on data from Nest and IoT Devices](https://databricks.com/blog/2017/06/13/five-spark-sql-utility-functions-extract-explore-complex-data-types.html?utm_content=66783528&utm_medium=social&utm_source=linkedin)

# Book
* **[Mastering Apache Spark 2.0](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/)**
* [Advanced Analytics with Spark Source Code](https://github.com/sryza/aas)
* [Best Apache Spark and Scala Books for Mastering Spark Scala](https://data-flair.training/blogs/best-apache-spark-scala-books/)

# Conference
* [Spark Day 2017@Seoul(Spark Bootcamp)](https://www.slideshare.net/SangbaeLim/spark-bootcamp2017inseoul-finalpt20170626distv1)
* [Spark Day 2017- Spark 의 과거, 현재, 미래](https://www.slideshare.net/MoonSooLee2/spark-day-2017-spark)
* [Spark Day 2017 Machine Learning & Deep Learnig With Spark](https://www.slideshare.net/sanghoonlee982/machine-learning-deep-learnig-with-spark)
* [Spark & Zeppelin을 활용한 한국어 텍스트 분류](https://www.slideshare.net/JunKim22/spark-zeppelin-77273056)
  * [Spark & Zeppelin을 활용한 한국어 텍스트 분류](https://www.facebook.com/groups/sparkkoreauser/permalink/1465550026840229/)
* [Zeppelin 노트북: NSMC Word2Vec & Sentiment Classification](https://github.com/uosdmlab/nsmc-zeppelin-notebook)
* [Spark day 2017@Seoul - Spark on Kubernetes](https://www.slideshare.net/jerryjung7/spark-day-2017seoul)
* [Spark, Mesos, Zeppelin, HDFS를 활용한 대용량 보안 데이터 분석](https://engineering.linecorp.com/ko/blog/detail/60)

# Deep Learning
* [yahoo/CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark)
* [CaffeOnSpark Open Sourced for Distributed Deep Learning on Big Data Clusters](http://yahoohadoop.tumblr.com/post/139916563586/caffeonspark-open-sourced-for-distributed-deep)
* [Large Scale Distributed Deep Learning on Hadoop Clusters](http://yahoohadoop.tumblr.com/post/129872361846/large-scale-distributed-deep-learning-on-hadoop)
* [SparkNet: Training Deep Networks in Spark](http://arxiv.org/abs/1511.06051)
  * [Spark + Deep Learning: Distributed Deep Neural Network Training with SparkNet](http://www.kdnuggets.com/2015/12/spark-deep-learning-training-with-sparknet.html)
* [large scale deep-learning_on_spark](http://www.slideshare.net/deview/246-large-scale-deeplearningonspark)
* [DeepSpark: Spark-Based Deep Learning Supporting Asynchronous Updates and Caffe Compatibility](http://hgpu.org/?p=15511)
* [The Unreasonable Effectiveness of Deep Learning on Spark](https://databricks.com/blog/2016/04/01/unreasonable-effectiveness-of-deep-learning-on-spark.html)
* [GPU Acceleration in Databricks Speeding Up Deep Learning on Apache Spark](https://databricks.com/blog/2016/10/27/gpu-acceleration-in-databricks.html)
* [Deep Learning on Databricks - Integrating with TensorFlow, Caffe, MXNet, and Theano](https://databricks.com/blog/2016/12/21/deep-learning-on-databricks.html)

# Hbase
* example
  * [HBaseTest.scala, hbase_inputformat.py](https://gist.github.com/hyunjun/d9d73c5fe8a7f7b17b28)
* [I simple API to interact with HBase with Spark](https://github.com/tmalaska/SparkOnHBase)
* [Apache Spark Comes to Apache HBase with HBase-Spark Module](http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/?elq=b8eb31d395f14250a2c264604a98ed0e&elqCampaignId=987&elqaid=2217&elqat=1&elqTrackId=8472a26fbfcb4511b1a86953234a7bed)

# [Ignite](https://ignite.apache.org/features/igniterdd.html) - Spark Shared RDDs

# Library
* [Hadoop Tutorial: the new beta Notebook app for Spark & SQL](https://vimeo.com/125792752)
* [AWS Athena Data Source for Apache Spark](https://github.com/tmheo/spark-athena)
* [BigDL: Distributed Deep learning on Apache Spark](https://software.intel.com/en-us/articles/bigdl-distributed-deep-learning-on-apache-spark)
  * [BigDL: Distributed Deep learning on Apache Spark](https://github.com/intel-analytics/BigDL)
* [CLOUD DATAPROC - Google Cloud Dataproc is a managed Spark and Hadoop service that is fast, easy to use, and low cost](https://cloud.google.com/dataproc/)
  * [구글, 스파크·하둡 관리 클라우드 서비스 공개](http://www.bloter.net/archives/239483)
  * [Google Cloud Dataproc 사용하기(http://whitechoi.tistory.com/48)
* [CueSheet - a framework for writing Apache Spark 2.x applications more conveniently](https://github.com/kakao/cuesheet)
  * [No More "sbt assembly": Rethinking Spark-Submit using CueSheet](http://www.slideshare.net/jongwookkim/rethinking-sparksubmit-using-cuesheet)
* [Dr. Elephant Self-Serve Performance Tuning for Hadoop and Spark](https://engineering.linkedin.com/blog/2016/04/dr-elephant-open-source-self-serve-performance-tuning-hadoop-spark)
* EMR
  * [Large-Scale Machine Learning with Spark on Amazon EMR](http://blogs.aws.amazon.com/bigdata/post/Tx21LOP0UQ2ZA9N/Large-Scale-Machine-Learning-with-Spark-on-Amazon-EMR)
  * [Amazon EMR, Apache Spark 지원 시작](https://aws.amazon.com/ko/blogs/korea/new-apache-spark-on-amazon-emr/?adbsc=social_20150616_47654126&adbid=1596813583908670&adbpl=fb&adbpr=1563378127252216)
  * [Spark on EMR](https://github.com/awslabs/emr-bootstrap-actions/tree/master/spark)
  * **[(BDT309) Data Science & Best Practices for Apache Spark on Amazon EMR](http://www.slideshare.net/AmazonWebServices/bdt309-data-science-best-practices-for-apache-spark-on-amazon-emr)**
* [Envelope - a configuration-driven framework for Apache Spark that makes it easy to develop Spark-based data processing pipelines on a Cloudera EDH](https://github.com/cloudera-labs/envelope/)
  * Envelope과 함께 Apache Spark, Apache Kudu 및 Apache Impala를 사용하여 Cloudera enterprise data hub (EDH)에 구현하는 방법
  * [Configuration specification](https://github.com/cloudera-labs/envelope/blob/master/docs/configurations.adoc#planners)
  * [Bi-temporal data modeling with Envelope](http://blog.cloudera.com/blog/2017/05/bi-temporal-data-modeling-with-envelope/)
  * [Cloudera Enterprise Data Hub - Our flagship can now be yours](https://www.cloudera.com/products/enterprise-data-hub.html)
* [flambo - A Clojure DSL for Apache Spark](https://github.com/yieldbot/flambo)
* GraphFrame
  * [On-Time Flight Performance with GraphFrames for Apache Spark](https://databricks.com/blog/2016/03/16/on-time-flight-performance-with-spark-graphframes.html)
* [Hail: Scalable Genomics Analysis with Apache Spark](http://blog.cloudera.com/blog/2017/05/hail-scalable-genomics-analysis-with-spark/)
  * Apache Spark로 유전체 분석을 수행하는 도구 인 Hail에 대한 개요
  * 샘플의 품질을 계산하고 간단한 게놈 차원의 연관 연구를 수행하는 예제 실행으로 시연하는 간단하고 강력한 프로그래밍 모델을 보유
* [Infinispan Spark connector 0.1 released!](http://blog.infinispan.org/2015/08/infinispan-spark-connector-01-released.html)
  * [infinispan-spark](https://github.com/infinispan/infinispan-spark)
  * [infinispan-spark-connector-examples](https://github.com/tedwon/infinispan-spark-connector-examples)
* [KeystoneML - Machine Learning Pipeline](http://keystone-ml.org/)
* [Livy, the Open Source REST Service for Apache Spark, Joins Cloudera Labs](http://blog.cloudera.com/blog/2016/07/livy-the-open-source-rest-service-for-apache-spark-joins-cloudera-labs/)
  * [Livy: A REST Web Service For Apache Spark](http://www.slideshare.net/JenAman/livy-a-rest-web-service-for-apache-spark)
* [MMLSpark - Microsoft Machine Learning for Apache Spark](https://github.com/Azure/mmlspark)
  * [Accelerated Spark on GPU-enabled clusters in Azure](https://azure.microsoft.com/en-us/blog/accelerated-spark-on-gpu-enabled-clusters-in-azure/?_lrsc=ff381697-5454-493b-9666-eaeaf066ba16)
* [Oryx 2: Lambda architecture on Apache Spark, Apache Kafka for real-time large scale machine learning http://oryx.io](https://github.com/OryxProject/oryx)
  * [Production Recommendation Systems with Cloudera](http://blog.cloudera.com/blog/2018/02/production-recommendation-systems-with-cloudera/)
  * Kafka + Spark + Cloudera Hadoop 를 이용한 추천시스템
* [pocketcluster - One-Step Spark/Hadoop Installer v0.1.0](https://github.com/stkim1/pocketcluster)
* [Ranking Algorithms for Spark Machine Learning Pipeline](https://github.com/oeegee/spark-ranking-algorithms) BM 25 + Wilson score on spark 2.2.0
* [snappydata - Unified Online Transactions + Analytics + Probabilistic Data Platform](http://www.snappydata.io/blog/snappydata-technical-vision)
  * [SnappyData: OLTP + OLAP Database built on Apache Spark http://www.snappydata.io](https://github.com/SnappyDataInc/snappydata)
* [spark cassandra connector - 스파크에 카산드라 연동하는 라이브러리](http://knight76.tistory.com/entry/spark-spark-cassandra-connector-%EC%8A%A4%ED%8C%8C%ED%81%AC%EC%97%90-%EC%B9%B4%EC%82%B0%EB%93%9C%EB%9D%BC-%EC%97%B0%EB%8F%99%ED%95%98%EB%8A%94-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC)
* [spark-indexed - An efficient updatable key-value store for Apache Spark](https://github.com/amplab/spark-indexedrdd)
* [spark-jobs-rest-client - Fluent client for interacting with Spark Standalone Mode's Rest API for submitting, killing and monitoring the state of jobs](https://github.com/ywilkof/spark-jobs-rest-client)
* [Sparkline SNAP](http://sparklinedata.com/)
  * [Introducing Sparkline SNAP: An Integrated OLAP platform on Spark](http://sparklinedata.com/sparkline-snap-olap-on-spark/)
* [Sparklint - The missing Spark Performance Debugger that can be drag and dropped into your spark application!](https://github.com/groupon/sparklint)
  * [SparkLint: a Tool for Monitoring, Identifying and Tuning Inefficient Spark Jobs (Simon Whitear)](https://www.youtube.com/watch?v=reGerTzcvoA)
* [spark-nkp Natural Korean Processor for Apache Spark](https://github.com/uosdmlab/spark-nkp)
* [Spark Notebook](http://spark-notebook.io/)
* [SparMysqlSample](https://github.com/hoonmokmoon/SparMysqlSample)
* [spark-packages - A community index of packages for Apache Spark](http://spark-packages.org/)
  * [스칼라 의존성, 패키지 검색하는 웹 - http://spark-packages.org](http://knight76.tistory.com/entry/scala-%EC%8A%A4%EC%B9%BC%EB%9D%BC-%EC%9D%98%EC%A1%B4%EC%84%B1-%ED%8C%A8%ED%82%A4%EC%A7%80-%EA%B2%80%EC%83%89%ED%95%98%EB%8A%94-%EC%9B%B9-httpsparkpackagesorg)
* [spark-ts - Time Series for Spark (The spark-ts Package)](https://github.com/sryza/spark-timeseries)
* [spark-xml - XML data source for Spark SQL and DataFrames](https://github.com/databricks/spark-xml)

# [GraphX](https://spark.apache.org/docs/1.0.0/graphx-programming-guide.html)
* [Spark Streaming and GraphX at Netflix - Apache Spark Meetup, May 19, 2015](https://www.youtube.com/watch?v=gqgPtcDmLGs)
* [스사모 테크톡 - GraphX](http://www.slideshare.net/sangwookimme/graphx)
* [Computing Shortest Distances Incrementally with Spark](http://insightdataengineering.com/blog/incr-short-dist-graphx/)
* [Strata 2016 - This repo is for MLlib/GraphX tutorial in Strata 2016](https://github.com/jayantshekhar/strata-2016)
* [Processing Hierarchical Data using Spark Graphx Pregel API](http://www.qubole.com/blog/processing-hierarchical-data-using-spark-graphx-pregel-api/)
  * GraphX API를 사용하는 예제와 방법

# Mesos
* [Spark + Mesos cluster mode, who uploads the jar?](http://stackoverflow.com/questions/33978672/spark-mesos-cluster-mode-who-uploads-the-jar)

# MLLib
* [Decision Trees](http://spark.apache.org/docs/latest/mllib-decision-tree.html)
* [MLlib: Machine Learning in Apache Spark](http://arxiv.org/pdf/1505.06807.pdf)
* [movie recommendation with mllib](http://ampcamp.berkeley.edu/big-data-mini-course/movie-recommendation-with-mllib.html)
* [WSO2 Machine Learner: Why would You care?](https://iwringer.wordpress.com/2015/09/25/wso2-machine-learner-why-would-you-care/)
* [Strata 2016 - This repo is for MLlib/GraphX tutorial in Strata 2016](https://github.com/jayantshekhar/strata-2016)
* [Spark ML Lab](https://github.com/Pivotal-Open-Source-Hub/StockInference-Spark/blob/master/SparkML.md)
* [Apache Spark로 시작하는 머신러닝 입문](https://www.youtube.com/watch?v=PRLz11vv7VA)
  * [Apache Spark 입문에서 머신러닝까지](http://www.slideshare.net/DonamKim/apache-spark-64226109)
* [Generating Recommendations at Amazon Scale with Apache Spark and Amazon DSSTNE](http://blogs.aws.amazon.com/bigdata/post/TxGEL8IJ0CAXTK/Generating-Recommendations-at-Amazon-Scale-with-Apache-Spark-and-Amazon-DSSTNE)
* [Deep Learning with Apache Spark and TensorFlow](https://databricks.com/blog/2016/01/25/deep-learning-with-apache-spark-and-tensorflow.html)
* [Introduction to Machine Learning on Apache Spark MLlib](https://www.youtube.com/watch?v=qKYpMPPL-fo)
* [pipelineio - End-to-End Spark ML and Tensorflow AI Data Pipelines](http://pipeline.io/)
* [TensorFlow On Spark: Scalable TensorFlow Learning on Spark Clusters - Andy Feng & Lee Yang](https://www.youtube.com/watch?v=IxWfAcrZQhc&sns=fb)
  * [BigData와 결합한, 분산 Deep Learning 그 의미와 접근 방법에 대하여](http://hoondongkim.blogspot.com/2017/09/bigdata-distributed-deep-learning.html)
* [github.com/yahoo/TensorFlowOnSpark](https://github.com/yahoo/TensorFlowOnSpark)
  * [Open Sourcing TensorFlowOnSpark: Distributed Deep Learning on Big-Data Clusters](http://yahoohadoop.tumblr.com/post/157196317141/open-sourcing-tensorflowonspark-distributed-deep)
* [Extend Spark ML for your own model/transformer types](https://www.oreilly.com/learning/extend-spark-ml-for-your-own-modeltransformer-types)
* [Deep learning for Apache Spark](https://www.oreilly.com/ideas/deep-learning-for-apache-spark)
* [Spark machine learning & deep learning](https://www.slideshare.net/ssusere94328/spark-machine-learning-deep-learning)
* [Accelerating Apache Spark MLlib with Intel® Math Kernel Library (Intel® MKL)](http://blog.cloudera.com/blog/2017/02/accelerating-apache-spark-mllib-with-intel-math-kernel-library-intel-mkl/)
* [Improving BLAS library performance for MLlib](http://www.spark.tc/blas-libraries-in-mllib/)
* [Introduction to Machine learning with Spark](http://blog.madhukaraphatak.com/machine-learning-with-spark/)
  * [Introduction to Machine Learning with Spark](https://www.slideshare.net/datamantra/introduction-to-machine-learning-with-spark)
  * [Code and setup information for Introduction to Machine Learning with Spark](https://github.com/phatak-dev/introduction_to_ml_with_spark)
* [Introduction to ML with Apache Spark MLib by Taras Matyashovskyy](https://www.youtube.com/watch?v=szpcW-SEJK4)
* [Extend Spark ML for your own model/transformer types](https://www.oreilly.com/learning/extend-spark-ml-for-your-own-modeltransformer-types)
* [Spark Deep Learning Pipelines](https://www.facebook.com/nextobe1/posts/342192122883456)

# [PySpark](http://spark.apache.org/docs/latest/api/python/)
* [PySpark & Hadoop: 1) Ubuntu 16.04에 설치하기](https://beomi.github.io/2017/11/09/Install-PySpark-and-Hadoop-on-Ubuntu-16-04/)
* [PySpark & Hadoop: 2) EMR 클러스터 띄우고 PySpark로 작업 던지기](https://beomi.github.io/2017/11/27/EMR-and-PySpark/)
* [PySpark Cheat Sheet: Spark in Python](https://www.datacamp.com/community/blog/pyspark-cheat-sheet-python)
* troubleshooting
  * [A Beginner's Guide on Troubleshooting Spark Applications](http://www.slideshare.net/jcmia1/a-beginners-guide-on-troubleshooting-spark-applications)
  * `Caused by: java.lang.ClassNotFoundException: * org.elasticsearch.spark.package` sbt configuration such as resolvers
    * [Spark Runtime Error - ClassDefNotFound: SparkConf](http://stackoverflow.com/questions/31171825/spark-runtime-error-classdefnotfound-sparkconf)
  * `java.lang.OutOfMemoryError: GC overhead limit exceeded` increase driver memory
  * `org.apache.spark.SparkException: Could not find BlockManagerEndpoint1 or it has been stopped` 검색해도 특별히 나오는게 없음
  * `spark java.io.IOException: Filesystem closed` usually result RDD is too big
  * `Task not serializable`
    * [Spark - Task not serializable: How to work with complex map closures that call outside classes/objects?](http://stackoverflow.com/questions/23050067/spark-task-not-serializable-how-to-work-with-complex-map-closures-that-call-o)
    * [Task not serializable: java.io.NotSerializableException when calling function outside closure only on classes not objects](http://stackoverflow.com/questions/22592811/task-not-serializable-java-io-notserializableexception-when-calling-function-ou)
    * [java+spark: org.apache.spark.SparkException: Job aborted: Task not serializable: java.io.NotSerializableException](http://stackoverflow.com/questions/24046744/javaspark-org-apache-spark-sparkexception-job-aborted-task-not-serializable)
  * `TypeError: 'bool' object is not callable` Use `PYSPARK_PYTHON=...`
    * [Check Python version in worker before run PySpark job](https://issues.apache.org/jira/browse/SPARK-6216)
    * [spark-runs-in-local-but-not-in-yarn](http://stackoverflow.com/questions/28879803/spark-runs-in-local-but-not-in-yarn)
  * `yarn.scheduler.maximum.allocation-mb`
    * increase configuration for yarn-site.xml
    * empty disk (not enough free space may cause this too)
  * [Cannot submit Spark app to cluster, stuck on “UNDEFINED”](http://stackoverflow.com/questions/26883701/cannot-submit-spark-app-to-cluster-stuck-on-undefined)
    * `yarn.nodemanager.resource.memory-mb` 조정 후 동작 확인
  * `contains a task of very large size warning`
    * 문제; Dataframe으로 읽어 온 row들을 텍스트 처리 해서 row끼리 비교를 해야 하는데, a task of very large size warning 발생
    * 해결; 텍스트 처리 된 중간 결과물을 Redis에 저장한 뒤 별도 Spark 애플리케이션을 사용해서 Row by Row 처리
    * 원인
      * Spark는 각 Executor가 수행해야 할 작업을 Task라는 단위로 관리
      * RDD에 가해지는 연산을 상호 의존성에 따라 묶은 뒤 (Logical Planning) 여기에 최적화 룰을 적용해서 실제로 Executor가 처리해야 할 Task의 형태로 생성 (Physical Planning)
      * 이걸 내부 queue에 넣어 뒀다가 순차적으로 Executor에 보내서 처리
      * 이 과정을 좀 더 구체적으로 설명하자면, Driver 프로세스가 작업 루틴과 작업 대상 위치를 TaskDescription 객체로 만든 뒤 Serialize를 해서 Worker
프로세스에 네트워크 상으로 전송
      * 문제는 Task당 100kb를 넘으면 "contains a task of very large size warning" 경고 발생
      * 이 제한은 소스코드 안에 하드 코딩되어 있어 변경 불가능
      * broadcast 기능을 사용할 경우 상황은 더 악화
      * broadcast 기능은 task를 전송할 때와는 달리 데이터 값 그 자체를 Worker에 하나하나 보내는 방식으로 동작
      * 이 경우 보내야 할 row가 한두 개가 아니므로, 당연히 성능에 문제 발생
      * 이런 이유 때문에 자연어 처리가 된 중간 결과물을 별도 스토리지에 저장한 뒤 별도 애플리케이션에서 읽어와서 처리하는 방법만 가능
      * 여러 storage 중에서 굳이 Redis를 추천하는 이유는 빠르고, Key-Value Store라 관리하기 좋고, Sharding 기능 덕분에 읽기 분산도 잘 동작하기 때문
      * 최근 Spark ML에서 학습된 모델이 Redis에 저장되는 식으로 개발되고 있음
  * [Spark Interpreter 이슈 해결](http://arclab.tistory.com/128)
* [Getting started with PySpark - Part 1](http://www.mccarroll.net/blog/pyspark/index.html)
* [Getting started with PySpark - Part 2](http://www.mccarroll.net/blog/pyspark2/)
* [PySpark Internals](https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals)
* [Fast Data Analytics with Spark and Python](http://www.slideshare.net/BenjaminBengfort/fast-data-analytics-with-spark-and-python)
* [pyspark-hbase.py](https://gist.github.com/MLnick/6ec916b646c3004d7523)
* [Deploying PySpark on Red Hat Storage GlusterFS](http://redhatstorage.redhat.com/2015/02/17/deploying-pyspark-on-red-hat-storage-glusterfs/)
* [Spark Python Performance Tuning](http://stackoverflow.com/questions/27757117/spark-python-performance-tuning)
* [weird case from pyspark-hbase (utf8 & unicode mixed)](https://gist.github.com/hyunjun/dea65972f3f723c0ad77)
* [Python Versus R in Apache Spark](http://www.datanami.com/2015/07/13/python-versus-r-in-apache-spark/)
* [biospark](https://github.com/biospin/biospark)
* [Plagiarizing and Paraphrasing Code From an Online Class for Content Marketing](http://minimaxir.com/2015/09/code-of-plagiarism/)
* [How-to: Use IPython Notebook with Apache Spark](http://blog.cloudera.com/blog/2014/08/how-to-use-ipython-notebook-with-apache-spark/)
* [Configuring IPython Notebook Support for PySpark](http://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/)
* [pyADAM - This is a wrapper to load Parquet data in PySpark](https://github.com/arahuja/pyadam)
* [Accessing PySpark in PyCharm](http://renien.github.io/blog/accessing-pyspark-pycharm/)
* [pyspark-project-example - A simple example for PySpark based project](https://github.com/HyukjinKwon/pyspark-project-example)
* [Recommendation Systems for Implicit Feedback](https://github.com/csung7/Recommendation-Systems-for-Implicit-Feedback)
* [Hassle Free ETL with PySpark](https://www.youtube.com/watch?v=1L6wp7AxfPE)
* [안명호 : Python + Spark, 머신러닝을 위한 완벽한 결혼 - PyCon APAC 2016](https://www.youtube.com/watch?v=JEBNNE09JEQ)
* [Fully Arm Your Spark with Ipython and Jupyter in Python 3](https://mengdong.github.io/2016/08/08/fully-armed-pyspark-with-ipython-and-jupyter/)
  * [Installation](http://toree.apache.org/documentation/user/installation.html)
* [PySpark Cheat Sheet: Spark in Python](http://www.datasciencecentral.com/profiles/blogs/pyspark-cheat-sheet-spark-in-python)
* **[Apache Spark for Data Science](https://www.youtube.com/playlist?list=PL0B_bv6Hd87dTd5890-nPKwl2JoqCATAf)**
* [BigDL on CDH and Cloudera Data Science Workbench](http://blog.cloudera.com/blog/2017/04/bigdl-on-cdh-and-cloudera-data-science-workbench/) BigDL (Apache Spark의 심층 학습 라이브러리)을 워크 벤치와 함께 사용하는 방법
* [Distributed Deep Learning At Scale On Apache Spark With BigDL](https://www.slideshare.net/YuliaTell/distributed-deep-learning-at-scale-on-apache-spark-with-bigdl)
* [Deep Learning to Big Data Analytics on Apache Spark Using BigDL - Yuhao Yang & Xianyan Jia](https://www.youtube.com/watch?v=cqUvrs2PPOY)
* [Deep Learning on Qubole Using BigDL for Apache Spark – Part 2](https://www.qubole.com/blog/deep-learning-qubole-using-bigdl-apache-spark-part-2/)
  * 딥러닝 라이브러리인 BigDL을 사용하여 모델을 학습하고 평가하는 방법을 보여주는 간단한 자습서
* [Use your favorite Python library on PySpark cluster with Cloudera Data Science Workbench](http://blog.cloudera.com/blog/2017/04/use-your-favorite-python-library-on-pyspark-cluster-with-cloudera-data-science-workbench/) Python 라이브러리를 사용하는 PySpark 작업을 작성하는 방법
* [Install Spark on Windows (PySpark)](https://medium.com/@GalarnykMichael/install-spark-on-windows-pyspark-4498a5d8d66c)
* [PySpark & Hadoop: 1) Ubuntu 16.04에 설치하기](https://beomi.github.io/2017/11/09/Install-PySpark-and-Hadoop-on-Ubuntu-16-04/)
* [Get Started with PySpark and Jupyter Notebook in 3 Minutes](https://blog.sicara.com/get-started-pyspark-jupyter-guide-tutorial-ae2fe84f594f)
* [Getting The Best Performance With PySpark](https://www.slideshare.net/SparkSummit/getting-the-best-performance-with-pyspark)

# R
* [Spark 1.4 for RStudio](http://www.r-bloggers.com/spark-1-4-for-rstudio/)
* [Python Versus R in Apache Spark](http://www.datanami.com/2015/07/13/python-versus-r-in-apache-spark/)
* [SparkR 설치 사용기 1 - Installation Guide On Yarn Cluster & Mesos Cluster & Stand Alone Cluster](http://hoondongkim.blogspot.com/2016/01/sparkr-1-installation-guide-on-yarn.html)
* [sparklyr — R interface for Apache Spark](http://spark.rstudio.com/index.html)
* [sparklyr — R interface for Apache Spark](https://blog.rstudio.org/2016/09/27/sparklyr-r-interface-for-apache-spark/)
* [sparklyr](https://drive.google.com/file/d/0Bw594TdiBdAUUWt6eGd0Vm5fWDg/view)
* [xwMOOC 기계학습 - dplyr을 Spark 위에 올린 sparklyr](http://statkclee.github.io/ml/ml-sparklyr.html)
* [sparklyr – An  R interface for Apache Spark](https://cdn.oreillystatic.com/en/assets/1/event/193/Sparklyr_%20An%20R%20interface%20for%20Apache%20Spark%20Presentation.pdf)
* [spark + R](https://drive.google.com/file/d/0Bw594TdiBdAUTGtUOERoOG1ac1E/view)
* [MS R(구 Revolution R) on Spark - 설치 및 가능성 엿보기(feat. SparkR)](http://hoondongkim.blogspot.com/2016/12/ms-r-revolution-r-on-spark-feat-sparkr.html)
* [빅데이터 분석을 위한 스파크 2 프로그래밍 : 대용량 데이터 처리부터 머신러닝까지](http://www.slideshare.net/ssuser88c366/2-71401153)
* [On-Demand Webinar and FAQ: Parallelize R Code Using Apache Spark](https://databricks.com/blog/2017/08/21/on-demand-webinar-and-faq-parallelize-r-code-using-apache-spark.html)

# Spark DL
* [A Vision for Making Deep Learning Simple From Machine Learning Practitioners to Business Analysts](https://databricks.com/blog/2017/06/06/databricks-vision-simplify-large-scale-deep-learning.html)

# Spark ML
* [KeystoneML - Machine Learning Pipeline](http://keystone-ml.org/)
* [Feature Engineering at Scale With Spark](http://eugenezhulenev.com/blog/2015/06/10/feature-engineering-at-scale/)
* [Audience Modeling With Spark ML Pipelines](http://eugenezhulenev.com/blog/2015/09/09/audience-modeling-with-spark-ml-pipelines/)

# Spark SQL
* [Spark SQL, DataFrames and Datasets Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)
* [분석가의 Spark SQL](https://wikidocs.net/book/1686)
* [Deep Dive into Spark SQL’s Catalyst Optimizer](https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html)
* [SparkSQL cacheTable 메소드 사용 성능 비교 - default vs cacheTable vs cacheTable (with columnar Compression)](http://hoondongkim.blogspot.kr/2015/04/sparksql-cachetable-default-vs.html?spref=fb)
* [SparkSQL Internals](http://www.trongkhoanguyen.com/2015/08/sparksql-internals.html)
* [Spark Data Source API. Extending Our Spark SQL Query Engine](https://hackernoon.com/extending-our-spark-sql-query-engine-5f4a088de986)
* [Five Spark SQL Utility Functions to Extract and Explore Complex Data Types](https://databricks.com/blog/2017/06/13/five-spark-sql-utility-functions-extract-explore-complex-data-types.html)
  * JSON 및 중첩 구조를 처리하기 위해 탑재된 Spark SQL 함수를 사용하기 위한 튜토리얼
* [FLARE: SCALE UP SPARK SQL WITH NATIVE COMPILATION AND SET YOUR DATA ON FIRE!](https://spark-summit.org/2017/events/flare-scale-up-spark-sql-with-native-compilation-and-set-your-data-on-fire/)
  * 실험 단계
  * 쿼리플랜을 native code로 바꾸고 spark runtime system도 수정해 Spark SQL성능을 대폭 향상
  * [Flare: Native Compilation for Heterogeneous Workloads in Apache Spark](https://arxiv.org/pdf/1703.08219.pdf)
* [MatFast: In-Memory Distributed Matrix Computation Processing and Optimization Based on Spark SQL](https://databricks.com/session/matfast-in-memory-distributed-matrix-computation-processing-and-optimization-based-on-spark-sql)
  * [Apache Spark™ Distributed Matrix Computation](https://github.com/yuyongyang800/SparkDistributedMatrix)

# Streaming
* [Improved Fault-tolerance and Zero Data Loss in Spark Streaming](https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html)
* [Four Things to know about Reliable Spark Streaming](http://www.typesafe.com/resources/video/four-things-to-know-about-reliable-spark-streaming)
* [Improved Fault-tolerance and Zero Data Loss in Spark Streaming](https://databricks.com/blog/2015/01/15/improved-driver-fault-tolerance-and-zero-data-loss-in-spark-streaming.html)
* [Real Time Data Processing using Spark Streaming | Data Day Texas 2015](http://www.slideshare.net/cloudera/spark-streamingdatadaytexas-43476479)
* [Real-Time Analytics with Spark Streaming](http://qconsp.com/sp2015/system/files/presentation-slides/QCon_paco.ok_.pdf)
  * [Diving into Spark Streaming’s Execution Model](https://databricks.com/blog/2015/07/30/diving-into-spark-streamings-execution-model.html)
* [Can Spark Streaming survive Chaos Monkey?](http://techblog.netflix.com/2015/03/can-spark-streaming-survive-chaos-monkey.html)
* [RecoPick 실시간 데이터 처리 시스템 전환기 (Storm에서 Spark Streaming으로 전환)](http://readme.skplanet.com/?p=13297)
* [From Big Data to Fast Data in Four Weeks or How Reactive Programming is Changing the World – Part 2](https://www.paypal-engineering.com/2016/11/18/from-big-data-to-fast-data-in-four-weeks-or-how-reactive-programming-is-changing-the-world-part-2/)
* [Spark Streaming으로 유실 없는 스트림 처리 인프라 구축하기](http://readme.skplanet.com/?p=12465)
* [Real-time Streaming ETL with Structured Streaming in Apache Spark 2.1](https://databricks.com/blog/2017/01/19/real-time-streaming-etl-structured-streaming-apache-spark-2-1.html)
* [Handling empty batches in Spark streaming](http://blog.madhukaraphatak.com/handling-empty-rdd-in-spark-streaming/)
* [Spark Streaming Example(예제로 알아보는 Spark Streaming)](http://hellowuniverse.com/2017/03/23/spark-streaming-example%EC%98%88%EC%A0%9C%EB%A1%9C-%EC%95%8C%EC%95%84%EB%B3%B4%EB%8A%94-spark-streaming/)
* [Long-running Spark Streaming Jobs on YARN Cluster](http://mkuthan.github.io/blog/2016/09/30/spark-streaming-on-yarn/)
  * spark-submit으로 장기간 streaming 분석 작업 실행하기
* [Spark Streaming 운영과 회고](http://slides.com/yonghweekim/streaming-system#/)
* [Deep Learning and Streaming in Apache Spark 2 x - Matei Zaharia & Sue Ann Hong](https://www.youtube.com/watch?v=zom9J9sK6wY)
* [24/7 Spark Streaming on YARN in Production](https://www.inovex.de/blog/247-spark-streaming-on-yarn-in-production/)

# YARN
* [Running Spark on YARN](http://spark.apache.org/docs/latest/running-on-yarn.html)
* [Apache Spark Resource Management and YARN App Models](http://blog.cloudera.com/blog/2014/05/apache-spark-resource-management-and-yarn-app-models/)
* [Spark-on-YARN: Empower Spark Applications on Hadoop Cluster](http://www.slideshare.net/Hadoop_Summit/sparkonyarn-empower-spark-applications-on-hadoop-cluster)
* [Spark Yarn Cluster vs Spark Mesos Cluster (vs 기타 다양한 모드) 수행성능 및 활용성 비교](http://hoondongkim.blogspot.kr/2015/10/spark-yarn-cluster-vs-spark-mesos.html)
* [Dynamic Resource Allocation Spark on YARN](http://www.slideshare.net/ozax86/spark-on-yarn-with-dynamic-resource-allocation)
* [Investigation of Dynamic Allocation in Spark](http://jerryshao.me/architecture/2015/08/22/spark-dynamic-allocation-investigation/)
* [Spark Cluster Settings On Yarn : Spark 1.4.1 + Hadoop 2.7.1](http://hoondongkim.blogspot.com/2015/08/spark-cluster-settings-on-yarn-spark.html)

# [Zeppelin](http://zeppelin-project.org/)
* [Apache Zeppelin Release 0.7.0](http://zeppelin.apache.org/releases/zeppelin-release-0.7.0.html)
* [www.zepl.com](https://www.zepl.com/) previously www.zeppelinhub.com
* Practice
  * [meetup](https://github.com/hyunjun/practice/tree/master/spark/meetup)
* [Introduction to Zeppelin](http://www.slideshare.net/KSLUG/kslug-zeppelin)
* [Zeppelin overview](https://www.youtube.com/watch?v=_PQbVH_aO5E&feature=youtu.be)
* [Zepplin (제플린) 설치하기](http://bcho.tistory.com/1022)
* [5. 웹 기반 명령어 해석기 Zeppelin Install](http://pubdata.tistory.com/28)
* [Angular display system dashboard on Zeppelin](https://www.youtube.com/watch?v=QdjZyOkcG_w)
* [Apache Zeppelin으로 데이터 분석하기 by VCNC](https://speakerdeck.com/vcnc/apache-zeppelineuro-deiteo-bunseoghagi)
* [Zeppelin Context](http://zeppelin-project.org/docs/zeppelincontext.html)
* [Apache Tajo 데스크탑 + Zeppelin 연동 하기](http://jjeong.tistory.com/1031)
* [How-to: Install Apache Zeppelin on CDH](http://blog.cloudera.com/blog/2015/07/how-to-install-apache-zeppelin-on-cdh/)
* [제플린 탑재한 이엠알 16년 4월](http://www.slideshare.net/diginorimin/16-4-60944385)
* [Zeppelin at Twitter](http://www.slideshare.net/prasadwagle/zeppelin-at-twitter-62171116)
* [아파치 제플린, 한국에서 세계로 가기까지](http://m.zdnet.co.kr/news_view.asp?article_id=20160601155438)
* [Zeppelin Lab](https://github.com/Pivotal-Open-Source-Hub/StockInference-Spark/blob/master/Zeppelin.md)
* [Presto, Zeppelin을 이용한 초간단 BI 구축 사례](http://www.slideshare.net/babokim/presto-zeppelin-bi)
* [Presto, Zeppelin을 이용한 초간단 BI 시스템 구축 사례(1)](http://www.popit.kr/presto-zeppelin%EC%9D%84-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%B4%88%EA%B0%84%EB%8B%A8-bi-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EA%B5%AC%EC%B6%95-%EC%82%AC%EB%A1%80-1/)
* [Serving Shiro enabled Apache Zeppelin with Apache mod_proxy + SSL (https)](https://nazgul33.wordpress.com/2016/08/31/serving-shiro-enabled-apache-zeppelin-with-apache-mod_proxy-ssl-https/)
* [Analyzing BigQuery datasets using BigQuery Interpreter for Apache Zeppelin](https://cloud.google.com/blog/big-data/2016/09/analyzing-bigquery-datasets-using-bigquery-interpreter-for-apache-zeppelin)
* [Zeppelin(제플린) 서울시립대학교 데이터 마이닝연구실 활용사례](http://www.slideshare.net/JunKim22/zeppelin-66264643)
  * [제플린 걸음마 서울시립대학교 데이터마이닝 활용사례 제플린 노트북 통계 추출 코드](https://gist.github.com/tae-jun/138f595228aa83e89387b5d39d33b315)
* [노트7의 소셜 반응을 분석해 보았다. #3 제플린 노트북을 이용한 상세 분석](http://bcho.tistory.com/1138)
* [9월 발렌타인 웨비너 - 민경국님의 Apache Zeppelin 입문 온라인 헨즈온강의](https://www.youtube.com/watch?v=VlqTPZVyP9Y)
* [오픈소스 일기 2: Apache Zeppelin 이란 무엇인가?](https://medium.com/apache-zeppelin-stories/%EC%98%A4%ED%94%88%EC%86%8C%EC%8A%A4-%EC%9D%BC%EA%B8%B0-2-apache-zeppelin-%EC%9D%B4%EB%9E%80-%EB%AC%B4%EC%97%87%EC%9D%B8%EA%B0%80-f3a520297938)
* [How Apache Zeppelin runs a paragraph](https://medium.com/apache-zeppelin-stories/how-apache-zeppelin-runs-a-paragraph-783a0a612ba9)
* **[Spark & Zeppelin을 활용한 머신러닝 실전 적용기](http://www.slideshare.net/JunKim22/spark-zeppelin)**
  * [Zeppelin 화재 뉴스 기사 분류 예제](https://github.com/uosdmlab/playdata-zeppelin-notebook)
* [스파크-제플린으로 통계 그래프 출력하기(윈도우환경)](http://blog.daum.net/web_design/396) 실패 이야기
* [Apache Zeppelin Data Science Environment 1/21/16](https://www.youtube.com/watch?v=chPw8Ts7ZW8)
* [도커로 간단 설치하는 Zeppelin](https://docs.google.com/presentation/d/1iUlprfqeQaXuW63qQpb7eHkV3oiegtl3OOylHpX6dGg/edit)
* [Zeppelin Build and Tutorial Notebook](https://www.youtube.com/watch?v=CfhYFqNyjGc)
* [DIT4C image for Apache Zeppelin](https://hub.docker.com/r/dit4c/dit4c-container-zeppelin/)
* [zdairi is zeppelin CLI tool](https://github.com/del680202/zdairi)
* [Zeppelin Paragraph 공유 시 자동 로그인 구현](http://www.popit.kr/zeppelin-paragraph-%EA%B3%B5%EC%9C%A0-%EC%8B%9C-%EC%9E%90%EB%8F%99-%EB%A1%9C%EA%B7%B8%EC%9D%B8-%EA%B5%AC%ED%98%84/)
* [25분 만에 Apache Zeppelin 으로 대시보드 만들기 - 박훈(@1ambda)](https://www.youtube.com/watch?v=VKMB8nFhjug&feature=youtu.be)
* [Using Amazon Athena with Apache Zeppelin](https://medium.com/@yutaimai/using-amazon-athena-with-apache-zeppelin-464a85678c46)
