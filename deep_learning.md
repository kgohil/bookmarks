Deep Learning
=============
* [입문자를 위한 딥러닝 튜토리얼 ](http://courseshare.co.kr/course/39?pageType=Intro)
* [테리의 딥러닝 토크](https://www.youtube.com/playlist?list=PL0oFI08O71gKEXITQ7OG2SCCXkrtid7Fq)
* [쉽게 풀어쓴 딥러닝(Deep Learning)의 거의 모든 것](http://t-robotics.blogspot.kr/2015/05/deep-learning.html)
* [딥러닝 기본 원리의 이해](https://www.slideshare.net/HeeWonPark11/ss-80653977)
* [완전쉬운 딥러닝](https://docs.google.com/document/d/11A7207YsYcKU7F3uq117pNGRGIReEP88__gZmflIXrs/edit)
  * [완전쉬운 딥러닝 동영상 원고](http://kr.deductiontheory.com/2017/03/blog-post_3.html)
  * [Super Easy Deep Learning](https://www.youtube.com/watch?v=8NkZohHnxck&feature=youtu.be)
* [자습해도 모르겠던 딥러닝, 머리속에 인스톨 시켜드립니다](https://www.slideshare.net/yongho/ss-79607172)
* [맥주마시며 만들어본 딥러닝 맥주 추천엔진](http://freesearch.pe.kr/archives/4656) python
* [구글, 유다시티에 딥러닝 강의 무료 공개](http://www.bloter.net/archives/248374)
* [헬로 딥러닝 : 쉽고도 명확하게 이해하는 딥러닝](https://www.youtube.com/playlist?list=PLefQdA1SdkhsO4yGqIFAWcG6vr211di1j)
* [모두를 위한 딥러닝 강좌](http://www.se.or.kr/m/post/161)
  * **[기본적인 머신러닝과 딥러닝 강의](http://hunkim.github.io/ml/)**
  * [TensorFlow-Tutorials](https://github.com/hunkim/TensorFlow-Tutorials)
  * [TensorFlow Basic Tutorial Labs](https://github.com/hunkim/DeepLearningZeroToAll)
  * [Lec 00 - Machine/Deep learning 수업의 개요와 일정](https://www.youtube.com/watch?v=BS6O0zOGX4E)
  * [lec11-1 ConvNet의 Conv 레이어 만들기](https://www.youtube.com/watch?v=Em63mknbtWo&feature=youtu.be)
  * [lab11: ConvNet을 TensorFlow로 구현하자 (MNIST 99%)](https://www.youtube.com/watch?v=6KlkiKyjEu0&feature=youtu.be)
  * [lec12: NN의 꽃 RNN 이야기](https://www.youtube.com/watch?v=-SHPG_KMUkQ&feature=youtu.be)
  * [lab12: TensorFlow에서 RNN 구현하기](https://www.youtube.com/watch?v=A8wJYfDUYCk&feature=youtu.be)
* [자바로 Mnist 구현하고 스프링웹서버붙이기](https://www.slideshare.net/meadunhansa/mnist)
* [모두를 위한 딥러닝 강좌](http://www.se.or.kr/161)
* [C++로 배우는 딥러닝](http://blog.naver.com/atelierjpro/220697890605)
  * [C++로 배우는 딥러닝](https://www.youtube.com/playlist?list=PLNfg4W25Tapy5hIBmFZgT5coii1HUX6BD)
  * [딥러닝 4-3. 프로그래머를 위한 경사 하강법 The Gradient Descent Method for Programmers](http://blog.naver.com/atelierjpro/220755873110)
  * [딥러닝 4-4. 프로그래머를 위한 연쇄 미분 Chain Rule](http://m.blog.naver.com/atelierjpro/220760659825)
  * [딥러닝 4.4 - 연쇄 미분 ChainRule](https://www.youtube.com/watch?v=g3nhLjYRT5I&feature=youtu.be)
  * [딥러닝 6. Fully Connected Neural Network](http://blog.naver.com/atelierjpro/220773276384)
  * [딥러닝 7. Implementing FCNN](http://blog.naver.com/atelierjpro/220774988242)
* [딥러닝 용어 정리](http://docs.likejazz.com/deep-learning-glossary/)
* [완전쉬운 딥러닝](http://kr.deductiontheory.com/2017/01/blog-post.html)
* [수학포기자를 위한 딥러닝-#1 머신러닝과 딥러닝 개요](http://bcho.tistory.com/1140)
* [수학포기자를 위한 딥러닝-#2 - 선형회귀분석을 통한 머신러닝의 기본 개념 이해](http://bcho.tistory.com/1139)
* [수학포기자를 위한 딥러닝-#3 텐서플로우로 선형회귀 학습을 구현해보자](http://bcho.tistory.com/1141)
* [수학포기자를 위한 딥러닝-#4 로지스틱 회귀를 이용한 분류 모델](http://bcho.tistory.com/1142)
* [수학포기자를 위한 딥러닝과 텐서플로우의 이해](http://bcho.tistory.com/1208)
* [딥러닝 마일스톤](https://www.facebook.com/nextobe1/posts/344853295950672)
* [fast.ai: How I built a deep learning application to detect invasive species in just 1 day (and for $12.60)](https://medium.com/the-business-of-ai/fast-ai-how-i-built-a-deep-learning-application-to-detect-invasive-species-in-just-1-day-and-for-38e0ced809e9)
* [러닝 딥러닝](https://www.youtube.com/playlist?list=PL1H8jIvbSo1q6PIzsWQeCLinUj_oPkLjc)
* [딥러닝 강의 자료 - 패스트캠퍼스의 비전공자를 위한 데이터 사이언스 스쿨에서 진행한 한국어 딥러닝 강의 자료](https://github.com/nmhkahn/deep_learning_tutorial)
* [딥러닝 교육 자료 (Deep Learning Lecture)](http://hugrypiggykim.com/2017/08/24/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EA%B5%90%EC%9C%A1-%EC%9E%90%EB%A3%8C-deep-learning-lecture/)
* [신경망에 대해 알아야 할 모든 것](http://blog.funhnc.com/entry/%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%97%90-%EB%8C%80%ED%95%B4-%EC%95%8C%EC%95%84%EC%95%BC-%ED%95%A0-%EB%AA%A8%EB%93%A0-%EA%B2%83)
* [Deep Learning for Beginners](http://randomekek.github.io/deep/deeplearning.html)
* [5 Best Deep Learning in Python videos for a Beginner](https://www.techleer.com/articles/416-5-best-deep-learning-in-python-videos-for-a-beginner)
* [Deep Learning — a gentle dive](https://becominghuman.ai/deep-learning-a-gentle-dive-92054e39bd8)
* [6.S191: Introduction to Deep Learning](http://introtodeeplearning.com/)
  * [MIT 6.S191: Introduction to Deep Learning](https://www.youtube.com/playlist?list=PLkkuNyzb8LmxFutYuPA7B4oiMn6cjD6Rs)
* [Deep Learning Book](https://www.youtube.com/channel/UCF9O8Vj-FEbRDA5DcDGz-Pg/playlists)
* [A Beginner’s Guide to Deep Neural Networks](http://googleresearch.blogspot.kr/2015/09/a-beginners-guide-to-deep-neural.html)
* [From Perceptron to Deep Neural Nets](https://becominghuman.ai/from-perceptron-to-deep-neural-nets-504b8ff616e)
* [인공신경망, 퍼셉트론](https://www.youtube.com/playlist?list=PLaTc2c6yEwmpYKMDfj737S7a_KYC5J5Tq)
* [Building a Deep Neural Net In Google Sheets](https://towardsdatascience.com/building-a-deep-neural-net-in-google-sheets-49cdaf466da0)
* [Deep Learning for Beginners](http://randomekek.github.io/deep/deeplearning.html)
* [Welcome to the Deep Learning Tutorial!](http://deeplearning.stanford.edu/tutorial/)
* [ISBA 2015 Morning Tutorial: Deep Learning (March 23, 2015)](https://www.youtube.com/watch?v=gCwYO7zVJs0)
* [tutorial, implementations](https://github.com/dsindex/blog/wiki/%5Bdeep-learning%5D-tutorial,-implementations)
* [Tutorial on Deep Learning](https://simons.berkeley.edu/talks/tutorial-deep-learning)
* [A Primer on Deep Learning](https://opendatascience.com/blog/change-a-primer-on-deep-learning/)
* [Building Safe A.I.  A Tutorial for Encrypted Deep Learning](https://iamtrask.github.io/2017/03/17/safe-ai)
* [Deep Learning - Taking machine learning to the next level](https://www.udacity.com/course/deep-learning--ud730)
* **[Awesome Deep Learning](https://github.com/ChristosChristofidis/awesome-deep-learning)**
* [Awesome Deep Learning Resources](https://github.com/guillaume-chevalier/awesome-deep-learning-resources)
* [DeepLearning-Summary](https://github.com/taki0112/Awesome-DeepLearning-Study)
* [Deep Learning - 2016년 8월부터 딥러닝공부를 하면서 봤던 강의영상, 동영상, 블로그들의 목록입니다](https://github.com/GunhoChoi/Deep_Learning_Collection)
* [digest.deeplearningweekly.com](http://digest.deeplearningweekly.com/)
* [Top Deep Learning Projects](https://github.com/hunkim/DeepLearningStars)
* [Deep Learning Resources](https://omtcyfz.github.io/2016/08/29/Deep-Learning-Resources.html)
* [Up to Speed on Deep Learning: September, Part 2 and October, Part 1](https://medium.com/the-mission/up-to-speed-on-deep-learning-september-part-2-and-october-part-1-d72d7e5df1ea)
* [My playlist – Top YouTube Videos on Machine HALearning, Neural Network & Deep Learning](http://www.analyticsvidhya.com/blog/2015/07/top-youtube-videos-machine-learning-neural-network-deep-learning/)
* [Designing Machine Learning Models: A Tale of Precision and Recall](http://nerds.airbnb.com/designing-machine-learning-models/)
* [tutorial, implementations](https://github.com/dsindex/blog/wiki/%5Bdeep-learning%5D-tutorial,-implementations)
* [Deep Learning Study](http://deeplearningstudy.github.io/material/) Caffe, TensorFlow
* [The Deep Learning Playbook](https://medium.com/@jiefeng/deep-learning-playbook-c5ebe34f8a1a)
* [A Brief Overview of Deep Learning](http://yyue.blogspot.kr/2015/01/a-brief-overview-of-deep-learning.html)
* [github.com/wbaek/deeplearing_exercise](https://github.com/wbaek/deeplearing_exercise)
* [deepcumen.com](http://deepcumen.com/)
* [Deep Learning Study - Study of HeXA at Ulsan National Institute of Science and Technology](https://github.com/carpedm20/deep-learning-study)
* [My Journal from Neural Network to Deep Learning: A Brief Introduction to Deep Learning. Contents](http://haohanw.blogspot.kr/2015/01/deep-learning-introduction.html)
* [Deep learning - Yann LeCun, Yoshua Bengio & Geoffrey Hinton](http://www.nature.com/nature/journal/v521/n7553/full/nature14539.html)
* [nvidia Deep Learning Courses](https://developer.nvidia.com/deep-learning-courses)
  * [Deep Learning Course](https://www.youtube.com/playlist?list=PL5B692fm6--tI-ijknnVZWbXU2H4JpSYe)
* [NVIDIA DEEP LEARNING DAY 2017 CONFERENCE](https://www.nvidia.com/ko-kr/deep-learning-day/agenda/)
* [CS224d: Deep Learning for Natural Language Processing](http://cs224d.stanford.edu/)
* [Why GEMM is at the heart of deep learning](http://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/)
* [GitHub Special: Data Scientists to Follow & Best Tutorials on GitHub](http://www.analyticsvidhya.com/blog/2015/07/github-special-data-scientists-to-follow-best-tutorials/)
* [딥러닝 워크샵: 딥러닝의 현재와 미래](http://mlcenter.postech.ac.kr/workshop)
  * [후기](http://whydsp.org/262)
* [Deep Learning at Flickr, Pierre Garrigues](http://techjaw.com/2015/03/04/deep-learning-at-flickr-pierre-garrigues/)
* [Andrew Ng: Why ‘Deep Learning’ Is a Mandate for Humans, Not Just Machines](http://www.wired.com/2015/05/andrew-ng-deep-learning-mandate-humans-not-just-machines/)
* [Andrew Ng: Deep Learning, Self-Taught Learning and Unsupervised Feature Learning](https://www.youtube.com/watch?v=n1ViNeWhC24)
* [The tensor renaissance in data science](http://radar.oreilly.com/2015/05/the-tensor-renaissance-in-data-science.html)
* [The Paradox of Deeper Learning: The Unlearning Curve](http://blogs.edweek.org/edweek/learning_deeply/2015/04/the_paradox_of_deeper_learning_the_unlearning_curve.html)
* [Are there Deep Reasons Underlying the Pathologies of Today’s Deep Learning Algorithms?](http://goertzel.org/DeepLearning_v1.pdf)
* [집단지성프로그래밍 05. 최적화(optimization) 김지은_20150522](http://www.slideshare.net/yeswldms/05-optimization-20150522)
* [ICLR 2015](http://www.iclr.cc/doku.php?id=iclr2015%3Amain)
  * [Artificial Tasks for Artificial Intelligence](https://www.dropbox.com/s/ly9y136saba0915/ICLR2015_Oral_Slides_All.pdf?oref=e&n=117881854)
* [Deep Learning Trends @ ICLR 2016](http://www.computervisionblog.com/2016/06/deep-learning-trends-iclr-2016.html)
* [ICLR 2017 - Conference Track International Conference on Learning Representations](http://openreview.net/group?id=ICLR.cc%2F2017%2Fconference)
  * [ICLR2017 Paper Index](https://tensorflowkorea.wordpress.com/2016/11/16/iclr2017-paper-index/)
  * [ICLR 2017 workshop track open review](http://nuit-blanche.blogspot.com/2017/02/iclr-2017-workshop-track-open-review.html)
  * [Learning to remember rare events](https://www.slideshare.net/ssuser06e0c5/learning-to-remember-rare-events)
* [Deep Learning for Computer Vision Barcelona](http://imatge-upc.github.io/telecombcn-2016-dlcv/)
* [Algorithms of the Mind](https://medium.com/deep-learning-101/algorithms-of-the-mind-10eb13f61fc4)
* [What's Wrong With Deep Learning?](https://drive.google.com/file/d/0BxKBnD5y2M8NVHRiVXBnOVpiYUk/edit)
* [Why does Deep Learning work?](https://charlesmartin14.wordpress.com/2015/03/25/why-does-deep-learning-work/)
* [Why Deep Learning Works II: the Renormalization Group](https://charlesmartin14.wordpress.com/2015/04/01/why-deep-learning-works-ii-the-renormalization-group/)
* [Why Deep Learning Works](https://artificial-understanding.com/why-deep-learning-works-1b0184686af6)
* [Normalization 방법](https://www.slideshare.net/ssuser06e0c5/normalization-72539464)
* [18 Great Deep Learning Resources, most free](http://blog.sense.io/18-great-deep-learning-resources)
* [인공지능의 눈으로 바라본 세상](http://techneedle.com/archives/20800)
* [XLDB2015: Accelerating Deep Learning at Facebook](https://www.youtube.com/watch?v=KviuMAF4pEA)
* [The Brain vs Deep Learning Part I: Computational Complexity — Or Why the Singularity Is Nowhere Near](https://timdettmers.wordpress.com/2015/07/27/brain-vs-deep-learning-singularity/)
* [A Beginner’s Guide to Restricted Boltzmann Machines](http://deeplearning4j.org/restrictedboltzmannmachine.html)
* [Energy based models and boltzmann machines - v2.0](http://www.slideshare.net/blaswan/energy-based-models-and-boltzmann-machines-v20)
* [내맘대로 이해하는 Deep Belief Network와Restricted Boltzmann Machine](http://whydsp.org/283)
* [Deep learning for assisting the process of music composition (part 1)](https://highnoongmt.wordpress.com/2015/08/11/deep-learning-for-assisting-the-process-of-music-composition-part-1/)
* [Neural Translation of Musical Style](http://imanmalik.com/cs/2017/06/05/neural-style.html)
* [Andrew Ng: Deep Learning, Self-Taught Learning and Unsupervised Feature Learning](https://www.youtube.com/watch?v=n1ViNeWhC24)
* [Deep Learning Summer School 2015](https://sites.google.com/site/deeplearningsummerschool/)
  * [Deep Learning Summer School, Montreal 2015](http://videolectures.net/deeplearning2015_montreal/)
* [Deep Learning Summer School 2016](https://sites.google.com/site/deeplearningsummerschool2016/)
  * [Deep Learning Summer School, Montreal 2016](http://videolectures.net/deeplearning2016_montreal/)
  * [What I learned from Deep Learning Summer School 2016](https://www.linkedin.com/pulse/what-i-learned-from-deep-learning-summer-school-2016-hamid-palangi)
* [Deep Learning Summer School, Montreal 2017](http://videolectures.net/deeplearning2017_montreal/)
* [26 THINGS I LEARNED IN THE DEEP LEARNING SUMMER SCHOOL](http://www.marekrei.com/blog/26-things-i-learned-in-the-deep-learning-summer-school/)
* [Deep Learning and Neural Networks](http://cl.naist.jp/~kevinduh/a/deep2014/)
* [한국에서 처음 열린 GTC, 딥러닝의 현재를 이야기하다](http://chitsol.com/2273)
* [네이버, 사람 없이 이미지 뉴스 만든다](http://www.bloter.net/archives/238742)
* [Deep Learning Startups, Applications and Acquisitions – A Summary](http://blog.dennybritz.com/2015/10/13/deep-learning-startups-applications-and-acquisitions-a-summary/)
* [Theoretical Motivations for Deep Learning](http://rinuboney.github.io/2015/10/18/theoretical-motivations-deep-learning.html)
* [How We Use Deep Learning to Classify Business Photos at Yelp](http://engineeringblog.yelp.com/2015/10/how-we-use-deep-learning-to-classify-business-photos-at-yelp.html)
* [Artificial Intelligence, Neural Networks, and Deep Learning](http://kimschmidtsbrain.com/2015/10/29/artificial-intelligence-neural-networks-and-deep-learning/)
* [Deep Learning in a Single File for Smart Devices](http://dmlc.ml/mxnet/2015/11/10/deep-learning-in-a-single-file-for-smart-device.html)
* [Boosting Methods](http://enginius.tistory.com/m/post/606)
* [Deep Residual Networks](https://github.com/KaimingHe/deep-residual-networks)
* [stat212b - Topics Course on Deep Learning for Spring 2016](https://github.com/joanbruna/stat212b)
* [Fujitsu develops new deep learning technology to analyze time-series data with high precision](http://phys.org/news/2016-02-fujitsu-deep-technology-time-series-high.html)
* 2016-02-17~18 자연어처리 튜토리얼 심층학습과 언어처리 응용
  * TensorFlow Tutorial; SKT 정상근 박사님
    * [GitHub](https://github.com/hugman/deep_learning)
    * mnist.py 파일을 먼저 본다.
    * mnist_with_monitoring.py
      * TensorBoard 서버에 모니터링 로그를 보내고 웹으로 볼 수 있다
    * Keras
      * computaion backend를 추상화하여 편하게 적용할 수 있도록 하는 라이브러리
      * theano, tensorflow 둘다 지원
      * 형태소 분석기같은 걸 빠르게 만들어 보기에 좋다
    * pos_tagger_fcn.py
      * word embedding => wikipedia로 SENNA에서 만들어서 오픈한 데이터
      * 변수명은 mnist와 일부러 동일하게 뒀으니 비교해서 보면 좋음
      * 참고: fcn => fully connected network
    * pos_tagger_rnn_seq.py
      * 궁극의 코드!
      * learning rate를 처음에는 크게해서 점점 작게 만들어 준다
      * optimizer를 바꿔가며 학습을 할 수도 있다
      * epoch이 50이 넘으면 중간에 바꿔라! 등
    * TensorFlow github 소스에 보면,
      * models -> rnn -> translate 구글의 기계번역 소스가 있다
      * https://github.com/tensorflow/tensorflow/tree/master/tensorflow/models/rnn/translate
  * Deep Learning for NLP 응용; 강원대 이창기 박사님
    * [NLP from Scratch](http://arxiv.org/abs/1103.0398)
    * [SENNA](http://ronan.collobert.com/senna)
      * CNN + CRF
    * RNN(LSTM) + attention score
      * attention score는 alignment prob과 동일한 역할을 해준다.
    * t-SNE scatter plot
      * 자질의 차원을 축소하여 2차원으로 뿌려주는 방식
* [Visualizing Deep Learning with t-SNE (Tutorial and Video)](https://medium.com/@awjuliani/visualizing-deep-learning-with-t-sne-tutorial-and-video-e7c59ee4080c)
* [Make Your Own 3D t-SNE Visualizations (Download Binary and Code)](https://medium.com/@awjuliani/make-your-own-3d-t-sne-visualizations-download-e0cdfe80d6e3)
* [오픈 소스 딥러닝 소프트웨어](http://kernelstudy.net/t/topic/188)
* [DataScience/Deep Learning](http://khanrc.tistory.com/category/DataScience/Deep%20Learning)
* [Introduction to Deep Learning for Image Analysis at Strata NYC, Sep 2015](http://www.slideshare.net/dato-inc/introduction-to-deep-learning-for-image-analysis-at-strata-nyc-sep-2015)
* [Show and tell takmin: A Neural Image Caption Generator](http://www.slideshare.net/takmin/show-andtell-takmin)
* [Building an Automated Image Captioning Application](https://daniel.lasiman.com/post/image-captioning/)
* [그림 그리는 AI](http://tv.naver.com/v/2417457)
* [딥러닝 임팩트가 온다](http://techholic.co.kr/archives/51820)
* [Deep Learning for Visual Question Answering](http://avisingh599.github.io/deeplearning/visual-qa/)
* [Visualizing and Understanding Deep Neural Networks by Matt Zeiler](https://www.youtube.com/watch?v=ghEmQSxT6tw&spfreload=10)
* [Visualizing Deep Learning Networks - Part II](http://blog.qure.ai/notes/deep-learning-visualization-gradient-based-methods)
* [When Does Deep Learning Work Better Than SVMs or Random Forests?](http://www.kdnuggets.com/2016/04/deep-learning-vs-svm-random-forest.html)
* [openai.com](https://openai.com)
  * [OpenAI Gym Beta](https://openai.com/blog/openai-gym-beta/)
    * [AI Gym Workout](https://learningai.io/projects/2017/07/28/ai-gym-workout.html)
    * [Walker2D: Learning Progression](https://www.youtube.com/watch?v=irkXnpZP89s&feature=youtu.be)
    * [gist.github.com/pat-coady](https://gist.github.com/pat-coady)
  * [requests-for-research](https://openai.com/requests-for-research/)
  * [PIXELCNN++: A PIXELCNN IMPLEMENTATION WITH DISCRETIZED LOGISTIC MIXTURE LIKELIHOOD AND OTHER MODIFICATIONS](https://openreview.net/pdf?id=BJrFC6ceg)
    * [pixel-cnn++ - This is a Python3 / Tensorflow implementation of PixelCNN++](https://github.com/openai/pixel-cnn)
    * [PixelCNN [1601.06759] Summary](https://tensorflow.blog/2016/11/29/pixelcnn-1601-06759-summary/)
  * [OpenAI Universe (OpenAI)](https://universe.openai.com/)
  * [오픈소스로…인공지능 학습 플랫폼](http://techholic.co.kr/archives/64126)
  * [GTA V + Universe](https://openai.com/blog/GTA-V-plus-Universe/)
  * [Actor Critic with OpenAI Gym](http://www.rage.net/~greg/2016-07-05-ActorCritic-with-OpenAI-Gym.html)
  * [Learning to communicate](https://openai.com/blog/learning-to-communicate/)
  * [첫번째 프로젝트: gym 기반으로 틱택토 환경 만들어 보기](https://github.com/kekmodel/gym-TicTacToe)
  * [atari_py - A Windows-MSYS2-MinGW compatible version of https://github.com/openai/ale_python_interface](https://github.com/rybskej/atari-py)
  * [강화학습 그리고 OpenAI - 1: Introduction to OpenAI](http://www.modulabs.co.kr/RL_library/1705)
  * [Gathering Human Feedback](https://blog.openai.com/gathering_human_feedback/)
  * [OpenAI Baselines: DQN](https://blog.openai.com/openai-baselines-dqn/)
  * [OpenAI GYM atari-py 설치 오류 해결](http://rrbb014.tistory.com/43)
  * [OpenAI GYM을 Jupyter notebook환경에서 실행하기 & headless playing](http://rrbb014.tistory.com/44)
* [Neural Programmer-Interpreters](http://www-personal.umich.edu/~reedscot/iclr_project.html)
* [Video Recordings of the ICML’15 Deep Learning Workshop](http://dpkingma.com/?page_id=483)
  * [딥러닝 워크샵 패널토의 @ ICML2015](http://t-robotics.blogspot.com/2015/07/icml2015.html#.VyyRohWLSZ0)
* [Deep Learning: Nine Lectures at Collège de France](http://cilvr.nyu.edu/doku.php?id=courses%3Adeeplearning-cdf2016%3Astart)
* [Deep Learning with RE•WORK #reworkDL](https://www.youtube.com/playlist?list=PLnDbcXCpYZ8m412d2KX5paGKdGUxxWCEP)
* [csl.sony.fr/publications](https://www.csl.sony.fr/publications.php)
  * [인공지능이 편곡한 '환희의 송가'](http://www.yonhapnews.co.kr/local/0899000000.html?cid=MYH20160511017400797)
  * [Machine Learning Techniques for Reorchestrating the European Anthem](https://www.youtube.com/watch?list=PLuOoXrWK6Kz5ySULxGMtAUdZEg9SkXDoq&v=0qnTaAz-xtQ)
* [Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records](http://www.nature.com/articles/srep26094)
* [deepnumbers.com](http://www.deepnumbers.com/)
* [10 Deep Learning Terms Explained in Simple English](http://www.datasciencecentral.com/m/blogpost?id=6448529%3ABlogPost%3A410633)
* [A Statistical View of Deep Learning](http://blog.shakirm.com/ml-series/a-statistical-view-of-deep-learning/)
* [Deep Learning in Practice: Speech Recognition and Beyond](http://events.technologyreview.com/emtech/digital/16/video/watch/andrew-ng-deep-learning/)
* [CHAR2WAV: END-TO-END SPEECH SYNTHESIS](https://mila.umontreal.ca/en/publication/char2wav-end-to-end-speech-synthesis/)
* [DEEP LEARNING AND REINFORCEMENT LEARNING SUMMER SCHOOL 2017](https://mila.umontreal.ca/en/cours/deep-learning-summer-school-2017/slides/)
* [Google DeepMind Teaches Artificial Intelligence Machines to Read](http://www.technologyreview.com/view/538616/google-deepmind-teaches-artificial-intelligence-machines-to-read/)
* [WaveNet: A Generative Model for Raw Audio](https://deepmind.com/blog/wavenet-generative-model-raw-audio/)
  * [A TensorFlow implementation of DeepMind's WaveNet paper](https://github.com/ibab/tensorflow-wavenet)
  * [Voice User Interfaces Project: Speech Recognition with Neural Networks](https://github.com/udacity/AIND-VUI-Capstone/blob/master/vui_notebook.ipynb)
  * [Speech-to-Text-WaveNet : End-to-end sentence level English speech recognition using DeepMind's WaveNet](https://github.com/buriburisuri/speech-to-text-wavenet)
* [10 Deep Learning Trends at NIPS 2015](http://codinginparadise.org/ebooks/html/blog/ten_deep_learning_trends_at_nips_2015.html)
  * [딥러닝의 10가지 트렌드 from NIPS 2015](http://t-robotics.blogspot.com/2016/01/10-from-nips-2015.html)
* [NIPS 2016](http://beamandrew.github.io/deeplearning/2016/12/12/nips-2016.html)
  * [nips.cc/Conferences/2016/Schedule](https://nips.cc/Conferences/2016/Schedule)
  * [All Code Implementations for NIPS 2016 papers](https://cdn.ampproject.org/c/s/amp.reddit.com/r/MachineLearning/comments/5hwqeb/project_all_code_implementations_for_nips_2016/)
  * [LET'S DISCUSS: LEARNING METHODS FOR DIALOGUE NIPS 2016 WORKSHOP](http://letsdiscussnips2016.weebly.com/schedule.html)
  * [NIPS 2016 tutorial - Summary Nuts and bolts of building AI applications using Deep Learning](http://jaejunyoo.blogspot.com/2017/03/nips-2016-tutorial-summary-nuts-and-bolts-of-building-AI-AndrewNg.html)
    * [초짜 대학원생의 입장에서 정리한 NIPS 2016 tutorial: Nuts and bolts of building AI applications using Deep Learning by Andrew Ng](http://jaejunyoo.blogspot.com/2017/03/kr-nips-2016-tutorial-summary-nuts-and-bolts-of-building-AI-AndrewNg.html)
  * [History of Bayesian Neural Networks (Keynote talk)](https://www.youtube.com/watch?v=FD8l2vPU5FY&feature=youtu.be)
* [Nuts and Bolts of Building Deep Learning Applications: Ng @ NIPS2016](http://www.computervisionblog.com/2016/12/nuts-and-bolts-of-building-deep.html)
* Fast and Provably Good Seedings for k-Means
  * [paper](http://papers.nips.cc/paper/6478-fast-and-provably-good-seedings-for-k-means.pdf)
  * [code](https://github.com/obachem/kmc2)
  * [slide](http://olivierbachem.ch/files/afkmcmc-oral-pdf.pdf)
  * [spotlight](https://youtu.be/QtQyeka-tlQ)
  * k-means 클러스터링에서 k-means++으로 초기 클러스터를 정하게 되면 O(log k)번 안에 최적의 클러스터로 수렴하는것이 증명되었지만 초기 O(nkd) 연산이 필요해 대용량 데이터에서는 적합하지 않은 문제가 있음
  * 기존의 k-means++보다 수백배 빠르면서도 결과가 근사한 Assumption-free K-MC^2 를 제안
  * 파이썬 패키지로 배포되어 있으며 scikit-learn에서도 바로 사용 가능
* [클러스터링과 KMeans를 이용한 데이타의 군집화](http://bcho.tistory.com/1203)
* [Hierarchical clustering을 이용한 데이타 군집화](http://bcho.tistory.com/1204)
* [DeepMind Papers @ NIPS (Part 1)](https://deepmind.com/blog/deepmind-papers-nips-part-1/)
* [DeepMind Papers @ NIPS (Part 2)](https://deepmind.com/blog/deepmind-papers-nips-part-2/)
* [DeepMind Papers @ NIPS (Part 3)](https://deepmind.com/blog/deepmind-papers-nips-part-3/)
* [Repo. for NIPS 2016 papers](https://tensorflow.blog/2016/12/15/repo-for-nips-2016-papers/)
* [Bayesian Deep Learning NIPS 2016 Workshop](http://bayesiandeeplearning.org/#schedule)
* [Bayesian Deep Learning](http://twiecki.github.io/blog/2016/06/01/bayesian-deep-learning/)
* [Bayesian Machine Learning, Explained](http://www.rightrelevance.com/search/articles/hero?article=5f8cc010177776a7f4d48089ec4e539dc42a1ff9)
* [Bayesian Recurrent Neural Networks](https://github.com/mirceamironenco/BayesianRecurrentNN)
* [Understanding Bayesian Deep Learning](https://github.com/sjchoi86/bayes-nn)
* [IRT and DKT implementation](https://github.com/Knewton/edm2016)
* [Magenta wins "Best Demo" at NIPS 2016!](https://magenta.tensorflow.org/2016/12/16/nips-demo/)
  * [Magenta AI Jam Session](https://www.youtube.com/watch?v=QlVoR1jQrPk&feature=youtu.be)
  * [Magenta: Music and Art Generation with Machine Intelligence](https://github.com/tensorflow/magenta)
  * [Interactive Musical Improvisation with Magenta (NIPS 2016)](https://github.com/tensorflow/magenta/tree/master/magenta/demos/NIPS_2016)
* [DeepMind Lab (DeepMind)](https://deepmind.com/blog/open-sourcing-deepmind-lab)
* [Learning explanatory rules from noisy data](https://deepmind.com/blog/learning-explanatory-rules-noisy-data/)
  * 신경망의 직관적/지각적 추론과 논리 프로그래밍쪽의 상징/논리 추론을 결합한 프레임워크 ∂ILP
* [모두의연구소 쫄지말자딥러닝](http://www.slideshare.net/modulabs/ss-62503747)
* [쫄지말자딥러닝2 - CNN RNN 포함버전](http://www.slideshare.net/modulabs/2-cnn-rnn)
* [www.modulabs.co.kr/DeepLAB](http://www.modulabs.co.kr/DeepLAB)
  * [딥러닝연구실](http://whydsp.org/m/post?categoryId=525022) 과거 자료
* [What My Deep Model Doesn't Know...](http://mlg.eng.cam.ac.uk/yarin/blog_3d801aa532c1ce.html)
* [Train your deep model faster and sharper — two novel techniques](https://hackernoon.com/training-your-deep-model-faster-and-sharper-e85076c3b047) training 속도 단축 방법
* [Deep Learning with Eigenvalue Decay Regularizer](https://www.researchgate.net/publication/301648136_Deep_Learning_with_Eigenvalue_Decay_Regularizer)
* [Eigenfaces](https://github.com/ml4a/ml4a-guides/blob/master/notebooks/Eigenfaces.ipynb)
* [Deep Network with Stochastic Depth](https://www.evernote.com/shard/s462/sh/2de09526-e8fe-48d9-90da-9baa356d5e1a/7a4259299b26c41d60e05e894dbbc2fa)
* [Prof. Geoff Hinton - Deep Learning](https://www.youtube.com/watch?v=VhmE_UXDOGs&feature=youtu.be)
* [Autoencoders](http://ufldl.stanford.edu/tutorial/unsupervised/Autoencoders/)
* [인공 신경망에 관한 설명. 스탠포드 대학 앤드류 응 교수의 sparse autoencoder 정리 노트로 인공신경망 이해하기](http://woongheelee.com/m/entry/%EC%9D%B8%EA%B3%B5-%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%97%90-%EA%B4%80%ED%95%9C-%EC%84%A4%EB%AA%85-%EC%8A%A4%ED%83%A0%ED%8F%AC%EB%93%9C-%EB%8C%80%ED%95%99-%EC%95%A4%EB%93%9C%EB%A5%98-%EC%9D%91-%EA%B5%90%EC%88%98%EC%9D%98-sparse-autoencoder-%EC%A0%95%EB%A6%AC-%EB%85%B8%ED%8A%B8%EB%A1%9C-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0)
* [What're the differences between PCA and autoencoder?](http://stats.stackexchange.com/questions/120080/whatre-the-differences-between-pca-and-autoencoder)
* [Deep AutoEncoders for Collaborative Filtering](https://github.com/NVIDIA/DeepRecommender) autoencoder 기반 추천 엔진
* [Absolute ANN: A simplified approach for structuring the learnt representations](https://medium.com/mlreview/aann-absolute-artificial-neural-network-ae8f1a65fa67)
* [차원 축소 (Principal Component Analysis)](http://blog.naver.com/anthouse28/221016346362)
* [차원 감소와 PCA 분석](http://bcho.tistory.com/1209)
* [#5.0. PCA의 이해 (1)](https://www.youtube.com/watch?v=9UggjVi9-9M&list=PL0oFI08O71gKEXITQ7OG2SCCXkrtid7Fq&index=23)
  * [PCA의 의미와 한계점](https://www.facebook.com/TRobotics/posts/796663103771140)
  * [PCA(주성분분석)의 의미와 한계점](http://t-robotics.blogspot.com/2015/12/pca.html)
* [PCA: "분산이 큰 축일 수록 정보가 더 많다?"](https://blog.naver.com/hancury/221215245092)
* [SVD를 이용한 추천 시스템 구현하기](http://leebaro.tistory.com/entry/SVD%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C-%EA%B5%AC%ED%98%84%ED%95%98%EA%B8%B0)
* [CS 7931: Deep Learning Seminar](http://ml.cs.utah.edu/deep-learning/)
* [Stanford Seminar - Song Han of Stanford University](https://www.youtube.com/watch?v=hfFkS_vHslI&feature=youtu.be)
  * ["Techniques for Efficient Implementation of Deep Neural Networks," a Presentation from Stanford](http://www.slideshare.net/embeddedvision/techniques-for-efficient-implementation-of-deep-neural-networks-a-presentation-from-stanford)
* [Deep Learning, Tools and Methods workshop](https://portal.klewel.com/watch/webcast/deep-learning-tools-and-methods-workshop/)
* [How to Start Learning Deep Learning](http://www.kdnuggets.com/2016/07/start-learning-deep-learning.html)
* [Summary of Deep Learning Environments](https://www.facebook.com/notes/239472486233783/Summary%20of%20Deep%20Learning%20Environments/587130401467988/)
* [Deep learning tutorials (2nd ed.)](https://github.com/sjchoi86/dl_tutorials)
* [Deep Learning Tutorial 4th](https://github.com/sjchoi86/dl_tutorials_4th)
* [Deep Learning Tutorial](https://github.com/sjchoi86/dl_tutorials_10weeks)
* [Deep Learning for Everyone – and (Almost) Free](http://www.datasciencecentral.com/profiles/blogs/deep-learning-for-everyone-and-almost-free)
* [Deep Learning for Object Detection with DIGITS](https://devblogs.nvidia.com/parallelforall/deep-learning-object-detection-digits/)
* [khanrc.tistory.com/category/DataScience/Deep Learning](http://khanrc.tistory.com/category/DataScience/Deep%20Learning)
* [도커와 AWS를 활용한 클라우드 딥러닝 환경 구축](https://gist.github.com/haje01/f13053738853f39ce5a2)
  * [Decoupled Neural Interfaces using Synthetic Gradients[1608.05343] Summary](https://tensorflowkorea.wordpress.com/2016/08/22/decoupled-neural-interfaces-using-synthetic-gradients1608-05343-summary/)
* [Initialization Of Deep Networks Case of Rectifiers](http://www.jefkine.com/deep/2016/08/08/initialization-of-deep-networks-case-of-rectifiers/)
* [ANN 구현하고 x^2 근사함수 찾기](https://github.com/dgtgrade/HumanLearning/blob/master/1001.py)
  * Universal Approximation Theorem 에 따르면 간단한 ANN으로도 가능
  * 구현된 간단한 ANN
    * 입력 레이어: 노드 1개 
      * x 그대로임. 즉, feature 로서 다항식이나 비선형 함수 사용하지 않음
    * 히든 레이어: 1개, 노드: 50개
      * a = sigmoid(wx + b1)
    * 출력 레이어: 노드 1개
      * o = wa + b2
    * 코스트 함수: Squared Error
    * 히든 레이어가 하나라서 DNN이라고 적지 않음
    * ANN 중 가장 표준적이고 기초적이라고 할 수 있는 ANN 그대로, 또는 그중에서도 가장 간단한 형태라고 보면 됨
  * 실험 결과
    * t(x)=x^2 함수: x=[1.0~12.0]까지 학습 시켰는데 잘 되었음
    * t(x)=sin(x) 함수: x=[1.0~10.0]까지 학습 시켰는데 잘 되었음
    * t(x)=x^3+3*sin(x)^2-10 함수: x=[-5.5~5.5]까지 학습 시켰는데 잘 되었음
    * 히든 노드수를 늘릴 수록 더 넓은 범위의 x 값을 커버할 수 있음을 일정 범위 내에서 확인
  * [코드](https://github.com/dgtgra…/HumanLearning/blob/master/1001.py)
    * numpy 외에 아무런 라이브러리도 사용하지 않았음
    * Back Propagation 외에 요구되는 배경 지식은 없음
    * python 3.5 환경에서 작성 하였고, numpy만 있으면 실행 됨
  * [실행 동영상](https://www.facebook.com/dgtgrade/videos/1161340170591514/)
    * iteration: 학습 회수
    * train: 학습 데이터
    * test: 학습되지 않은 데이터
    * x: 입력
    * h: 학습된 네트워크의 결과
    * y: 정답 출력
    * cost: squared error
* "손으로 아무렇게나 그린 함수"를 "초간단 ANN으로 근사 시키기"; Universal Approximation Theorem의 내용 직접 실험
  * [Human Learning #1003 : Visual Test of Universal Approximation Theorem]((https://www.youtube.com/watch?v=SahmdQs6X74&list=PLefQdA1SdkhtRUuN_D3PdxaR2XTGQw8Ph&index=9)
  * ANN; 지난 글에서와 마찬가지 초간단 (Deep도 아닌) ANN
    * 히든 레이어 1개, 히든 노드 100개
    * 입력 레이어: 노드 1개 
      * x 그대로임. 즉, feature 로서 다항식이나 비선형 함수 사용하지 않음
    * 히든 레이어: 1개, 노드: 100개
      * a = sigmoid(w1*x + b1)
    * 출력 레이어: 노드 1개
      * o = w2*a + b2
    * 코스트 함수: Squared Error
  * [코드](https://github.com/dgtgra…/HumanLearning/blob/master/1003.py)
    * Artificial Neural Network의 기초, Gradient Descent의 기초, Back Propagation의 기초
    * python에서 매트릭스 다루는 법: numpy
    * python에서 그래프 그리는 법: matplotlib 
    * python에서 이미지 읽는 법: skimage
    * Learning Rate 변경에 따른 학습 능력의 변화, 히든 노드수 변경에 따른 학습 능력의 변화
  * 실행환경
    * python3.5 및 numpy
    * [처음부터 새로 설치 하려면 다음 영상을 참고](https://www.youtube.com/watch?v=pMkwjXFZdH4)
  * 실행방법
    * python 1003.py [이미지파일경로]
    * 이미지 파일 경로에 data/1003_plot1.png 등을 넣어주면 됨
    * [예제 이미지 git 페이지](https://github.com/dgtgrade/HumanLearning/tree/master/data)
* [Universal Approximation Theorem](https://en.wikipedia.org/wi…/Universal_approximation_theorem)
* [Universal Approximation Theorem](http://www.slideshare.net/theeluwin/universal-approximation-theorem-70937339)
  * 수식 t(x)는 아무 곡선이나 임의로 그려 보기 위해서 사용한 도구일 뿐인 것으로 이해해야 함
  * 즉, 수식 t(x)의 식이 중요한 것은 아님. 수식 t(x)의 내용을 문제 출제자도 모르는 상태에서 아무렇게나 (물론 함수로 표현은 가능하게) 곡선들을 그려넣으면 단순한 ANN으로도 언제나 그 곡선이 표현 가능
  * 즉 그 곡선에 거의 딱 맞는 함수 t(x)를 (사람은 모르고, 아마 만들 낼수도 없어도) 기계가 (단순 함수 f1, f2 등의 조합으로) 만들수 낼수 있다는 것을 실험해 본 것
  * 그렇기 때문에 학습한 범위 밖의 t(x)를 추정 할수 있느냐 없느냐는 여기서는 중요하지 않음
  * 왜냐하면 수식 t(x)는 범위 안의 값을 그리기 위해서 사용한 도구였으므로 사실 t(x)가 아니라 (범위 안의 출력만 일치 한다면) 수식 t2(x) 또는 t3(x)였어도 상관이 없음
  * ANN은 머신러닝을 통해서 수식 t(x)가 아니라 수식 t10(x)를 만들어 낸 것
  * 자율운전의 이상적인 정답 함수 세트 T(x)가 존재 한다면 그 함수들은 사람이 수식으로 쓸수는 없으나(!) (운전 데이터를 통해서) 곡선 그림 t(x)를 그려주면 기계가 머신러닝으로 근사함수 h(x)를 만들어 낼수도 있지 않을까? 하는 것을 보여주는 실험
  * 바둑의 이상적인 정답 함수 세트 T(x) 또한 사람은 그 함수들의 수식을 정리할 능력이 없지만 기초 학습을 위한 그림 t(x)는 (기보 데이터를 통해서) 그려줄수는 있고 알파고는 우선 그 사람이 그려준 그림 t(x)에 근사하는 h(x)를 머신러닝을 통해서 만들어 보는 것으로 시작
  * 그 근사가 어느 정도 완료된 이후엔 자기 h1(x) vs 자기 h2(x) 싸움을 통한 학습에 들어가므로 t(x)는 불필요
  * 실험 실행 영상; 다음 3가지 함수에 대하여 실험한 영상 첨부
    * x^2
    * 8*x^2-X^3
    * 10*sin(X)+(X-4)^2-10
    * 초록색 선: 실제 함수
    * 파란색 점: 학습용 정답 데이터
    * 빨간색 점: 학습 결과 만들어진 근사 함수의 출력 데이터
  * [코드](https://github.com/dgtgra…/HumanLearning/blob/master/1002.py)
  * ANN; 지난 글에서와 마찬가지 초간단 (Deep도 아닌) ANN이고, 히든 노드만 100개로 변경함
    * 입력 레이어: 노드 1개 
      * x 그대로임. 즉, feature 로서 다항식이나 비선형 함수 사용하지 않음
    * 히든 레이어: 1개, 노드: 100개
      * a = sigmoid(w1*x + b1)
    * 출력 레이어: 노드 1개
      * o = w2*a + b2
    * 코스트 함수: Squared Error
  * 실행 환경 준비; python3.5, numpy, matplotlib [설치](https://www.youtube.com/watch?v=pMkwjXFZdH4)
  * 목표 함수 t에 따라서 사람이 조정해야 하는 값
    * Learning Rate: 너무 작게 하면 학습이 느리고, 너무 크게 하면 학습이 안 됨
    * 히든 노드수: 너무 적으면 학습이 불가능할테고, 너무 많으면 학습이 느려짐
  * [머신 러닝이란 무엇일까?](https://www.youtube.com/watch?v=3vcG61VC90c)
* [Neural Network Algorithms - Learn How To Train ANN](https://www.datasciencecentral.com/profiles/blogs/neural-network-algorithms-learn-how-to-train-ann)
* [research.artifacia.com](http://research.artifacia.com/)
* [Source Code Classification Using Deep Learning](http://blog.aylien.com/source-code-classification-using-deep-learning/)
* [Deep Learning Cases: Text and Image Processing](http://www.slideshare.net/grigorysapunov/deep-learning-cases-text-and-image-processing)
* [Introduction to Deep Learning part 1](https://www.youtube.com/watch?v=hoN1mnUBUyI)
* [Introduction to Deep Learning part 2](https://www.youtube.com/watch?v=E71SNUqi2cw)
* [An introduction to Deep Learning by Breandan Considine](https://speakerdeck.com/breandan/an-introduction-to-deep-learning)
* [딥러닝의 인공지능 수단으로서의 성격과 방향](http://www.slideshare.net/neuralix/deep-learning-aswaveextractor)
* [CM 세미나](https://www.youtube.com/playlist?list=PLzWH6Ydh35ggVGbBh48TNs635gv2nxkFI)
* [Deep Learning in real world @Deep Learning Tokyo](http://www.slideshare.net/pfi/deep-learning-in-real-world-deep-learning-tokyo)
* [Deep learning tutorials](https://github.com/sjchoi86/dl-workshop)
* [Bay Area DL School Live Stream!](https://tensorflowkorea.wordpress.com/2016/09/24/bay-area-dl-school-live-stream/)
  * [Bay Area Deep Learning School Day 1 at CEMEX auditorium, Stanford](https://www.youtube.com/watch?v=eyovmAtoUx0&feature=youtu.be)
  * [Bay Area Deep Learning School Day 2 at CEMEX auditorium, Stanford](https://www.youtube.com/watch?v=9dXiAecyJrY&feature=youtu.be)
* [Deep Generative Models](http://www.slideshare.net/MijungKim9/deep-generative-models)
* [Generative Model 101](https://www.facebook.com/SKTBrain/posts/313726382331516) 실제와 유사한 음악이나 이미지를 만들어내는 "Generative Model" 주요 논문 정리
* [Deep Advances in Generative Modeling](https://www.youtube.com/watch?v=KeJINHjyzOU)
* [Latent Constraints: Conditional Generation from Unconditional Generative Models](https://colab.research.google.com/notebook#fileId=1oJKhIXi27R3An0sAd6IzazvrKplTnhj-) coalb code
* [Nuts and Bolts of Applying Deep Learning: Tips and Tricks by Andrew Ng](https://bigdatascientistblog.wordpress.com/2016/09/26/nuts-and-bolts-of-applying-deep-learning-tips-and-tricks-by-andrew-ng/)
* [Evaluation of Deep Learning Toolkits](https://github.com/zer0n/deepframeworks/blob/master/README.md)
* [딥러닝 프레임워크 조사와 몇가지 홍보](http://www.popit.kr/%EB%94%A5%EB%9F%AC%EB%8B%9D-%ED%94%84%EB%A0%88%EC%9E%84%EC%9B%8C%ED%81%AC-%EC%A1%B0%EC%82%AC%EC%99%80-%EB%AA%87%EA%B0%80%EC%A7%80-%ED%99%8D%EB%B3%B4/)
* [Deep Learning Frameworks](https://developer.nvidia.com/deep-learning-frameworks) 주요 프레임워크들의 설치를 쉽게 안내하는 엔비디아 페이지
* [딥러닝프레임워크비교](https://www.slideshare.net/JunyiSong1/ss-75552936)
* [Comparison of deep learning software](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software) 위키피디아의 방대한 딥러닝 프레임워크 비교 표
* [Comparison of deep learning software/Resources](https://en.wikipedia.org/wiki/Comparison_of_deep_learning_software/Resources) 위에서 커버되지 않은 최신 프레임워크들
* [【PyTorch、Chainer、Keras、TensorFlow】ディープラーニングのフレームワークの利点・欠点【2017年10月更新】](http://s0sem0y.hatenablog.com/entry/2017/05/15/063753)
* [Release Chainer Chemistry: A library for Deep Learning in Biology and Chemistry](https://preferredresearch.jp/2017/12/18/chainer-chemistry-beta-release/)
* [A Look at Popular Machine Learning Frameworks](http://redmonk.com/fryan/2016/06/06/a-look-at-popular-machine-learning-frameworks/) 프레임워크들의 깃허브와 스택오버플로에서의 관심도 차이
* [Battle of the Deep Learning frameworks — Part I: 2017, even more frameworks and interfaces](https://towardsdatascience.com/battle-of-the-deep-learning-frameworks-part-i-cff0e3841750)
* [딥러닝 분산처리 기술동향](https://www.nextobe.com/single-post/2017/06/09/%EB%94%A5%EB%9F%AC%EB%8B%9D-%EB%B6%84%EC%82%B0%EC%B2%98%EB%A6%AC-%EA%B8%B0%EC%88%A0%EB%8F%99%ED%96%A5)
* [DEEP LEARNING Your daily dose of Deep learning](http://www.notey.com/blogs/deep-learning) 딥러닝에 대한 기사
* [The Next Wave of Deep Learning Architectures](http://www.nextplatform.com/2016/09/07/next-wave-deep-learning-architectures/) 이후 딥러닝 HW에 대한 전망 (2016년 3Q 기준)
* [Deep Architecture Genealogy](https://github.com/hunkim/deep_architecture_genealogy)
* [Reward Augmented Maximum Likelihood for Neural Structured Prediction](http://static.googleusercontent.com/…/pubs/archive/45580.pdf)
  * reinforcement learning에서의 아이디어를 가져와 maximum likelihood objective를 확장해 training data로부터 추가적인 데이터를 샘플링
  * 결과적으로 알고리즘은 간단한 데이터 전처리에 불과한, Speech recognition과 neural machine translation 모두에 있어서 상당한 성능의 향상
  * reinforcement learning과 supervised learning의 아이디어가 결합. structured prediction에서 전통적인 기계학습의 아이디어와 신경망이 결합해 좋은 결과를 가져옴
* [Deep Learning Reading Group: SqueezeNet](http://www.kdnuggets.com/2016/09/deep-learning-reading-group-squeezenet.html)
* [Uncertainty in Deep Learning (PhD Thesis)](http://mlg.eng.cam.ac.uk/yarin/blog_2248.html)
* [Tensor Physics for Deep Learning](http://www.slideshare.net/uspace/tensor-physics-for-deep-learning)
* [Deep Visualization Toolbox](https://www.youtube.com/watch?v=AgkfIQ4IGaM)
* [DEVIEW 2016](https://deview.kr/2016/schedule) 딥러닝/머신러닝 관련 슬라이드
  * [통역하는 앵무새 파파고 이야기](http://www.slideshare.net/deview/134papago)
  * [딥러닝을 이용한 지역 컨텍스트 검색](http://www.slideshare.net/deview/221-67605830)
  * [딥러닝을 활용한 이미지 검색: 포토요약과 타임라인](http://www.slideshare.net/deview/222-20161024)
  * [딥러닝과 강화 학습으로 나보다 잘하는 쿠키런 AI 구현하기](http://www.slideshare.net/deview/ai-67608549)
    * [리뷰 DEVIEW : 쿠키런 AI 구현하기](https://mingrammer.com/review-deview-cookierun-ai)
    * [발표 자료](http://www.slideshare.net/carpedm20/ai-67616630)
    * [데모 영상](https://www.youtube.com/watch?v=exXD6wJLJ6s)
    * [첫번째 시도 (Deep Q-Network)](https://youtu.be/XsJWbAd6rYk)
    * [두번째 시도 (+ Double Q-learning)](https://youtu.be/rurJICmHfHQ)
    * [세번째 시도 (+ Dueling Network)](https://youtu.be/XQJA1Rob0ng)
    * [Deep Q-learning](https://github.com/devsisters/DQN-tensorflow/)
    * [Dueling Newtork, Double Q-learning](https://github.com/carpedm20/deep-rl-tensorflow/)
    * [쿠키런과 같은 discrete action space가 아닌 continuous action space에서의 강화 학습 방법](https://github.com/carpedm20/NAF-tensorflow/)
  * [Backend 개발자의 Neural Machine Translation 개발기](http://www.slideshare.net/deview/224-backend-neural-machine-translation-67608580)
  * [YARN 기반의 Deep Learning Application Cluster 구축](http://www.slideshare.net/deview/225yarn-deep-learning-application-cluster)
  * [Multimodal Residual Learning for Visual Question-Answering](http://www.slideshare.net/deview/multimodal-residual-learning-for-visual-questionanswering)
  * [딥러닝 예제로 보는 개발자를 위한 통계](http://www.slideshare.net/deview/216-67609104)
  * [Deep Recurrent Neural Network를 이용한 대용량 텍스트 마이닝 기술 및 실제 응용사례](http://www.slideshare.net/deview/226-67609105)
  * [빅데이터 분석에 적합한 LDA & HDP 베이지안 토픽모형에 대한 알고리즘](http://www.slideshare.net/deview/214-67608573)
* [Deep Learning is Revolutionary - 10 reasons why deep learning is living up to the hype](https://medium.com/@olivercameron/deep-learning-is-revolutionary-d0f3667bafa0)
* [Intelligence Platform Stack](https://medium.com/@surmenok/intelligence-platform-stack-8c623f71f990)
* [UFLDL Tutorial](http://deeplearning.stanford.edu/wiki/index.php/UFLDL_Tutorial)
* [Batch Normalization 설명 및 구현](https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/)
* [Batch normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](http://blog.soundcorset.kr/2017/07/batch-normalization-accelerating-deep.html)
* [Deep Learning - Achieve faster training of deep neural networks on a robust, scalable infrastructure](https://software.intel.com/en-us/ai/deep-learning)
* [CPU, GPU Put to Deep Learning Framework Test](https://www.nextplatform.com/2016/09/01/cpu-gpu-put-deep-learning-framework-test/)
* [딥러닝의 역사와 기본 개념](http://bcho.tistory.com/1147)
* [김현호: 오늘 당장 딥러닝 실험하기 - PyCon Korea 2015](https://www.youtube.com/watch?v=j-CojQwIt70)
* [Nuts and Bolts of Applying Deep Learning](https://kevinzakka.github.io/2016/09/26/applying-deep-learning/)
* [한눈에 보는 실리콘밸리 AI 트렌드(2)](https://brunch.co.kr/@synabreu/13)
* [The major advancements in Deep Learning in 2016](https://tryolabs.com/blog/2016/12/06/major-advancements-deep-learning-2016/)
* [Deep Learning Into Advance - 1. Image, ConvNet](http://www.slideshare.net/hellovista/deep-learning-into-advance-1-image-convnet)
* [Deep Learning SIMPLIFIED](https://www.youtube.com/playlist?list=PLjJh1vlSEYgvGod9wWiydumYl8hOXixNu)
* [Intel® Distribution for Python for high performance to supercharge all your Python applications on modern Intel platforms](https://software.intel.com/en-us/intel-distribution-for-python)
* [Deep Learning Demystified](https://www.youtube.com/watch?v=Q9Z20HCPnww&spfreload=10)
* [Recognizing Sounds (A Deep Learning Case Study)](https://medium.com/@awjuliani/recognizing-sounds-a-deep-learning-case-study-1bc37444d44d)
* [Has Deep Learning Made Traditional Machine Learning Irrelevant?](http://www.datasciencecentral.com/profiles/blogs/has-deep-learning-made-traditional-machine-learning-irrelevant)
* [Feedback Networks](https://youtu.be/MY5Uhv38Ttg)
  * [paper](https://arxiv.org/abs/1612.09508)
  * 영상분야 Deep Learning에서 일반적인 학습 모델은 연속적인 ConvNets layers를 이용하여 Feature를 추출한 다음 classification layer가 이어지는 모델을 기반
  * 본 논문에서는 이러한 일반적인 Feedfoward Multi Layers 대신 동일한 목표를 달성 할 수있는 대안을 제시
  * Recurrent Neural Networks의 개념을 도입하여 이전 출력에서 받은 피드백을 기반으로 반복적으로 표현이 형성되는 Feedback 기반 접근 방식을 제시
  * Feedback 기반 접근 방식은 Feedfoward보다 몇 가지 장점
    * 연산과정 중 초기에 예측 가능
    * 출력은 자연스럽게 레이블 공간의 계층 구조 (예 : 분류법)를 따르며 커리큘럼의 새로운 기초를 제공
    * Feedback 네트워크는 이러한 장점외에 Feedfoward 대응 네트워크와 비교하여 상당히 다른 표현의 개발이 가능(Feedback architecture (예 : skip connections in time) 및 디자인 선택 (예 : 피드백 길이))
* [A Theory Explains Deep Learning](http://www.deductiontheory.com/2016/12/study-deep-learning-from-scratch.html)
* [20170121 한국인공지능협회 - 제7차 오픈세미나 - 딥러닝](https://www.youtube.com/watch?v=FtHwOo5aICI)
  * [딥러닝 살짝 보기](https://docs.google.com/presentation/d/1K7imkoZy0drztv5_ylP8ZajuM_lUO9wk_nlhp9Z1-vQ/)
* [DeepMind just published a mind blowing paper: PathNet.](https://medium.com/@thoszymkowiak/deepmind-just-published-a-mind-blowing-paper-pathnet-f72b1ed38d46)
  * [Tensorflow Implementation of PathNet: Evolution Channels Gradient Descent in Super Neural Networks https://arxiv.org/pdf/1701.08734.pdf](https://github.com/jaesik817/pathnet)
  * [PyTorch implementation of PathNet: Evolution Channels Gradient Descent in Super Neural Networks](https://github.com/kimhc6028/pathnet-pytorch)
* [Bringing HPC Techniques to Deep Learning](http://research.baidu.com/bringing-hpc-techniques-deep-learning/)
  * 여러대 GPU머신을 이용하여 parallel하게 학습할 때 네트웍 오버헤드 때문에 오히려 속도가 감소
  * 바이두에서 ring allreduce라는 알고리즘으로 해결
* [実世界の人工知能@DeNA TechCon 2017](https://www.slideshare.net/pfi/dena-techcon-2017)
* [Deep Forest: Towards An Alternative to Deep Neural Networks](https://arxiv.org/abs/1702.08835)
* **[Asynchronous Advantage Actor-Critic (A3C)](https://jay.tech.blog/2017/01/19/asynchronous-advantage-actor-critic-a3c/)**
* [스스로 코딩을 하는 인공지능의 현 주소-Deepcoder](http://etinow.me/187)
* [Improving Hardware Efficiency for DNN Applications](https://www.slideshare.net/ChesterChen/improving-hardware-efficiency-for-dnn-applications)
* [DeepLearning 연구 2016 년의 정리](https://translate.google.com/translate?sl=ja&tl=ko&js=y&prev=_t&hl=ko&ie=UTF-8&u=http%3A%2F%2Fqiita.com%2Feve_yk%2Fitems%2Ff4b274da7042cba1ba76&edit-text) 일본어 번역
* [Squeezing Deep Learning Into Mobile Phones](https://www.slideshare.net/anirudhkoul/squeezing-deep-learning-into-mobile-phones)
* [딥러닝 분산처리 기술동향](https://ettrends.etri.re.kr/ettrends/pubreader.do?volume=31&issue=3&page=131&paperno=0905002137)
* [Understanding deep learning requires rethinking generalization (2017) 1/2](https://www.slideshare.net/JungHoonSeo2/understanding-deep-learning-requires-rethinking-generalization-2017-12)
* [Understanding deep learning requires rethinking generalization (2017) 2 2(2)](https://www.slideshare.net/JungHoonSeo2/understanding-deep-learning-requires-rethinking-generalization-2017-2-22)
* [Deep Learning From A to Z (Raphael Gontijo Lopes)](https://www.youtube.com/watch?v=DYlHnxfrrZY)
* [Linear algebra cheat sheet for deep learning](https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c)
* [The Black Magic of Deep Learning - Tips and Tricks for the practitioner](https://nmarkou.blogspot.com/2017/02/the-black-magic-of-deep-learning-tips.html)
* [Game Theory reveals the Future of Deep Learning](https://medium.com/intuitionmachine/game-theory-maps-the-future-of-deep-learning-21e193b0e33a)
* [todaysdeeplearning.com](http://www.todaysdeeplearning.com/)
* [Try Deep Learning in Python now with a fully pre-configured VM](https://medium.com/@ageitgey/try-deep-learning-in-python-now-with-a-fully-pre-configured-vm-1d97d4c3e9b) VMWare
* [Why Momentum Really Works](http://distill.pub/2017/momentum/)
* [dlwiki.finfra.com/start](http://dlwiki.finfra.com/start)
* [Deconvolution and Checkerboard Artifacts](http://distill.pub/2016/deconv-checkerboard/)
* [Compressing and regularizing deep neural networks 번역](https://fuzer.github.io/Compressing-and-regularizing-deep-neural-networks/)
* [Deep Learning for Program Synthesis](https://www.microsoft.com/en-us/research/blog/deep-learning-program-synthesis/)
* [Welcome to The Advanced Matrix Factorization Jungle](https://sites.google.com/site/igorcarron2/matrixfactorizations)
* [인공지능은 의료를 어떻게 혁신할 것인가 (1) 제2의 기계시대와 의료 인공지능](http://www.yoonsupchoi.com/2017/05/07/ai-medicine-1/)
* [Deep learning techniques](https://www.researchgate.net/publication/316829498_Deep_learning_techniques_-_overview)
* [Deep Learning Methods for Image Classification](http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2015/NN%20Lecture/dcnn-intro-WinstonHsu-15s.pdf)
* [Kullback-Leibler Divergence Explained](https://www.countbayesie.com/blog/2017/5/9/kullback-leibler-divergence-explained)
* [The Strange Loop in Deep Learning](https://medium.com/intuitionmachine/the-strange-loop-in-deep-learning-38aa7caf6d7d)
* [Training a deep learning model to steer a car in 99 lines of code](https://hackernoon.com/training-a-deep-learning-model-to-steer-a-car-in-99-lines-of-code-ba94e0456e6a)
* [Lane Detection with Deep Learning (Part 1)](https://medium.com/towards-data-science/lane-detection-with-deep-learning-part-1-9e096f3320b7)
* 카카오AI리포트
  * [ICML,NIPS 발표논문 분석](https://brunch.co.kr/@kakao-it/63)
  * [딥러닝연구의 현재와미래 part 1](https://brunch.co.kr/@kakao-it/65)
  * [AI 의료영상 기술 활용 사례](https://brunch.co.kr/@kakao-it/81)
  * [더욱 똑똑해진 AI 광고 알고리듬](https://brunch.co.kr/@kakao-it/84)
* [Deep Learning Project Workflow](https://github.com/thomasj02/DeepLearningProjectWorkflow)
* [Affine Transformation](http://blog.naver.com/atelierjpro/221014255070)
* [THE MNIST DATABASE of handwritten digits](http://yann.lecun.com/exdb/mnist/)
* **[Applying deep learning to real-world problems](https://medium.com/merantix/applying-deep-learning-to-real-world-problems-ba2d86ac5837)**
* [Practical UseCases of Deep Learning Techniques](http://www.cognitivetoday.com/2016/11/practical-deeplearning-usecases-2.html)
* [Practical UseCases of Deep Learning Techniques… Part II](http://www.cognitivetoday.com/2017/02/practical-deeplearning-usecases.html)
* [알기쉬운 Variational autoencoder](https://www.slideshare.net/ssuser06e0c5/variational-autoencoder-76552518)
* [Policy Gradient](https://nbviewer.jupyter.org/format/slides/gist/kkweon/e8522c4b04e1e7c6665c53ac12ac7f1d#)
* [You can probably use deep learning even if your data isn't that big](http://beamandrew.github.io/deeplearning/2017/06/04/deep_learning_works.html)
* **[번역 2017년 iOS를 위한 나의 개발 툴 셋](http://blog.canapio.com/107)**
* [The $1700 great Deep Learning box: Assembly, setup and benchmarks](https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415)
* [Kernel Mixture Networks](https://janvdvegt.github.io/2017/06/07/Kernel-Mixture-Networks.html)
* [Taxonomy of Methods for Deep Meta Learning](http://www.kdnuggets.com/2017/06/taxonomy-methods-deep-meta-learning.html)
* [신경 번역 seq2seq 모델 튜토리얼](https://www.facebook.com/nextobe1/posts/339880869781248)
* [seq2seq icml tutorial](https://sites.google.com/view/seq2seq-icml17/)
* [Sequence to sequence tutorial](https://towardsdatascience.com/sequence-to-sequence-tutorial-4fde3ee798d8)
* [Google net](https://www.slideshare.net/BrianKim244/google-net)
* ["Advances in Deep Neural Networks," at ACM Turing 50 Celebration](https://www.youtube.com/watch?v=mFYM9j8bGtg)
* [BUILDING A SOUND CLASSIFIER FROM SCRATCH USING NEURAL NETWORKS](https://www.skcript.com/svr/building-audio-classifier-nueral-network/)
* [WHEN NOT TO USE DEEP LEARNING](http://hyperparameter.space/blog/when-not-to-use-deep-learning/)
* [MLJejuCamp - Call for application for Machine Learning Camp Jeju 2017](https://github.com/TensorFlowKR/MLJejuCamp)
  * [Special Seminar](https://github.com/TensorFlowKR/MLJejuCamp/blob/master/Special_Seminar.md)
* [김태희의 닮은꼴도 머신러닝으로 구분할 수 있을까?](https://brunch.co.kr/@kmbmjn95/20)
* [The limitations of deep learning](https://blog.keras.io/the-limitations-of-deep-learning.html)
* [번외 Why does deep and cheap learning work so well?](http://suma_maple.blog.me/221064089784)
* **[MNIST 시각화 - 차원 감소](https://brunch.co.kr/@chris-song/37)**
* [#4.0. 정보량을 나타내는 엔트로피 (Entropy)](https://www.youtube.com/watch?v=zJmbkp9TCXY&list=PL0oFI08O71gKEXITQ7OG2SCCXkrtid7Fq&index=19)
* [Designing a Deep Learning Project](https://medium.com/@erogol/designing-a-deep-learning-project-9b3698aef127)
* [github.com/parkskwan](https://github.com/parkskwan)
* [Create self-driving trucks inside Euro Truck Simulator 2](https://medium.com/mars-auto/create-self-driving-trucks-inside-euro-truck-simulator-2-c64424d528ed)
  * [europilot - A toolkit for controlling Euro Truck Simulator 2 with python to develop self-driving algorithms](https://github.com/marshq/europilot)
* [딥러닝, 머신러닝의 차이점은?](https://brunch.co.kr/@itschloe1/8)
* [RNNoise: 소음 감소를 위한 딥러닝 모델](http://hacks.mozilla.or.kr/2017/10/rnnoise-using-deep-learning-for-noise-suppression/)
* [Why Probability Theory Should be Thrown Under the Bus](https://medium.com/intuitionmachine/why-probability-theory-should-be-thrown-under-the-bus-36e5d69a34c9)
* [An Overview of ResNet and its Variants](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)
* [Software 2.0](https://medium.com/@karpathy/software-2-0-a64152b37c35)
* [Estimating an Optimal Learning Rate For a Deep Neural Network](https://medium.com/@surmenok/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0)
* [Improving the way we work with learning rate](https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b)
* [딥러닝을 제대로 이해하기 위해서 필요한 배경지식맵](http://bahnsville.tistory.com/1155)
* [Deep Learning Specialization by Andrew Ng — 21 Lessons Learned](https://towardsdatascience.com/deep-learning-specialization-by-andrew-ng-21-lessons-learned-15ffaaef627c)
* [Grad CAM을 이용한 딥러닝 모형 해석](http://freesearch.pe.kr/archives/4685)
* [딥러닝이 덧셈을 하는 방법, Attention Mechanism으로 살펴보기](http://freesearch.pe.kr/archives/4724)
* [gradient를 활용한 DNN 해석 방안](https://seujung.github.io/deep_learning/2017/12/19/Itpr_model.html)
* [onnx.ai](https://onnx.ai)
  * [github.com/onnx/onnx](https://github.com/onnx/onnx)
  * [Getting Started](http://onnx.ai/getting-started)
  * [Operator Schemas](https://github.com/onnx/onnx/blob/master/docs/Operators.md)
  * [Viewer for ONNX neural network models https://www.lutzroeder.com/ai](https://github.com/lutzroeder/Netron)
  * [Convert ONNX models into Apple CoreML format](https://github.com/onnx/onnx-coreml)
  * [Tensorflow Backend for ONNX](https://github.com/onnx/onnx-tensorflow)
* [Comparison of Deepnet & Neuralnet](https://www.datasciencecentral.com/profiles/blogs/comparison-of-deepnet-neuralnet)
* [Modern Theory of Deep Learning: Why Does It Work so Well](https://medium.com/mlreview/modern-theory-of-deep-learning-why-does-it-works-so-well-9ee1f7fb2808)
* [당근마켓에서 딥러닝 활용하기](https://medium.com/n42-corp/%EB%8B%B9%EA%B7%BC%EB%A7%88%EC%BC%93%EC%97%90%EC%84%9C-%EB%94%A5%EB%9F%AC%EB%8B%9D-%ED%99%9C%EC%9A%A9%ED%95%98%EA%B8%B0-3b48844eba62)
* [Beyond Deep Learning – 3rd Generation Neural Nets](https://www.datasciencecentral.com/profiles/blogs/beyond-deep-learning-3rd-generation-neural-nets)
* [AI and Deep Learning in 2017 – A Year in Review](http://www.wildml.com/2017/12/ai-and-deep-learning-in-2017-a-year-in-review/)
  * [2017년 인공지능과 딥러닝 리뷰 (번역 by softgear)](http://softgearko.blogspot.com/2018/01/2017-by-softgear.html)
* [The Google Brain Team — Looking Back on 2017 (Part 1 of 2)](https://research.googleblog.com/2018/01/the-google-brain-team-looking-back-on.html)
* [Interpreting Deep Neural Networks](http://www.shallowmind.co/jekyll/pixyll/2017/12/30/tree-regularization/)
* [Deep Learning from first principles in Python, R and Octave – Part 1](https://gigadom.wordpress.com/2018/01/04/deep-learning-from-basic-principles-in-python-r-and-octave-part-1/)
* [Deep Learning from first principles in Python, R and Octave – Part 2](https://gigadom.wordpress.com/2018/01/11/deep-learning-from-first-principles-in-python-r-and-octave-part-2/)
* [NBT 교육 1탄. Ad-Tech](https://www.youtube.com/playlist?list=PLdWpPj-RibJsrc5NzIsO9TKs3ac4HBg8Z)
* [금융의 역사를 통해 본 딥러닝의 함정](https://www.youtube.com/watch?v=mpZSb9DzAR8&feature=youtu.be)
  * [금융의 역사를 통해 본 딥러닝의 함정](https://www.slideshare.net/NaverEngineering/ss-86209440)
* [Introduction of Neural Network Console](https://www.youtube.com/watch?v=-lXjnaUSEtM&feature=youtu.be)
  * [Sony’s Neural Network Console Software is Available at No Cost: A Deep Learning Tool for Training, Evaluating & Designing Neural Networks for Artificial Intelligence Creation](https://www.photoxels.com/sony-neural-network-console-software-is-available-at-no-cost-a-deep-learning-tool-for-training-evaluating-designing-neural-networks-for-artificial-intelligence-creation/)
* [Geometric Deep Learning | Michael Bronstein](https://www.radcliffe.harvard.edu/video/geometric-deep-learning-michael-bronstein)
* [8 Deep Learning Best Practices I Learned About in 2017](https://hackernoon.com/8-deep-learning-best-practices-i-learned-about-in-2017-700f32409512)
* [deepfakes_faceswap - Faceswap is a tool that utilizes deep learning to recognize and swap faces in pictures and videos](https://github.com/deepfakes/faceswap)
  * [practice](https://gist.github.com/hyunjun/fe2293dc299e0d0c4a1a761f13a173cd)
* [Guitar-Set, a New Dataset for Music Information Retrieval](https://medium.com/center-for-data-science/guitar-set-a-new-dataset-for-music-information-retrieval-41b7861a87d7)
* [Deep Learning summary for 2017: Text and Speech Applications](https://towardsdatascience.com/deep-learning-summary-for-2017-text-and-speech-applications-9ea02bb3835f)
* [The Deep Learning Roadmap](https://medium.com/intuitionmachine/the-deep-learning-roadmap-f0b4cac7009a)
* [Continuous Unsupervised Training of Deep Architectures](https://www.slideshare.net/VincenzoLomonaco/continuous-unsupervised-training-of-deep-architectures)
  * [core50 - A new Dataset and Benchmark for Continuous Object Recognition](https://vlomonaco.github.io/core50/)
* [Only Numpy: Implementing Highway Network, OOP approach with Mini Batch with Interactive Code](https://towardsdatascience.com/only-numpy-implementing-highway-network-oop-approach-with-mini-batch-with-interactive-code-b5c2de2df842)
* [New Deep Learning Techniques](http://www.ipam.ucla.edu/programs/workshops/new-deep-learning-techniques/?tab=schedule)

# AlphaGo
* [Rochester-NRT/AlphaGo](https://github.com/Rochester-NRT/AlphaGo)
* [AlphaGo의 인공지능 알고리즘 분석](http://spri.kr/post/14725)
* [AlphaGo 알고리즘 요약](http://www.slideshare.net/zenithon/alphago?from_m_app=android)
* [알파고 (바둑 인공지능)의 작동 원리](http://www.slideshare.net/ShaneSeungwhanMoon/ss-59226902)
* [이세돌과 대국으로 ‘알파고’ 설계자가 꿈꾸는 것은?](http://www.bloter.net/archives/251528)
* [모두의 알파고](http://www.slideshare.net/DonghunLee20/ss-59338971)
* [Mastering the game of Go with deep neural networks and tree search](http://www.willamette.edu/~levenick/cs448/goNature.pdf)
* [AlphaGo에 적용된 딥러닝 알고리즘 분석](https://brunch.co.kr/@justinleeanac/2)
* [알파고는 어떻게 바둑을 둘까](https://brunch.co.kr/@madlymissyou/9)
* [이세돌이 알파고를 이기려면 선입견을 버려야 한다](http://m.blog.daum.net/_blog/_m/articleView.do?blogid=0NovT&articleno=3331)
* [바둑인을 위한 알파고](http://www.slideshare.net/DonghunLee20/ss-59413793)
* [알파고 해부하기 1부](http://www.slideshare.net/DonghunLee20/1-59501887)
* [알파고 해부하기 2부](http://www.slideshare.net/DonghunLee20/2-59620244)
* [알파고 해부하기 3부](http://www.slideshare.net/DonghunLee20/3-61454159)
* [알파고, 강화학습을 현실에 데뷔시키다](http://t-robotics.blogspot.co.id/2016/03/blog-post_26.html)
* [알파고는 어떤 컴퓨터를 썼을까?](http://www.slideshare.net/jysoo/ss-61950212)
* [AlphaGo 대국 - 한국어](https://deepmind.com/research/alphago/alphago-games-korean/)
* [알파고는 스스로 신의 경지에 올랐다](https://brunch.co.kr/@madlymissyou/18)
* [Alphago zero cheatsheet](https://medium.com/applied-data-science/alphago-zero-explained-in-one-diagram-365f5abf67e0)
* [AlphaGo Zero - How and Why it Works](http://tim.hibal.org/blog/alpha-zero-how-and-why-it-works/)
* [Deep Learning: AlphaGo Zero Explained In One Picture](https://www.datasciencecentral.com/profiles/blogs/deep-learning-alphago-zero-explained-in-one-picture)
* [minigo - An open-source implementation of the AlphaGoZero algorithm](https://github.com/tensorflow/minigo)

# Amazon
* [Amazon DSSTNE: Deep Scalable Sparse Tensor Network Engine](https://github.com/amznlabs/amazon-dsstne)

# Backpropagation
* [Deep Learning - Geoffrey Hinton - how to do backpropagation in a brain](https://www.youtube.com/watch?v=kxp7eWZa-2M&feature=youtu.be&t=38m13s)
* [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
* [Neural Networks: The Backpropagation algorithm in a picture](http://www.datasciencecentral.com/profiles/blogs/neural-networks-the-backpropagation-algorithm-in-a-picture)
* **[Backpropagation 예제와 함께 완전히 이해하기](http://jaejunyoo.blogspot.com/2017/01/backpropagation.html)**
* [A Derivation of Backpropagation in Matrix Form](http://sudeepraja.github.io/Neural/)
* **[Calculus on Computational Graphs: Backpropagation](http://colah.github.io/posts/2015-08-Backprop/index.html)**
* [Gradient Descent with Backpropagation](http://outlace.com/Beginner-Tutorial-Backpropagation/)
* [A Step by Step Backpropagation Example](http://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
* [A Step by Step Backpropagation Example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)
* [계산 그래프로 역전파 이해하기](https://brunch.co.kr/@chris-song/22)
* [Back-Propagation is very simple. Who made it Complicated ?](https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c)
* [Backpropagation Through Time: recurrent neural network training technique](http://www.techleer.com/articles/185-backpropagation-through-time-recurrent-neural-network-training-technique/)
* [Backpropagation: A supervised learning neural network method](http://www.techleer.com/articles/242-backpropagation-a-supervised-learning-neural-network-method/)
* [Backprop is not just the chain rule](http://timvieira.github.io/blog/post/2017/08/18/backprop-is-not-just-the-chain-rule/)
  * 단순히 체인룰에 의해 계산되는 BP로만 보면 안된다는 글
  * BP와 같은 종류를 autodiff 라고 이 글에서는 표현했는데, 이게 우리 수학에서 배운 미분과는 다르다는 것을 강조(기존의 미분을 symbolic differentiation 이라고 표기)
  * 기존의 방식은 intermediate variables를 쓸 수 없지만, autodiff 에서는 이것이 가능해서 간결하고 효율적으로 사용 가능(마치 함수와 같음)
  * 또한 방정식의 선형시스템의 형태를 갖추었고, acyclic graph의 형식으로 연산이 연결되므로 전체적인 연산의 복잡도가 대폭 감소
  * 이걸 잘 mix하면 대부분의 문제 해결 가능
  * 이걸 수학적으로 풀어내는 과정에서 Lagrange multiplier, implicit function theorem 등 다소 복잡하고 어려운 내용들이 등장
  * 결론적으로 BP는 단순 체인룰을 도입해서 풀어낸 것이 아니라, intermediate variables를 가진 프로그램으로 전환시켜서 효율과 유연성을 갖추게 하였고, 또한 더 복잡한 문제를 풀어낼 수 있는 기초가 될 수 있도록 했다는 내용
* [Gradient Descent Overview](https://brunch.co.kr/@chris-song/50)
  * [simple_gradient_descent.py](https://gist.github.com/chris-chris/808d383f19f74c537d9b4476b019c59a)
* [Backpropagation In Convolutional Neural Networks](http://www.jefkine.com/general/2016/09/05/backpropagation-in-convolutional-neural-networks/)
* [Backpropagation](https://www.slideshare.net/MingukKang/backpropagation-85544666?qid=a791b026-a2e5-4d08-bda2-54ad1608b4b7)
* [Efficient Batch Normalization](https://costapt.github.io/2016/07/09/batch-norm-alt/)
* [Fast-BN-BackPropagation.pdf](https://github.com/hccho2/hccho2.github.io/blob/master/Fast-BN-BackPropagation.pdf/)
* [Gradient Descent & Normal Eq.](https://www.youtube.com/watch?v=M9Gsi3VBTYM&feature=youtu.be)
* [Numerical Gradient Descent vs. BackPropagation](https://github.com/dgtgrade/HumanLearning/blob/master/1102.py)
* [Keep it simple! How to understand Gradient Descent algorithm](http://www.kdnuggets.com/2017/04/simple-understand-gradient-descent-algorithm.html)
* [Gradient Descent(경사하강법)](http://blog.anthouse.co.kr/221210836545)
* [경사하강법에서 모든 parameter를 동시에 갱신해야 하는 이유](https://blog.naver.com/atelierjpro/220954798036)
* [An overview of gradient descent optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/)
* [Gradient descent, how neural networks learn | Deep learning, part 2](https://www.youtube.com/watch?v=IHZwWFHWa-w)
* [경사하강법](http://likejazz.com/post/171142973969/%EA%B2%BD%EC%82%AC%ED%95%98%EA%B0%95%EB%B2%95%EC%9D%80-%EC%A0%95%EB%A7%90-%EC%8B%A0%EB%B9%84%EB%A1%9C%EC%9A%B4-%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98%EC%9D%B4%EB%8B%A4-%EC%96%B4%EB%96%BB%EA%B2%8C-%EA%B7%B8%EB%A0%87%EA%B2%8C-%EC%9E%98-%EB%8F%99%EC%9E%91%ED%95%98%EB%8A%94%EC%A7%80-%EC%97%AC%EC%A0%84%ED%9E%88-%EC%8B%A0%EA%B8%B0%ED%95%98%EB%8B%A4)
* [Improving Vanilla Gradient Descent](https://towardsdatascience.com/improving-vanilla-gradient-descent-f9d91031ab1d)
* [The Two Phases of Gradient Descent in Deep Learning](https://medium.com/intuitionmachine/the-peculiar-behavior-of-deep-learning-loss-surfaces-330cb741ec17)

# Baidu
* [Silicon Valley AI Lab](https://svail.github.io/)
* [상호 작용을 통해 말하기 학습](https://www.nextobe.com/single-post/2017/06/09/%EB%B0%94%EC%9D%B4%EB%91%90-%EC%83%81%ED%98%B8-%EC%9E%91%EC%9A%A9%EC%9D%84-%ED%86%B5%ED%95%B4-%EB%A7%90%ED%95%98%EA%B8%B0-%ED%95%99%EC%8A%B5)
* [Baidu Reseaech Presentation @ GTC](https://www.facebook.com/nextobe1/photos/a.313464989089503.1073741829.303538826748786/340449486391053)
* [DeepBench - Benchmarking Deep Learning operations on different hardware](https://github.com/baidu-research/DeepBench)
  * 심층 신경 네트워크를 학습 할 때 서로 다른 프로세서가 어떻게 작동하는지 평가할 수 있는 최초의 도구
  * [An update to DeepBench with a focus on deep learning inference](https://svail.github.io/DeepBench-update/)

# Book
* [머신러닝에서 딥러닝까지](http://digital.kyobobook.co.kr/digital/ebook/ebookDetail.ink?selectedLargeCategory=001&barcode=480150001023P&orderClick=LAN&Kc)
* [C++와 CUDA C로 구현하는 딥러닝 알고리즘 Vol.1 Restricted Boltzman Machine의 이해와 Deep Belief Nets 구현](http://www.acornpub.co.kr/book/dbn-cuda-vol1)
* [Deep Learning 이론과 실습 (개정중)](https://wikidocs.net/book/498)
* [Deep Learning - A Practitioner's Approach](http://shop.oreilly.com/product/0636920035343.do)
* [Fundamentals of Deep Learning](http://shop.oreilly.com/product/0636920039709.do)
  * [‘Fundamental of Deep Learning’ Preview](https://tensorflowkorea.wordpress.com/2016/04/18/fundamental-of-deep-learning-preview/#more-2018)
* [Deep Learning - An MIT Press book in preparation](http://www.deeplearningbook.org/)
  * [DeepLearningBook](https://github.com/HFTrader/DeepLearningBook)
  * [Notation](http://www.deeplearningbook.org/contents/notation.html)
  * [DeepLearningBook.pdf](https://l.facebook.com/l.php?u=https%3A%2F%2Fraw.githubusercontent.com%2FHFTrader%2FDeepLearningBook%2Fmaster%2FDeepLearningBook.pdf&h=ATOmM--Cv2DlWcHaWEzjNxiu9As5K0h4cO7R0NQ0wQGMXweMaq0Nh5f4O8GoGGDKoQfkXafqXAYWmeQyxoDNUnH3Qsl7DvB4PlQc9p6w4heLx2JjgZWC6mLyKIG2A2wf4Azz_sYLtX_wJ_FfLw38ptv_s-_0wkWu2xgeBkiuIEgaBYHnbkEke3_gi80SG4VH6f0k5Z-5GZKaPXmQYeT0laVv0gQdDV-dfAOUI3gwZxybqwTO6PeAjBc2Hhs522oPYh-eoy3lz-wnK170vE2GFEeCo_dFBQVY4GpFtRBt7Djznuo)
* [Book: Deep Learning With Python](http://www.datasciencecentral.com/forum/topics/book-deep-learning-with-python) Theano and TensorFlow using Keras
* [Deep Learning With Python](https://machinelearningmastery.com/deep-learning-with-python/)
* [Fundamental of Reinforcement Learning](https://dnddnjs.gitbooks.io/rl/content/)
* [Reinforcement Learning: An Introduction](https://webdocs.cs.ualberta.ca/~sutton/book/bookdraft2016sep.pdf)
  * [교과서 읽고 느낀점](http://blog.naver.com/atelierjpro/220896756412)
* [도서 마인드맵](https://www.mindmeister.com/812276967/_)
* [Artificial Intelligence Book of January 2017](http://artificialbrain.xyz/artificial-intelligence-book-of-january-2017/)
* [Deep-Learning-for-Beginners - Sample code in MATLAB/Octave and Python for Deep Learning for Beginners](https://github.com/philbooks/Deep-Learning-for-Beginners)
* [Free Deep Learning Textbook](http://www.datasciencecentral.com/profiles/blogs/free-deep-learning-textbook)
* [11 Deep Learning Articles, Tutorials and Resources](http://www.datasciencecentral.com/profiles/blogs/11-deep-learning-articles-tutorials-and-resources)
* [파이썬을 이용한 머신러닝, 딥러닝 실전 개발 입문](http://wikibook.co.kr/python-machine-learning/?path=facebook)
  * [머신러닝/딥러닝 실전 입문](https://www.youtube.com/playlist?list=PLBXuLgInP-5m_vn9ycXHRl7hlsd1huqmS)
* [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python)
  * [github.com/fchollet/deep-learning-with-python-notebooks](https://github.com/fchollet/deep-learning-with-python-notebooks)
  * [Companion Jupyter notebooks for the book "Deep Learning with Python"](https://www.floydhub.com/redeipirati/projects/deep-learning-with-python-notebooks)
* [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
  * **[numpy-neuralnet-exercise](https://github.com/hwalsuklee/numpy-neuralnet-exercise)**

# CAN
* [CAN (Creative Adversarial Network) — Explained](https://hackernoon.com/can-creative-adversarial-network-explained-1e31aea1dfe8)

# Deep Q Learning DQL
* [Deep Q-Learning (Space Invaders)](http://maciejjaskowski.github.io/2016/03/09/space-invaders.html)
* [Using Deep Q-Network to Learn How To Play Flappy Bird](https://github.com/DeepLearningProjects/DeepLearningFlappyBird)
* **[Deep Reinforcement Learning](http://andersonjo.github.io/artificial-intelligence/2017/06/03/Deep-Reinforcement-Learning/)**
  * [Deep Q-Learning with Pytorch](https://github.com/AndersonJo/dqn-pytorch)
* [Hello DeepQ](http://koaning.io/hello-deepq.html)
* [Deep Q Learning with Keras and Gym](https://keon.io/rl/deep-q-learning-with-keras-and-gym/)
* [Q-learning Test](http://computingkoreanlab.com/app/jAI/jQLearning/)

# Extreme Learning Machines
* [Extreme Learning Machines](http://www.ntu.edu.sg/home/egbhuang/pdf/IEEE-IS-ELM.pdf)
* [Basic ELM Algorithms](http://www.ntu.edu.sg/home/egbhuang/elm_codes.html)

# Facebook
* [Facebook Presentation @ GTC](https://www.facebook.com/nextobe1/photos/a.313464989089503.1073741829.303538826748786/340462899723045)

# GAN Generative Adversarial Networks
* [All-About-the-GAN](https://github.com/hollobit/All-About-the-GAN)
* [really-awesome-gan](https://github.com/nightrome/really-awesome-gan)
* NIPS 2016 Tutorial: Generative Adversarial Networks [paper](https://arxiv.org/pdf/1701.00160v1.pdf) [slide](http://www.iangoodfellow.com/slides/2016-12-04-NIPS.pdf)
* [SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient](https://arxiv.org/pdf/1609.05473v2.pdf)
  * GAN이 처음으로 sequence generation task에 사용
  * GAN은 진짜같은 Fake data를 만들어내는 Generator과 진짜 data와 Fake data를 구분해내는 Discriminator를 학습시키는 알고리즘
  * 실수 픽셀들로 이루어진 그림과 달리 discrete한 토큰들의 sequence를 생성해낼 때 현재 얼마나 Generator가 잘 학습을 하고 있는지 평가할 방법이 마땅치 않아 sequence generation task에서는 사용되지 않음
  * 이번에 발표된 SeqGAN 은 discriminator를 Policy Gradient 의 Reward 로 사용해서 이 문제를 해결, Text Generation, Music Generation Task 에 적용
  * [Ian Goodfellow (GAN 저자) 의 Reddit 문답(왜 NLP에 GAN이 사용되기 힘든가)](https://www.reddit.com/r/MachineLearning/comments/40ldq6/generative_adversarial_networks_for_text/)
* [번역 - Generative Adversarial Network (GAN) 설명](http://keunwoochoi.blogspot.com/2016/12/generative-adversarial-network-gan.html)
* [GANs will change the world](https://medium.com/@Moscow25/gans-will-change-the-world-7ed6ae8515ca)
* [A tensorflow implementation of Junbo et al's Energy-based generative adversarial network ( EBGAN ) paper](https://github.com/buriburisuri/ebgan)
* [초짜 대학원생의 입장에서 이해하는 Energy-Based Generative Adversarial Networks (1)](https://jaejunyoo.blogspot.com/2018/02/energy-based-generative-adversarial-nets-1.html)
* [초짜 대학원생의 입장에서 이해하는 f-GAN](http://jaejunyoo.blogspot.com/2017/06/f-gan.html)
* [초짜 대학원생의 입장에서 이해하는 f-GAN (2)](http://jaejunyoo.blogspot.com/2017/06/f-gan-2.html)
* [초짜 대학원생의 입장에서 이해하는 f-GAN (3)](http://jaejunyoo.blogspot.com/2017/07/f-gan-3.html)
* [초짜 대학원생 입장에서 이해하는 Generative Adversarial Nets (1)](http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-1.html)
* [초짜 대학원생 입장에서 이해하는 Generative Adversarial Nets (2)](http://jaejunyoo.blogspot.com/2017/01/generative-adversarial-nets-2.html)
* **[Generative adversarial networks](http://www.slideshare.net/ssuser77ee21/generative-adversarial-networks-70896091)**
* [Generative Adversarial Networks](https://github.com/nlintz/TensorFlow-Tutorials/blob/master/11_gan.ipynb)
* [PaintsChainer Demo](http://paintschainer.preferred.tech/)
  * [PaintsCahiner Code](https://github.com/pfnet/PaintsChainer)
  * [tai2an 본인이 올린 글](http://qiita.com/taizan/items/cf77fd37ec3a0bef5d9d)
  * [U-Net](http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/)
* [Chainer-GAN-lib](https://github.com/pfnet-research/chainer-gan-lib/blob/master/README.md)
* [초짜 대학원생의 입장에서 이해하는 Domain-Adversarial Training of Neural Networks (DANN) (1)](http://jaejunyoo.blogspot.com/2017/01/domain-adversarial-training-of-neural.html)
  * [Pr12 dann jaejun yoo](https://www.slideshare.net/thinkingfactory/pr12-dann-jaejun-yoo)
* [초짜 대학원생의 입장에서 이해하는 Domain-Adversarial Training of Neural Networks (DANN) (2)](http://jaejunyoo.blogspot.com/2017/01/domain-adversarial-training-of-neural-2.html)
* [초짜 대학원생의 입장에서 이해하는 Domain-Adversarial Training of Neural Networks (DANN) (3)](http://jaejunyoo.blogspot.com/2017/01/domain-adversarial-training-of-neural-3.html)
* [tf-dann-py35 - Tensorflow-gpu (1.0.0.rc2, Window, py35) implementation of Domain Adversarial Neural Network](https://github.com/jaejun-yoo/tf-dann-py35)
* [Domain Adaptation Methods](https://www.slideshare.net/samchoi7/domain-adaptation-methods)
  * [DOMAIN ADVERSARIAL NEURAL NETWORK](https://github.com/sjchoi86/advanced-tensorflow/blob/master/dann/dann_mnist.ipynb)
* [초짜 대학원생의 입장에서 이해하는 Deep Convolutional Generative Adversarial Network (DCGAN) (1)](http://jaejunyoo.blogspot.com/2017/02/deep-convolutional-gan-dcgan-1.html)
* [초짜 대학원생의 입장에서 이해하는 Deep Convolutional Generative Adversarial Network (DCGAN) (2)](http://jaejunyoo.blogspot.com/2017/02/deep-convolutional-gan-dcgan-2.html)
* [Dcgan](https://www.slideshare.net/BrianKim244/dcgan-77452250)
* [InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets](http://www.slideshare.net/ssuser06e0c5/infogan-interpretable-representation-learning-by-information-maximizing-generative-adversarial-nets-72268213)
* [Generative Adversarial Networks (GANs) in 50 lines of code (PyTorch)](https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f)
* [초짜 대학원생의 입장에서 이해하는 Unrolled Generative Adversarial Networks (1)](http://jaejunyoo.blogspot.com/2017/02/unrolled-generative-adversarial-network-1.html)
* [초짜 대학원생의 입장에서 이해하는 Unrolled Generative Adversarial Networks (2)](http://jaejunyoo.blogspot.com/2017/02/unrolled-generative-adversarial-network-2.html)
* [초짜 대학원생의 입장에서 이해하는 InfoGAN (1)](http://jaejunyoo.blogspot.com/2017/03/infogan-1.html)
* [초짜 대학원생의 입장에서 이해하는 InfoGAN (2)](http://jaejunyoo.blogspot.com/2017/03/infogan-2.html)
* [Read-through: Wasserstein GAN](http://www.alexirpan.com/2017/02/22/wasserstein-gan.html)
* [Wasserstein GAN 수학 이해하기 I](https://www.slideshare.net/ssuser7e10e4/wasserstein-gan-i)
* [Wasserstein GAN(WGAN) and the Kantorovich-Rubinstein Duality](http://nbviewer.jupyter.org/github/maestrojeong/wgan_duality/blob/master/WGAN_duality.ipynb)
* [Wasserstein GAN and the Kantorovich-Rubinstein Duality](https://vincentherrmann.github.io/blog/wasserstein/)
* [Wasserstein GAN: An Alternative To The Traditional GAN Training](https://www.techleer.com/articles/471-wasserstein-gan-an-alternative-to-the-traditional-gan-training/)
* [The story about WGAN](https://medium.com/@sunnerli/the-story-about-wgan-784be5acd84c)
* [InfoGAIL](https://www.slideshare.net/samchoi7/infogail)
* [A collection of Keras GAN notebooks](https://github.com/osh/KerasGAN)
* [AI기획 - 경쟁 통해 배우는 인공지능 기술 GAN](http://techm.kr/bbs/?t=Wh)
* [Generative Adversarial Networks Explained](http://www.rubedo.com.br/2017/03/generative-adversarial-networks.html)
* [겐스는 왜 기존 비창의 인공지능과 다른가?](https://www.linkedin.com/pulse/%EA%B2%90%EC%8A%A4-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5%EC%9D%80-%EC%99%9C-%EA%B8%B0%EC%A1%B4-%EB%B9%84%EC%B0%BD%EC%9D%98-%EA%B8%B0%EC%88%A0%EA%B3%BC-%EB%8B%A4%EB%A5%B8%EA%B0%80-sung-jin-james-kim)
* [Adversarial Attacks on Neural Network Policies](http://rll.berkeley.edu/adversarial/)
* [Disco GAN - SK T-Brain Research](https://www.facebook.com/notes/sk-t-brain/sk-t-brain-research/398821727155314)
  * [DiscoGAN - Official PyTorch implementation of Learning to Discover Cross-Domain Relations with Generative Adversarial Networks](https://github.com/SKTBrain/DiscoGAN)
  * [DiscoGAN in PyTorch - PyTorch implementation of Learning to Discover Cross-Domain Relations with Generative Adversarial Networks](https://github.com/carpedm20/DiscoGAN-pytorch)
  * [discogan_tensorflow.py](https://github.com/wiseodd/generative-models/blob/master/GAN/disco_gan/discogan_tensorflow.py)
  * [Tensorflow Implementation of DiscoGAN](https://github.com/GunhoChoi/DiscoGAN_TF)
  * [논문반/논문세미나 DiscoGAN](http://www.modulabs.co.kr/DeepLAB_library/12820)
    * [Discover Cross-Domain Relations with GAN (DiscoGAN) with TensorFlow & slim](https://github.com/ilguyi/discoGAN.tensorflow.slim)
  * [Generative Adversarial Networks for Style Transfer (LIVE)](https://www.youtube.com/watch?v=MgdAe-T8obE)
* [Generative Models - Collection of generative models, e.g. GAN, VAE in Pytorch and Tensorflow](https://github.com/wiseodd/generative-models)
  * [Agustinus Kristiadi's Blog](http://wiseodd.github.io/techblog/)
  * [GAN](https://github.com/wiseodd/generative-models/tree/master/GAN)
* [Keras Adversarial Models](https://github.com/bstriner/keras-adversarial)
* [초짜 대학원생의 입장에서 이해하는 LSGAN (1)](http://jaejunyoo.blogspot.com/2017/03/lsgan-1.html)
* [초짜 대학원생의 입장에서 이해하는 LSGAN (2)](http://jaejunyoo.blogspot.com/2017/04/lsgan-2.html)
* [Generative Adversarial Networks](http://cs.stanford.edu/people/karpathy/gan/)
* [aliensunmin.github.io/project/accv16tutorial](http://aliensunmin.github.io/project/accv16tutorial/)
* [Deep Feedforward Generative Models](http://aliensunmin.github.io/project/accv16tutorial/media/generative.pdf)
* [Generative Adversarial Networks to Make 8-bit Pixel Art](http://www.rubedo.com.br/2017/03/generative-adversarial-networks-to-make.html)
* [겐(GANs)이 꿈꾸는 인공지능 번역 끝판왕](https://medium.com/@jskDr/%EA%B2%90-gans-%EC%9D%B4-%EA%BF%88%EA%BE%B8%EB%8A%94-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%B2%88%EC%97%AD-%EB%81%9D%ED%8C%90%EC%99%95-4df872ffa13f)
* [Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks](https://junyanz.github.io/CycleGAN/)
* [CycleGAN](https://github.com/junyanz/CycleGAN)
* [Understanding and Implementing CycleGAN in TensorFlow](https://hardikbansal.github.io/CycleGANBlog/)
* [Finding connections among images using CycleGAN](http://tv.naver.com/v/2203900)
  * [Finding connections among images using CycleGAN](https://www.slideshare.net/NaverEngineering/finding-connections-among-images-using-cyclegan)
* [초짜 대학원생의 입장에서 이해하는 BEGAN: Boundary Equilibrium Generative Adversarial Networks (1)](http://jaejunyoo.blogspot.com/2017/04/began-boundary-equilibrium-gan-1.html)
* [초짜 대학원생의 입장에서 이해하는 BEGAN: Boundary Equilibrium Generative Adversarial Networks (2)](http://jaejunyoo.blogspot.com/2017/04/began-boundary-equilibrium-gan-2.html)
* [Tensorflow implementation of "BEGAN: Boundary Equilibrium Generative Adversarial Networks"](https://github.com/carpedm20/BEGAN-tensorflow)
* [BEGAN: STATE OF THE ART GENERATION OF FACES WITH GENERATIVE ADVERSARIAL NETWORKS](https://blog.heuritech.com/2017/04/11/began-state-of-the-art-generation-of-faces-with-generative-adversarial-networks/)
* [Generalization and Equilibrium in Generative Adversarial Networks (GANs)](http://www.offconvex.org/2017/03/30/GANs2/)
* [zi2zi: Master Chinese Calligraphy with Conditional Adversarial Networks](https://kaonashi-tyc.github.io/2017/04/06/zi2zi.html)
* [PR12 딥러닝 논문읽기 모임](https://www.youtube.com/playlist?list=PLlMkM4tgfjnJhhd4wn5aj8fVTYJwIpWkS&disable_polymer=true)
* [PR12 intro. to gans jaejun yoo](https://www.slideshare.net/thinkingfactory/pr12-intro-to-gans-jaejun-yoo)
* [Variants of GANs - Jaejun Yoo](https://www.slideshare.net/thinkingfactory/variants-of-gans-jaejun-yoo)
* [아주 간단한 GAN 구현하기](http://blog.naver.com/atelierjpro/220984758512)
* [The GAN Zoo](https://github.com/hindupuravinash/the-gan-zoo)
* [Generative Adversarial Networks (LIVE)](https://www.youtube.com/watch?v=0VPQHbMvGzg)
* [Deep generative model.pdf](https://www.slideshare.net/HyungjooCho2/deep-generative-modelpdf)
* [GANs (Generative Adversarial Networks)](http://nbviewer.jupyter.org/github/metamath1/ml-simple-works/blob/master/GAN/GANs.ipynb)
* [CaloGAN - Simulating 3D High Energy Particle Showers in Multi-Layer Electromagnetic Calorimeters with Generative Adversarial Networks](https://github.com/hep-lbdl/CaloGAN)
* [Generative adversarial networks](https://www.slideshare.net/YunjeyChoi/generative-adversarial-networks-75916964)
  * [1시간만에 GAN(Generative Adversarial Network) 완전 정복하기](https://www.youtube.com/watch?v=odpjk7_tGY0&t=2s)
* [Adversarial Variational Bayes](https://github.com/LMescheder/AdversarialVariationalBayes) GAN + VAE
* [A new kind of deep neural networks](https://medium.com/towards-data-science/a-new-kind-of-deep-neural-networks-749bcde19108)
* [A Generative Model of People in Clothing](http://files.is.tue.mpg.de/classner/gp)
* [PR-001: Generative adversarial nets by Jaejun Yoo (2017/4/13)](https://www.youtube.com/watch?v=L3hz57whyNw)
* [PR-005: Playing Atari with Deep Reinforcement Learning (NIPS 2013 Deep Learning Workshop)](https://www.youtube.com/watch?v=V7_cNTfm2i8&feature=youtu.be)
* [kkweon.github.io/pr12-web-app-elm](https://kkweon.github.io/pr12-web-app-elm/)
* [Generative Adversarial Networks for Beginners Build a neural network that learns to generate handwritten digits](https://www.oreilly.com/learning/generative-adversarial-networks-for-beginners)
  * [Introduction to generative adversarial networks](https://github.com/jonbruner/generative-adversarial-networks)
  * GAN은 알려진 입력 데이터와 비슷한 합성 데이터를 만드는 방법을 학습하는 신경망
  * 예를 들어, 연구원들은 침실에서 앨범 표지에 이르는 모든 사진의 설득력있는 이미지를 생성하고 고차원적 논리를 반영 할 수있는 뛰어난 능력
  * 이러한 예제는 매우 복잡하지만 아주 간단한 이미지를 생성하는 GAN을 만드는 것은 쉬움
  * 이 자습서에서는 손으로 쓴 숫자의 이미지를 분석하고 점진적으로 새로운 이미지를 생성하는 GAN을 생성
  * 본질적으로 신경망을 작성하는 법을 가르칠 것
* [GANGogh: Creating Art with GANs](https://medium.com/towards-data-science/gangogh-creating-art-with-gans-8d087d8f74a1)
* **[(Pytorch를 사용한) 단 50줄로 코드로 짜보는 GAN](http://ddanggle.github.io/GANinTorch)**
* [Meow Generator](https://ajolicoeur.wordpress.com/cats/)
  * DCGAN, WGAN, WGAN-GP, LSGAN 및 ReLU를 일괄 표준 대 SELU와 비교
* [Deep-learning-with-cats](https://github.com/AlexiaJM/Deep-learning-with-cats)
* [Deep Learning에서 "일러스트와 바람 인간 이미지 생성 모델 '을 만든 이야기 (DCGAN, Wasserstein GAN)](http://mickey24.hatenablog.com/entry/irasutoya_deep_learning)
* [art-DCGAN](https://github.com/robbiebarrat/art-DCGAN/blob/master/README.md)
* [Generative Adversarial Networks (GANs)](http://guertl.me/post/162759264070/generative-adversarial-networks)
* [Audio & Video Manipulation](http://notes.michaeldempsey.me/post/159418832409/audio-video-manipulation)
* [A Step-by-Step Guide to Synthesizing Adversarial Examples](http://www.anishathalye.com/2017/07/25/synthesizing-adversarial-examples/)
* [tf-exercise-gan - Tensorflow implementation of different GANs and their comparisions](https://github.com/sanghoon/tf-exercise-gan)
* [속고 속이는 게임 - Minimax Game](http://learnai.tistory.com/1)
* [GAN 스터디 공유자료](http://www.datamarket.kr/xe/index.php?mid=board_LCmL04&document_srl=33070)
* [2017 beginner's review of GAN architectures](https://sigmoidal.io/beginners-review-of-gan-architectures/)
* [MoCoGAN: Decomposing Motion and Content for Video Generation](https://github.com/sergeytulyakov/mocogan)
* [Do you know GAN? 1/2](https://brunch.co.kr/@kakao-it/145)
* [GAN Playground - Explore Generative Adversarial Nets in your Browser](https://reiinakano.github.io/gan-playground/)
* [Fantastic GANs and where to find them](http://guimperarnau.com/blog/2017/03/Fantastic-GANs-and-where-to-find-them)
* [Fantastic GANs and where to find them II](http://guimperarnau.com/blog/2017/11/Fantastic-GANs-and-where-to-find-them-II)
* [Do you know GAN? (2/2)](https://brunch.co.kr/@kakao-it/162)
* [Delving deep into Generative Adversarial Networks (GANs)](https://github.com/GKalliatakis/Delving-deep-into-GANs)
* [PassGAN](https://github.com/brannondorsey/PassGAN)
* [Google open sources TFGAN: Lightweight Library for Generative Adversarial Networks](https://www.techleer.com/articles/437-google-open-sources-tfgan-lightweight-library-for-generative-adversarial-networks/)
* [Consecutive category morphing of GANs generated images (submitted to ICLR 2018)](https://www.youtube.com/watch?v=r6zZPn-6dPY&feature=youtu.be)
* [An Intuitive Introduction to Generative Adversarial Networks](http://blog.kaggle.com/2018/01/18/an-intuitive-introduction-to-generative-adversarial-networks/)
* [The New Neural Internet is Coming - And it looks pretty scary from here](https://hackernoon.com/the-new-neural-internet-is-coming-dda85b876adf)
* [Auto-Regressive Generative Models (PixelRNN, PixelCNN++)](https://towardsdatascience.com/auto-regressive-generative-models-pixelrnn-pixelcnn-32d192911173)
* [1시간만에 GAN(Generative Adversarial Network) 완전 정복하기](http://tv.naver.com/v/1947034)

# MOOC, Lecture
* [Deep Learning in Python](https://www.datacamp.com/courses/deep-learning-in-python)
* [Theories of Deep Learning (STATS 385)](https://stats385.github.io/)
  * [Theories of Deep Learning (STATS 385)](https://stats385.github.io/lecture_slides)
* [Learning From Data - Machine Learning course - recorded at a live broadcast from Caltech](http://work.caltech.edu/telecourse.html)
  * [Course Review: Learning from Data (Introductory Machine Learning course)](https://www.class-central.com/report/review-caltech-learning-from-data-intro-machine-learning/)
* [Every single Machine Learning course on the internet, ranked by your reviews](https://medium.freecodecamp.com/every-single-machine-learning-course-on-the-internet-ranked-by-your-reviews-3c4a7b8026c0)
* [Creative Applications of Deep Learning with TensorFlow](https://www.kadenze.com/programs/creative-applications-of-deep-learning-with-tensorflow)
* **[edwith.org/deeplearningchoi](http://www.edwith.org/deeplearningchoi/)**
* [How to learn Deep Learning in 6 months](https://towardsdatascience.com/how-to-learn-deep-learning-in-6-months-e45e40ef7d48)
* [EE-559 – Deep Learning](https://documents.epfl.ch/users/f/fl/fleuret/www/dlc/)

# Neural Network
* [Google's AI Chief Geoffrey Hinton - How Neural Networks Really Work](https://www.youtube.com/watch?v=l2dVjADTEDU&feature=player_embedded)
* [1. Overview of Mini Batch Gradient Descent](https://www.youtube.com/watch?v=GvHmwBc9N30&feature=share)
* [Learning How To Code Neural Networks](https://medium.com/learning-new-stuff/how-to-learn-neural-networks-758b78f2736e)
  * [뉴럴네트워크 코드 짜는 법 배우기](http://ddanggle.github.io/ml/ai/cs/2016/07/16/LearningHowToCodeNeuralNetworks.html)
* [Machine Learning 스터디 (18) Neural Network Introduction](http://sanghyukchun.github.io/74/)
* [Artificial Neural Networks for Beginners](http://blogs.mathworks.com/loren/2015/08/04/artificial-neural-networks-for-beginners/)
* [A Visual Explanation of the Back Propagation Algorithm for Neural Networks](http://www.kdnuggets.com/2016/06/visual-explanation-backpropagation-algorithm-neural-networks.html)
* [Machine Learning - Neural Networks Tutorial](http://www.existor.com/en/news-neural-networks.html)
* [A Fast and Accurate Dependency Parser using Neural Networks](http://cs.stanford.edu/~danqi/papers/emnlp2014.pdf)
* [waifu2x - Image Super-Resolution for Anime/Fan-Art](https://github.com/nagadomi/waifu2x)
* [Visualizing and Understanding Deep Neural Networks by Matt Zeiler](https://www.youtube.com/watch?v=ghEmQSxT6tw)
* [Machine-Learning Algorithm Mines Rap Lyrics, Then Writes Its Own](http://www.technologyreview.com/view/537716/machine-learning-algorithm-mines-rap-lyrics-then-writes-its-own/)
* [시인 뉴럴](http://pail.unist.ac.kr/carpedm20/poet/)
* [VGG Convolutional Neural Networks Practical](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/index.html)
* [Spectral Representations for Convolutional Neural Networks](http://arxiv.org/pdf/1506.03767.pdf)
* [How to implement a neural network: Part 1](http://peterroelants.github.io/posts/neural_network_implementation_part01/)
* [Inceptionism: Going Deeper into Neural Networks](http://googleresearch.blogspot.kr/2015/06/inceptionism-going-deeper-into-neural.html)
* [Quantifying Creativity in Art Networks](http://arxiv.org/pdf/1506.00711v1.pdf)
* [Neural network의 변천사 이태영](https://www.slideshare.net/secret/dzVcikxOkWg8TP)
* [ai junkie - neural networks in plain english](http://www.ai-junkie.com/ann/evolved/nnt1.html)
* [10 Billion Parameter Neural Networks in your Basement](http://on-demand.gputechconf.com/gtc/2014/presentations/S4694-10-billion-parameter-neural-networks.pdf)
* [Understanding Neural Networks Through Deep Visualization](http://yosinski.com/deepvis)
  * ["Understanding Neural Networks Through Deep Visualization" (2015), J. Yosinski et al.](http://yosinski.com/media/papers/Yosinski__2015__ICML_DL__Understanding_Neural_Networks_Through_Deep_Visualization__.pdf)
  * [github.com/yosinski/deep-visualization-toolbox](https://github.com/yosinski/deep-visualization-toolbox)
* [Interactive Deep Neural Net Hallucinations (+source code) Large Scale Deep Neural Net visualizing top level features](https://317070.github.io/Dream/)
* [Neural Network for Concrete Strength using R](http://andersonjo.github.io/neural-network/2015/07/25/Neural-Network-for-concrete/)
* [fann.js - FANN compiled through Emscripten](https://github.com/louisstow/fann.js/)
* [neurogram - Creating abstract art by evolving neural networks in Javascript](http://blog.otoro.net/2015/07/31/neurogram/)
* [NeuroBind--Yet Another Model for Finding Binding Sites Using Neural Networks](https://github.com/Kyubyong/neurobind)
* [Neural Network for Concrete Strength using R](http://andersonjo.github.io/neural-network/2015/07/25/Neural-Network-for-concrete/)
* [Recent Trends in Neural Net Policy Learning](http://www.slideshare.net/samchoi7/recent-trends-in-neural-net-policy-learning)
* [Hardware Guide: Neural Networks on GPUs](http://pjreddie.com/darknet/hardware-guide/)
* [neural networks by browser](http://neurovis.dataphoric.com/)
* [Scalable Bayesian Optimization Using Deep Neural Networks](http://arxiv.org/abs/1502.05700)
* **[An implementation of the paper 'A Neural Algorithm of Artistic Style'](https://github.com/kaishengtai/neuralart)**
  * [거장의 그림을 30초만에 만들다: DeepStyle](http://redtea.kr/?b=3&n=951)
* [neural-style - Torch implementation of neural style algorithm](https://github.com/jcjohnson/neural-style)
* [Comparing Artificial Artists](https://medium.com/@kcimc/comparing-artificial-artists-7d889428fce4)
* [Neural Networks, Types, and Functional Programming](http://colah.github.io/posts/2015-09-NN-Types-FP/)
* **[Implementing a Neural Network from Scratch – An Introduction](http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/)**
* [Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/)
  * **[해커가 알려주는 뉴럴 네트워크](https://tensorflowkorea.wordpress.com/2016/09/13/%ED%95%B4%EC%BB%A4%EA%B0%80-%EC%95%8C%EB%A0%A4%EC%A3%BC%EB%8A%94-%EB%89%B4%EB%9F%B4-%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC/)**
* [neural-network-papers](https://github.com/robertsdionne/neural-network-papers)
* [Pedestrian detection using convolutional neural networks](http://www.diva-portal.org/smash/get/diva2:839692/FULLTEXT01.pdf)
* [Scalable Distributed DNN Training Using Commodity GPU Cloud Computing](https://drive.google.com/file/d/0B6dKRGPLFSd0UGNOYkNaSC1UZTA/view)
* [Deep Style: Inferring the Unknown to Predict the Future of Fashion](http://multithreaded.stitchfix.com/blog/2015/09/17/deep-style/)
* [DeepHear - Composing and harmonizing music with neural networks](http://web.mit.edu/felixsun/www/neural-music.html)
* [Why are Eight Bits Enough for Deep Neural Networks?](http://petewarden.com/2015/05/23/why-are-eight-bits-enough-for-deep-neural-networks/)
* [Neural Net in C++ Tutorial](https://vimeo.com/19569529)
* [A too naive approach to video compression using artificial neural networks](https://github.com/Dobiasd/articles/blob/master/a_too_naive_approach_to_video_compression_using_artificial_neural_networks.md)
* [An interactive introduction to neural network](http://neurovis.mitchcrowe.com/)
* [Understanding Natural Language with Deep Neural Networks Using Torch](http://devblogs.nvidia.com/parallelforall/understanding-natural-language-deep-neural-networks-using-torch)
* [Skynet for Beginners - Using a Neural Network to Train a Ruby Twitter bot](http://www.fullstackfest.com/agenda/skynet-for-beginners-using-a-neural-network-to-train-a-ruby-twitter-bot)
* [Neural Networks, Manifolds, and Topology](http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/)
  * [그림으로 이해하는 뉴럴 네트워크 학습과정 1편](https://brunch.co.kr/@chris-song/19)
  * [그림으로 이해하는 뉴럴 네트워크 학습과정 2편](https://brunch.co.kr/@chris-song/20)
* [Deep Neural Network with Pre-training](http://enginius.tistory.com/607)
* [Build your own neural network classifier in R](http://junma5.weebly.com/data-blog/build-your-own-neural-network-classifier-in-r)
* [GNU Gneural Network](https://www.gnu.org/software/gneuralnetwork/)
* [Neural Networks Demystified](http://lumiverse.io/series/neural-networks-demystified)
* [colornet](https://techstory.shma.so/colornet-c10ec398cd45)
* [Sketch-simplifying neural network lets artists leap from pencil to ink](http://boingboing.net/2016/04/28/sketch-simplifying-neural-netw.html)
* [Neural Networks Are Impressively Good At Compression](https://probablydance.com/2016/04/30/neural-networks-are-impressively-good-at-compression/)
* [An Analysis of Deep Neural Network Models for Practical Applications](https://arxiv.org/pdf/1605.07678v1.pdf)
* [Why are Eight Bits Enough for Deep Neural Networks?](https://petewarden.com/2015/05/23/why-are-eight-bits-enough-for-deep-neural-networks/)
* [Adventures learning Neural Nets and Python](http://katbailey.github.io/post/neural-nets-in-python/)
* [Deep Learning in Neural Networks: An Overview](http://arxiv.org/abs/1404.7828)
* [Using Neural Networks With Regression](http://deeplearning4j.org/linear-regression.html)
* [How To Interpret R-squared and Goodness-of-Fit in Regression Analysis](https://www.datasciencecentral.com/profiles/blogs/regression-analysis-how-do-i-interpret-r-squared-and-assess-the)
* [Neural networks for algorithmic trading. Part One — Simple time series forecasting](https://medium.com/@alexrachnog/neural-networks-for-algorithmic-trading-part-one-simple-time-series-forecasting-f992daa1045a)
* [Deep Learning using Deep Neural Networks](https://www.linkedin.com/pulse/deep-learning-using-neural-networks-niraj-kumar)
* [K-Fold Cross-Validation for Neural Networks](https://jamesmccaffrey.wordpress.com/2013/10/25/k-fold-cross-validation-for-neural-networks/)
* [What is the Role of the Activation Function in a Neural Network?](http://www.kdnuggets.com/2016/08/role-activation-function-neural-network.html)
  * 신경망에서 activation 함수와 cost(또는 loss, target, objective) 함수는 별개
    * Activation함수, cost함수에 어떤 심오한 (과학적) 의미는 없고, 그냥 인공 신경망을 잘 동작시키기 위해 만든 함수
  * Activation 함수
    * 개별 뉴런에 적용
    * 그 뉴런에 들어온 입력(들)의 합을 출력으로 바꾸는 역할
    * 입력을 그냥 scale 정도 해서 그대로 출력으로 내 보내는 linear 타입, sigmoid 타입, hyperbolic tangent 타입이 존재
  * Cost/loss 함수
    * activation 함수와는 별개로 보통은 신경망 전체에 적용
    * NN이 weight나 bias를 학습할 때 (최적화 할 때) 지표 (metric)이 되는 함수
    * weight, bias를 변수로 갖고 있고, 보통 이 loss/cost를 낮추는 (함수가 에러/cost를 나타낼 때, gradient descent (gradient 반대 방향) 또는 높이는 방향 (함수가 장점/merit을 나탸낼 때, gradient 방향) 으로 만드는 weight(bias)를 계산
    * Sigmoid activation 함수는 초기부터 사용
      * Squared Error 타입 cost 함수와 같이 쓰면 saturation 발생(입력이 매우 negative, 또는 positive 여서 activation이 0 또는 1에 가까울 때), 초기화 잘못으로 학습이 거의 일어나지 않음
    * Cross-entropy loss 함수같은 것은 sigmoid neuron에 대해 써도 이런 saturation 문제 미발생
    * (linear 타입) Relu 를 쓰면, Squared Error 타입 loss 함수를 써도 saturation 미발생
    * [왜 크로스-엔트로피를 쓸까?](https://www.youtube.com/watch?v=srdDQr07sGg&feature=youtu.be)
* [Visualising Activation Functions in Neural Networks](https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/)
* [Sketch Simplification](http://hi.cs.waseda.ac.jp/~esimo/en/research/sketch/)
* [Neural Network-based Sketch Simplification](http://hi.cs.waseda.ac.jp:8081/)
* [논문 요약 - Deep Neural Networks for YouTube Recommendations](http://keunwoochoi.blogspot.com/2016/09/deep-neural-networks-for-youtube.html)
* [Neural Network Architectures](https://culurciello.github.io/tech/2016/06/04/nets.html)
* [THE NEURAL NETWORK ZOO](http://www.asimovinstitute.org/neural-network-zoo/)
  * [번역](https://www.facebook.com/SKTBrain/photos/pcb.306040569766764/306035899767231/?type=3&theater)
* [10 misconceptions about Neural Networks](http://www.turingfinance.com/misconceptions-about-neural-networks/)
* [딥러닝_Neural Network_멀티 퍼셉트론1](http://m.blog.naver.com/dunopiorg/220180453865)
* [인공지능(뉴럴 네트워크) 베토벤 월광소나타 훈련시키기](http://blog.naver.com/atelierjpro/220851418829)
* [Four Experiments in Handwriting with a Neural Network](http://distill.pub/2016/handwriting/)
* [Neural Network Architectures](https://culurciello.github.io/tech/2016/06/04/nets.html)
* [CorrNet - an implementation of Correlational Neural Network (CorrNet)](https://github.com/jskDr/CorrNet)
* [Coding a Deep Neural Network to Steer a Car: Step By Step](https://medium.com/udacity/coding-a-deep-neural-network-to-steer-a-car-step-by-step-c075a12108e2)
* [뉴럴네트워크, 그것이 알고싶다](https://medium.com/@deepvalidation/%EB%89%B4%EB%9F%B4%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC-%EA%B7%B8%EA%B2%83%EC%9D%B4-%EC%95%8C%EA%B3%A0%EC%8B%B6%EB%8B%A4-8de810a97e69)
* [Stuttgart Neural Network Simulator](http://www.ra.cs.uni-tuebingen.de/SNNS/)
* [How to Choose a Neural Network](https://deeplearning4j.org/neuralnetworktable.html)
* [Neural Network 개선](https://www.nextobe.com/single-post/2017/05/11/Neural-Network-%25EA%25B0%259C%25EC%2584%25A0)
* [Using Machine Learning to Explore Neural Network Architecture](https://research.googleblog.com/2017/05/using-machine-learning-to-explore.html)
* [COMMON REPRESENTATION LEARNING USING DEEP CORRNET](https://deeplearn.school.blog/2017/05/24/common-representation-learning-using-deep-corrnet/)
* [A simple neural network module for relational reasoning](https://arxiv.org/pdf/1706.01427.pdf)
  * 스탠포드와 FAIR이 발표한, 구성 언어 및 초등 시각 추론을 위한 진단 데이터 세트(CLEVR)
    * [cs.stanford.edu/people/jcjohns/clevr](http://cs.stanford.edu/people/jcjohns/clevr/)
    * [arxiv.org/pdf/1612.06890.pdf](https://arxiv.org/pdf/1612.06890.pdf)
  * 관련연구 : 시각 추리를 위한 프로그램 추론 및 실행
    * [cs.stanford.edu/people/jcjohns/iep](http://cs.stanford.edu/people/jcjohns/iep/)
    * [arxiv.org/pdf/1705.03633.pdf](https://arxiv.org/pdf/1705.03633.pdf)
    * [github.com/facebookresearch/clevr-iep](https://github.com/facebookresearch/clevr-iep)
  * 순수한 텍스트기반 QnA 데이터세트인 페이스북의 bAbI
    * [research.fb.com/downloads/babi](https://research.fb.com/downloads/babi/)
    * [github.com/facebook/bAbI-tasks](https://github.com/facebook/bAbI-tasks)
  * [keras implementation of [A simple neural network module for relational reasoning](https://arxiv.org/pdf/1706.01427.pdf)](https://github.com/Alan-Lee123/relation-network)
  * [PR-018: A Simple Neural Network Module for Relational Reasoning (DeepMind)](https://www.youtube.com/watch?v=Lb1PVpFp9F8&feature=youtu.be)
  * [Relation Networks for Visual QA](https://tykimos.github.io/Keras/2017/06/10/Relation_Network/)
  * [DeepMind’s Relational Reasoning Networks — Demystified](https://hackernoon.com/deepmind-relational-networks-demystified-b593e408b643)
  * [DeepMind's AI Learns Superhuman Relational Reasoning | Two Minute Papers #168](https://www.youtube.com/watch?v=vzg5Qe0pTKk&feature=youtu.be)
* **[Bridging Relational and Deep Learning](https://people.cs.kuleuven.be/~sebastijan.dumancic/RelationalDeepLearning/index.html)**
* [Learning to Reason with Neural Module Networks](http://bair.berkeley.edu/blog/2017/06/20/learning-to-reason-with-neural-module-networks/)
* [Self-Normalizing Neural Networks](https://gist.github.com/eamartin/d7f1f71e5ce54112fe05e2f2f17ebedf) 자기 정규화 신경망 이해 및 시각화
* [37 Reasons why your Neural Network is not working](https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607)
  * [37 Reasons why your Neural Network is not working 번역](http://daehoon.tistory.com/2)
* [Understanding Neural Network: A beginner’s guide](https://www.datasciencecentral.com/profiles/blogs/understanding-neural-network-a-beginner-s-guide)
* [A Visual and Interactive Guide to the Basics of Neural Networks](http://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)
* [How neural networks are trained](https://ml4a.github.io/ml4a/how_neural_networks_are_trained/)

## ConvNets
* [컨볼루셔널 뉴럴넷 (Convolutional Neural Network)](http://t-robotics.blogspot.com/2016/05/convolutional-neural-network_31.html)
  * [ConvNet을 시계열 데이터에 적용하는 세가지 방법](https://www.facebook.com/terryum/posts/10154337242359417)
    * Convolutional Neural Network (ConvNet, 또는 CNN)은 원래는 2D 이미지를 인식하기 위해 만듦
      * 뛰어난 성능에 다른 영역에서도 점점 CNN을 적용
      * CNN은 기본적으로 shared parameter를 통해 계산량을 줄이는 동시에 overfitting도 완화해주고 더욱 유용한 피쳐를 생성해주는 등 classification에 좋음
    * 이것을 시계열 데이터(time-series data)에 적용하려면 기본적으로 각각의 데이터마다 길이가 다른 문제를 해결해야 함
      * 예를 들어 음성인식을 한다고 하면 각각 단어마다 길이가 다른데, 뉴럴넷은 기본적으로 고정된 사이즈의 벡터를 인풋으로 받는다는 것이 문제
    * 가장 간단한 해결책은 아마 fixed size window를 슬라이딩하면서 적용하는 것
      * 예를 들어 길이가 하나는 1000이고, 하나는 1200이라면 사이즈 100짜리 윈도우로 각각 10개, 12개의 벡터들을 뽑고 각각을 독립된 예제들로 간주
    * 하지만 이건 그렇게 좋은 방법은 아님
      * 왜냐하면 어떤건 앞쪽 부분을 보고, 어떤건 가운데를 보고, 어떤건 뒤쪽을 보는데 이들을 모두 같은 데이터로 학습해야하기 때문
      * 물론 이 데이터 위에 RNN과 같은 것을 쌓을 수도 있겠지만, 암튼 이건 좀 bruteforce
    * 음성인식에선 이것을 HMM을 통해 해결
      * 딥러닝이 나오기 이전, 음성인식은 보통 HMM-GMM (Hidden Markov Model - Gaussian Mixture Model)을 이용해 해결
      * 아주 간단히 말해 연속된 데이터를 몇 개의 Gaussian의 states로 모델링하고 이를 학습
      * 최근의 딥러닝의 도입은 GMM을 딥러닝으로 대체함으로서 GMM-DNN모델을 제시
    * CNN을 음성인식에 적용하는 기본적인 방법은 먼저 HMM-GMM을 통해 대략의 states를 학습한 후 GMM을 CNN으로 대체해 다시 학습
      * 이렇게 하면 기존엔 아주 많은 윈도우를 각각 학습했어야 하는 것과 달리, 이제는 적절한 크기의 states들만 학습하면 됨
    * 자연어처리에선 max pooling over time을 통해 이 문제를 해결
      * 예를 들어 "나는 오늘 아침에 학교에 갔어요"란 문장을 배운다면 Convolution window를 (나는, 오늘), (나는, 오늘, 아침에), (오늘, 아침에) 등등에 적용한 이후 각각의
윈도우로부터 딱 한 개의 값들만을 max pool
      * 이렇게 하면 만약 feature map의 갯수만 같다면 원래 문장의 길이와는 상관없이 동일한 길이의 벡터가 추출
        *  각각의 피쳐맵에서 딱 한 개씩만 값들을 추출하기 때문
      * 이걸 마지막에 기본 뉴럴넷(FFNN)에 넣음으로서 문장 분류와 같은 일을 함
    * 음성인식과 자연어처리가 다른 점
      * 자연어처리(문장 분류)는 이미 문장 단위로 segment 되어있는 상태에서 다른 길이들을 처리
      * 음성인식은 연속적인 데이터에서 임의로 states를 나누는 경우라는 점
    * [Convolutional neural networks for speech recognition (2014)](http://research-srv.microsoft.com/…/…/TASLP2339736-proof.pdf)
    * [Convolutional neural networks for sentence classification (2014)](http://arxiv.org/pdf/1408.5882)
    * [1509.01626 Character-level Convolutional Networks for Text Classification](http://arxiv.org/abs/1509.01626) 자연어를 word 단위로 보는 것이 아니라 character 단위로 보고 마치 한글자 한글자를 웨이브의 한 점처럼 생각
    * Max over time pooling같은 경우 대부분의 sentence classification류의 문제에서 '실용적으로' 잘 동작
      * 굳이 한계점을 꼽자면 feature가 (예로 들어주신 것 처럼, '나는 오늘'과 같은 단어들을 검출할거라고 예상되는) 문장 내에서 나왔는지/없었는지만을 볼 수 있고, 몇 번 나왔는지는 알 수 없다는 단점
      * 따라서, 긴 문장, 혹은 대화/문서까지를 다룬다고 하면 feature extractor로써 적절하지 않을 것
      * 이를 조금 보완한 것이 [dynamic k-max pooling](http://www.aclweb.org/anthology/P14-1062)
    * 시계열을 다룰때는 (음성인식이나, 자연어처리나) RNN이 더 적합하다고 생각
      * 물론 task가 단순하고, 데이터가 적다면 CNN이나 심지어는 전통적인 TF-IDF방법이 더 좋은 경우도 있음
* [My 1st Kaggle ConvNet: Getting to 3rd Percentile in 3 months](http://ilyakava.tumblr.com/post/125230881527/my-1st-kaggle-convnet-getting-to-3rd-percentile)
* [Image Scaling using Deep Convolutional Neural Networks](http://engineering.flipboard.com/2015/05/scaling-convnets/)
* [cs.stanford.edu/people/karpathy/convnetjs](http://cs.stanford.edu/people/karpathy/convnetjs/)
  * [ConvnetJS demo: Image "Painting"](http://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html)
  * [ConvNetJS CIFAR-10 demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)
* [Fast Convolutional Nets With fbfft: A GPU Performance Evaluation](https://research.facebook.com/publications/695244360582147/fast-convolutional-nets-with-fbfft-a-gpu-performance-evaluation/)
* [Learning Game of Life with a Convolutional Neural Network](http://danielrapp.github.io/cnn-gol/)
* [A Tutorial on Deep Learning Part 2: Autoencoders, Convolutional Neural Networks and Recurrent Neural Networks](http://www-cs.stanford.edu/~quocle/tutorial2.pdf)
* [Texture Synthesis with Convolutional Neural Networks](http://bethgelab.org/deeptextures/)
* [Understanding Convolutional Neural Networks for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)
* [Convolutional Neural Network (CNN)](http://enginius.tistory.com/608)
* [CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.stanford.edu/syllabus.html)
  * [Convolutional Neural Networks (CNNs / ConvNets)](http://cs231n.github.io/convolutional-networks/)
  * [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/)
  * [archive.org/download/cs231n-CNNs](https://archive.org/download/cs231n-CNNs)
  * [Andrej Karpathy](https://www.youtube.com/channel/UCPk8m_r6fkUSYmvgCBwq-sw)
  * [CS231n : Neural Networks Part 1: Setting up the Architecture (한국어 번역)](http://ishuca.tistory.com/381)
  * [CS231n Winter 2016 Lecture 4 Backpropagation, Neural Networks 1-Q_UWHTY_TEQ.mp4](https://www.youtube.com/watch?v=GZTvxoSHZIo&feature=youtu.be&t=1h11m38s)
  * [Visualizing what ConvNets learn](http://cs231n.github.io/understanding-cnn/)
  * [CS231n/Module 1: Neural Networks](http://ishuca.tistory.com/category/CS231n/Module%201%3A%20Neural%20Networks)
  * [CONVOLUTIONAL NEURAL NETWORKS FOR VISUAL RECOGNITION](http://online.stanford.edu/course/convolutional-neural-networks-visual-recognition)
  * [DSBA CS231n](https://www.youtube.com/playlist?list=PLetSlH8YjIfXMONyPC1t3uuDlc1Mc5F1A)
  * [cs231n_2017_lecture8.pdf](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture8.pdf)
  * [github.com/Curt-Park/cs231n_assignments](https://github.com/Curt-Park/cs231n_assignments)
  * [Course Project Reports: Spring 2017](http://cs231n.stanford.edu/reports.html)
  * [Lecture Collection | Convolutional Neural Networks for Visual Recognition (Spring 2017)](https://www.youtube.com/playlist?list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv)
  * [github.com/cthorey/CS231](https://github.com/cthorey/CS231) numpy만으로 작성
* [Implementing a CNN for Text Classification in TensorFlow](http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/)
* [Case Study of Convolutional Neural Network](http://www.slideshare.net/nmhkahn/case-study-of-convolutional-neural-network-61556303)
* [Denoising auto encoders(d a)](http://www.slideshare.net/taeyounglee1447/denoising-auto-encodersd-a)
* [Must Know Tips/Tricks in Deep Neural Networks (by Xiu-Shen Wei)](http://lamda.nju.edu.cn/weixs/project/CNNTricks/CNNTricks.html)
* [Kashif Rasul - Intro to ConvNets](https://www.youtube.com/watch?v=W9_SNGymRwo)
* [CNN(Convolution Neural Network)으로 인물을 인식 시켜보자...](https://github.com/jaeho-kang/deep-learning/blob/master/blog/post1/contents.md)
* [VGG Convolutional Neural Networks Practical](http://www.robots.ox.ac.uk/~vgg/practicals/cnn/)
* [Q Learning과 CNN을 이용한 Object Localization](http://www.slideshare.net/ssuser06e0c5/q-learning-cnn-object-localization)
* [Benchmarks for popular CNN models](https://github.com/jcjohnson/cnn-benchmarks)
* [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks/)
  * [번역 A Beginner's Guide To Understanding Convolutional Neural Networks](http://steady7.tistory.com/m/7)
* [A Beginner's Guide To Understanding Convolutional Neural Networks Part 2](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/)
  * [번역 A Beginner's Guide To Understanding Convolutional Neural Networks Part 2](http://steady7.tistory.com/8)
* [The 9 Deep Learning Papers You Need To Know About (Understanding CNNs Part 3)](https://adeshpande3.github.io/adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html)
* [Faster R-CNN in MXNet with distributed implementation and data parallelization](https://github.com/dmlc/mxnet/tree/master/example/rcnn)
* [Faster R-CNN](https://curt-park.github.io/2017-03-17/faster-rcnn/)
* [A guide to convolution arithmetic for deep learning](https://tensorflowkorea.wordpress.com/a-guide-to-convolution-arithmetic-for-deep-learning/)
* [An Intuitive Explanation of Convolutional Neural Networks](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)
* [An Intuitive Explanation of Convolutional Neural Networks](https://www.opendatascience.com/blog/an-intuitive-explanation-of-convolutional-neural-networks)
* [‘구글 맵’ 영상에 AI 접목하니, 빈곤국가 경제실태 한눈에](http://www.dongascience.com/news/view/13461)
* [Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks](https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721)
* [Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning](https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78)
* [NeoCognitron](https://youtu.be/Qil4kmvm2Sw)
  * [1980년 논문: Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position](http://www.cs.princeton.edu/…/co…/Readings/Fukushima1980.pdf)
  * [홈페이지](http://personalpage.flsi.or.jp/fukushima/)
  * [Scholarpedia](http://www.scholarpedia.org/article/Neocognitron)
* [GRAPH CONVOLUTIONAL NETWORKS](http://tkipf.github.io/graph-convolutional-networks/)
* [Convolutional neural network in practice](http://www.slideshare.net/ssuser77ee21/convolutional-neural-network-in-practice)
* [A Beginner's Guide To Understanding Convolutional Neural Networks](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)
* [딥러닝 - 초보자를 위한 컨볼루셔널 네트워크를 이용한 이미지 인식의 이해](http://bcho.tistory.com/1149)
* [딥러닝을 이용한 숫자 이미지 인식 #1/2](http://bcho.tistory.com/1156)
* [딥러닝을 이용한 숫자 이미지 인식 #2/2](http://bcho.tistory.com/1157)
* [Speed/accuracy trade-offs for modern convolutional object detectors](https://arxiv.org/pdf/1611.10012v1.pdf)
  * 구글에서 요즘 나오는 CNN 기반의 object detectors들을 정리
  * Faster R-CNN, R-FCN, SSD 등 디텍션 알고리즘을 다양한 방법으로 실험해 보고 자세히 결과를 리포트
* [CNN VS Preschool Student Eyes](http://loveayase.tumblr.com/post/155708552419/cnn-vs-preschool-student-eyes)
* [DyNet - The Dynamic Neural Network Toolkit](https://github.com/clab/dynet)
* **[CNN 역전파를 이해하는 가장 쉬운 방법 The easist way to understand CNN backpropagation](https://metamath1.github.io/cnn/index.html)**
* [Paints Chainer - line drawing colorizer using chainer. Using CNN, you can colorize your scketch automatically / semi-automatically](https://github.com/taizan/PaintsChainer)
  * [paintschainer.preferred.tech](http://paintschainer.preferred.tech/)
* [Convolutional Neural Networks (CNNs): An Illustrated Explanation](http://xrds.acm.org/blog/2016/06/convolutional-neural-networks-cnns-illustrated-explanation/)
* [CNNs from different viewpoints - Prerequisite: Basic neural networks](https://medium.com/@matthewkleinsmith/cnns-from-different-viewpoints-fab7f52d159c)
* [14 DESIGN PATTERNS TO IMPROVE YOUR CONVOLUTIONAL NEURAL NETWORKS](http://www.topbots.com/14-design-patterns-improve-convolutional-neural-network-cnn-architecture)
* [합성곱 신경망(CNN)](http://astrod.github.io/2017/04/09/%ED%95%A9%EC%84%B1%EA%B3%B1-%EC%8B%A0%EA%B2%BD%EB%A7%9D(CNN).html)
* [#21. Deformable Convolutional Networks](http://t-robotics.blogspot.kr/2017/04/21-deformable-convolutional-networks.html)
* [#P.1. Deformable Convolutional Networks (2017)]https://www.youtube.com/watch?v=RRwaz0fBQ0Y&feature=youtu.be&list=PL0oFI08O71gKEXITQ7OG2SCCXkrtid7Fq)
* [A Brief History of CNNs in Image Segmentation: From R-CNN to Mask R-CNN](https://blog.athelas.com/a-brief-history-of-cnns-in-image-segmentation-from-r-cnn-to-mask-r-cnn-34ea83205de4)
* [Deep Learning #2: Convolutional Neural Networks](https://medium.com/towards-data-science/deep-learning-2-f81ebe632d5c)
* [CNNs in Practice](http://nmhkahn.github.io/CNN-Practice)
* [Conv Nets: A Modular Perspective](http://colah.github.io/posts/2014-07-Conv-Nets-Modular/)
  * [Conv Net: 모듈 방식의 관점](https://brunch.co.kr/@chris-song/23)
* [Picasso: A free open-source visualizer for Convolutional Neural Networks](https://medium.com/merantix/picasso-a-free-open-source-visualizer-for-cnns-d8ed3a35cfc5)
* [간단한 구조의 CNN Layer별 시각화](http://scs.ryerson.ca/~aharley/vis/conv/flat.html)
  * handwritten_digit 이미지를 통해 CNN의 각 Layer별로 시각화
  * Input 값으로 직접 그려 넣기 가능
  * CNN에서 각각의 계산과정 특히 Fully Connected Layer을 이해하는데 많은 도움
  * CNN의 여러 Parameter값들을 변경가능해 이를 시각화해줬으면 더 좋았을 것
  * 시각화된 CNN구조

    ```
    handwritten_digit_Input Image : 32x32
    Filter_size = 5x5, stride = 1 , padding = VALID
    pooling = max_pooling(2x2), stride = 2
    activation_function = Tanh
    input -> conv1(6) -> Tanh -> maxpool1 -> conv2(16) -> Tahn -> maxpool2 -> fc1 -> Tanh -> fc2 -> output
    ```
* [Applied Deep Learning 11/03 Convolutional Neural Networks](https://www.slideshare.net/ckmarkohchang/applied-deep-learning-1103-convolutional-neural-networks)
* [A friendly introduction to Convolutional Neural Networks and Image Recognition](https://www.youtube.com/watch?v=2-Ol7ZB0MmU)
* [컨볼루션(Convolution) 이해하기](https://brunch.co.kr/@chris-song/24)
* [CortexNet: Robust Visual Temporal Representations](https://engineering.purdue.edu/elab/CortexNet/)
* [컨볼루션 신경망 모델을 위한 데이터 부풀리기](https://tykimos.github.io/Keras/2017/06/10/CNN_Data_Augmentation/)
* [Convolution 종류 설명](https://www.slideshare.net/ssuser06e0c5/convolution-77257148)
* [Deepeyeballers - Ushering in deep learning's foray into the netherworld of eyeballing](https://github.com/vinayprabhu/Deepeyeballers)
  * Scatter2Pearson - 산점도에서 상관 계수를 회귀시키는 CNN 훈련
* [Interpreting (and fooling) convolutional neural networks: Part 1](https://www.jebruner.com/2017/07/interpreting-and-fooling-convolutional-neural-networks-part-1/)
* [Dance Dance Convolution](https://github.com/chrisdonahue/ddc)
* [A friendly introduction to Convolutional Neural Networks and Image Recognition](https://www.youtube.com/watch?v=2-Ol7ZB0MmU&t=427s)
* Capsule Networks
  * [Geoffrey Hinton talk "What is wrong with convolutional neural nets ?"](https://www.youtube.com/watch?v=rTawFwUvnLE&feature=youtu.be)
    * [Does the Brain do Inverse Graphics?](http://cseweb.ucsd.edu/~gary/cs200/s12/Hinton.pdf)
  * Understanding Hinton’s Capsule Networks
    * [Part I: Intuition.](https://medium.com/@pechyonkin/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b)
    * [Part II: How Capsules Work](https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66)
    * [Part III: Dynamic Routing Between Capsules](https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-iii-dynamic-routing-between-capsules-349f6d30418)
  * [A Visual Representation of Capsule Network Computations](https://medium.com/@mike_ross/a-visual-representation-of-capsule-network-computations-83767d79e737)
  * [Dynamic Routing Between Capsules - 캡슐 간 동적 라우팅](http://www.python3statement.org/)
  * [Matrix capsules with EM Routing](http://blog.naver.com/sogangori/221136950703)
  * [“Understanding Matrix capsules with EM Routing (Based on Hinton's Capsule Networks)”](https://jhui.github.io/2017/11/14/Matrix-Capsules-with-EM-routing-Capsule-Network/)
  * [Capsule Networks Are Shaking up AI  – Here’s How to Use Them](https://www.kdnuggets.com/2017/11/capsule-networks-shaking-up-ai.html)
  * [Capsule Networks](http://www.cedar.buffalo.edu/~srihari/CSE676/9.12%20CapsuleNets.pdf)
  * [Capsule Networks (CapsNets) – Tutorial](https://www.youtube.com/watch?v=pPN8d0E3900)
  * [How to implement CapsNets using TensorFlow](https://www.youtube.com/watch?v=2Kawrd5szHE)
  * [제프리 힌튼의 캡슐망을 풀이하다](https://brunch.co.kr/@kakao-it/158)
  * [Capsule Networks: An Improvement to Convolutional Networks](https://www.techleer.com/articles/444-capsule-networks-an-improvement-to-convolutional-networks/)
  * [A Nice Easy Tutorial To Follow On Capsule Networks Based On Sabour, Frosst, And Hinton's Paper](https://www.techleer.com/articles/447-a-nice-easy-tutorial-to-follow-on-capsule-networks-based-on-sabour-frosst-and-hintons-paper/)
  * [github.com/Sarasra/models/tree/master/research/capsules](https://github.com/Sarasra/models/tree/master/research/capsules)
  * [Understanding Capsule Networks — AI’s Alluring New Architecture](https://medium.freecodecamp.org/understanding-capsule-networks-ais-alluring-new-architecture-bdb228173ddc)
* [How do CNNs Deal with Position Differences?](https://petewarden.com/2017/10/29/how-do-cnns-deal-with-position-differences/)
* [Slides for paper “the effects of noisy labels on deep convolutional neural networks for music tagging”](https://keunwoochoi.wordpress.com/2017/11/04/slides-for-paper-the-effects-of-noisy-labels-on-deep-convolutional-neural-networks-for-music-tagging/)
* [A tutorial on deep learning for music information retrieval (dl4mir) slides, paper](https://keunwoochoi.wordpress.com/2017/11/02/a-tutorial-on-deep-learning-for-music-information-retrieval-dl4mir-slides-paper/)
* [CNN in numpy](http://chris-chris.ai/2017/11/26/conv-in-numpy/)
* [Convolution operation in Neural Network](http://blog.naver.com/suma_maple/221181408813)
* [컨벌루션 뉴런 네트워크(CNN) 란 무엇인가?](http://www.aitimes.kr/news/articleView.html?idxno=11294)
* [ResNet, AlexNet, VGG, Inception: Understanding various architectures of Convolutional Networks](http://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/)

## LSTM
* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)
* [엘에스티엠 네트워크 이해하기 (Understanding LSTM Networks)](http://roboticist.tistory.com/m/post/571)
* [GRU & LSTM for machine translations.ipynb](https://github.com/kobikun/study/blob/master/babelpish/GRU_LSTM_for_machine_translation/GRU%20%26%20LSTM%20for%20machine%20translations.ipynb)
* [Backpropogating an LSTM: A Numerical Example](http://blog.aidangomez.ca/2016/04/17/Backpropogating-an-LSTM-A-Numerical-Example/)
* [LSTM Networks for Sentiment Analysis](http://deeplearning.net/tutorial/lstm.html)
* [번역 니코니코동화의 공개코멘트 데이터를 Deep Learning로 해석하기](https://blog.umay.be/2016/06/02/niconico-nlp.html)
  * [わかるLSTM ～ 最近の動向と共に](http://qiita.com/t_Signull/items/21b82be280b46f467d1b)
* [Unsupervised Learning of Video Representations using LSTMs](http://nbviewer.jupyter.org/github/babelpish/deep-elastic/blob/master/part2/paper/unsupervised_lstms_video/Unsupervised_Learning_of_Video_Representations_using_LSTMs.ipynb)
* [LSTMVis - Visual Analysis for Recurrent Neural Networks](http://lstm.seas.harvard.edu/)
* [LSTM(RNN) 소개](https://brunch.co.kr/@chris-song/9)
* [Understanding the new Google Translate](https://codesachin.wordpress.com/2017/01/18/understanding-the-new-google-translate/)
* [Knowing when to look : Adaptive Attention via A Visual Sentinel for Image Captioning](http://www.slideshare.net/ssuser06e0c5/knowing-when-to-look-adaptive-attention-via-a-visual-sentinel-for-image-captioning)
  * 본 논문에서는 Hidden layer 뒤에 추가적인 새로운 시각중지 벡터 (visual sentinel vector)를 갖는 LSTM의 확장형을 채택함
  * 시각신호로부터 필요 시 언어모델로 전환이 가능한 Adaptive attention encoder-decoder framework을 제안
  * 이로 인하여 “white”, “bird”, “stop,”과 같은 시각적 단어에 대해서는 좀 더 이미지에 집중하고, “top”, “of”, “on.”의 경우에는 시각중지를 사용함으로서 Image Captioning의 정확도를 향상
  * “Image Captioning”기술은 위성이나 항공영상 분석의 경우 아주 중요한 기술
  * 일반적인 Image Captioning에 비해 예상외로 아주 수월하게 처리가 가능
* [Clickbaits Revisited: Deep Learning on Title + Content Features to Tackle Clickbaits](https://www.linkedin.com/pulse/clickbaits-revisited-deep-learning-title-content-features-thakur)
  * [Clickbaits Revisited](https://github.com/abhishekkrthakur/clickbaits_revisited)
* [Neural Complete](https://github.com/kootenpv/neural_complete)
* [엘에스티엠 네트워크 이해하기](http://whydsp.org/280)
* [Exploring LSTMs](http://blog.echen.me/2017/05/30/exploring-lstms/)
* **[머신러닝 BASIC - RNN과 LSTM에 대해](http://blog.naver.com/anthouse28/221026536458)**
* [딥러닝 기반 기상 예측 모델 연구 사례 (1) : Convolutional LSTM](https://mikigom.github.io/jekyll/update/2017/06/13/deep-learning-forecast-research-1.html)
* [#23. RNN & LSTM](http://t-robotics.blogspot.com/2017/06/23-rnn-lstm.html)
* [RNN : LSTM 구조](https://www.youtube.com/watch?v=3h2tpYWzEa4&t=193s)
* [#6.0. RNN & LSTM](https://www.youtube.com/watch?v=SoNtAjxA3Jo&feature=youtu.be&list=PL0oFI08O71gKEXITQ7OG2SCCXkrtid7Fq)
* [RNN & LSTM](http://astrod.github.io/2017/11/26/26.html)
* [NoisyNet-A3C - NoisyNet (LSTM) asynchronous advantage actor-critic (A3C) on the CartPole-v1 environment](https://github.com/Kaixhin/NoisyNet-A3C)
* [RNN and LSTM 01 - stock_data_load.csv](http://beanexpert.tistory.com/entry/RNN-and-LSTM-01-stockdataloadcsv)
* [Evolution: from vanilla RNN to GRU & LSTMs](https://medium.com/towards-data-science/lecture-evolution-from-vanilla-rnn-to-gru-lstms-58688f1da83a)
* **[Vanilla LSTM with numpy](http://blog.varunajayasiri.com/numpy_lstm.html)**
* [LSTM in numpy - Let's calculate LSTMCell in numpy manually](http://chris-chris.ai/2017/10/10/LSTM-LayerNorm-breakdown-eng/)
* [Image recognition using tensorflow in Deep learning | Python | Ajay Jatav](https://www.youtube.com/watch?v=8c0-42U2Rn0)
* [RNN과 LSTM을 이해해보자!](https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/)
* [왕초보를 위한 RNN](http://nbviewer.jupyter.org/github/bage79/nlp4kor/blob/master/ipynb/RNN_for_beginners.pdf)
* [Understanding GRU networks](https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be)
* [Introduction to Deep Learning Trading in Hedge Funds](https://www.toptal.com/deep-learning/deep-learning-trading-hedge-funds)
* [Stock Price Prediction of Apple Inc. Using Recurrent Neural Network](https://github.com/NourozR/Stock-Price-Prediction-LSTM)

## Python
* [A Neural Network in 11 lines of Python (Part 1)](http://iamtrask.github.io/2015/07/12/basic-python-network/)
  * [11줄의 파이썬 코드로 뉴럴 네트워크를 만들어보자](http://ddanggle.github.io/ml/ai/cs/2016/07/16/11lines.html)
* [A Neural Network in 13 lines of Python (Part 2 - Gradient Descent)](http://iamtrask.github.io/2015/07/27/python-network-part2/)
  * [13줄의 파이썬 코드로 뉴럴 네트워크를 만들어보자. (파트2 - 경사하강법)](http://ddanggle.github.io/ml/ai/cs/2016/09/03/13lines.html)
  * [13Lines.ipynb](https://github.com/DDanggle/blogNetwork/blob/master/13Lines.ipynb)
* [파이썬 코딩으로 말하는 데이터 분석 - 6. 경사하강법](http://hamait.tistory.com/747)
* [Hinton's Dropout in 3 Lines of Python](http://iamtrask.github.io/2015/07/28/dropout/)
* [NeuPy - Neural Networks in Python](http://neupy.com/)
* [Neural Doodle - Use a deep neural network to borrow the skills of real artists and turn your two-bit doodles into masterpieces](https://github.com/alexjc/neural-doodle)
  * [Feed-forward neural doodle](http://dmitryulyanov.github.io/feed-forward-neural-doodle/)
  * [Online neural doodle](https://likemo.net/)
* [Training (deep) Neural Networks Part: 1](http://upul.github.io/2015/10/12/Training-(deep)-Neural-Networks-Part:-1/)
* [Deep learning – Convolutional neural networks and feature extraction with Python](http://blog.christianperone.com/2015/08/convolutional-neural-networks-and-feature-extraction-with-python/)
* [Irene Chen A Beginner's Guide to Deep Learning PyCon 2016](https://www.youtube.com/watch?v=nCPf8zDJ0d0)
* [Introduction to Deep Learning with Python](https://www.youtube.com/watch?v=S75EdAcXHKk&feature=share)
* [A Complete Guide on Getting Started with Deep Learning in Python](https://www.analyticsvidhya.com/blog/2016/08/deep-learning-path/)
* [Python Code Suggestions Using a Long Short-Term Memory RNN](http://blog.algorithmia.com/python-code-suggestions-lstm-rnn)
* [Python for Image Understanding: Deep Learning with Convolutional Neural Nets](http://www.slideshare.net/roelofp/python-for-image-understanding-deep-learning-with-convolutional-neural-nets)
* [neon Fast, scalable, easy-to-use Python based Deep Learning Framework by Nervana™ http://neon.nervanasys.com/](https://github.com/NervanaSystems/neon)
* [Fitting Gaussian Process Models in Python](https://blog.dominodatalab.com/fitting-gaussian-process-models-python/)
  * [Triple Pendulum CHAOS!](http://jakevdp.github.io/blog/2017/03/08/triple-pendulum-chaos/)
* [Gaussian process regression using functional programming](https://github.com/LucaAmbrogioni/Functional-GP-analysis-in-Python/blob/master/Example_Notebook.ipynb)
* [hamait.tistory.com/category/통계&머신러닝&딥러닝](http://hamait.tistory.com/category/%ED%86%B5%EA%B3%84%20%26%20%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D%20%26%20%EB%94%A5%EB%9F%AC%EB%8B%9D)
  * [파이썬 코딩으로 말하는 데이터 분석 - 6. 경사하강법](http://hamait.tistory.com/747)
* [Understanding and coding Neural Networks From Scratch in Python and R](https://www.analyticsvidhya.com/blog/2017/05/neural-network-from-scratch-in-python-and-r/)
* [Safe Crime Prediction - Homomorphic Encryption and Deep Learning for More Effective, Less Intrusive Digital Surveillance](https://iamtrask.github.io/2017/06/05/homomorphic-surveillance)
* [DEEP LEARNING AND REINFORCEMENT LEARNING SUMMER SCHOOL 2017](https://www.facebook.com/nextobe1/photos/a.313464989089503.1073741829.303538826748786/338217953280873/)
* [neat-python](https://pypi.python.org/pypi/neat-python/) A NEAT (NeuroEvolution of Augmenting Topologies) implementation

## Recurrent Flow Net
* [Recurrent Flow Network on Different Error Rates](https://www.youtube.com/watch?v=twR3wYjwLrM)
  * binary matrix를 입력으로 받아서, 미래의 matrix를 예측하며, 각 cell의 속도 역시 구할 수 있음
  * 예를 들어서 matrx의 1과 0이 해당 공간이 점유됨과 비어있음을 의미하면, 이 네트워크는 무인 자동차와 같은 어플리케이션에서 장애물들의 속도와 미래에 어떻게 움직일지를 예측 가능
  * 구조적으론 재귀 신경망 구조이지만, 기존의 RNN과는 모든 연산과 구조가 다릅니다.
  * 장점
    * 노이즈에 강인. 동영상에서 알 수 있듯이 왼쪽의 입력 matrix에 노이즈가 많이 있어도 오른쪽의 예측된 방향은 꽤나 정확
    * 빠른 속도. 200 * 200의 행렬을 받아 처리하는데 40ms 이하(MATLAB coder 환경)
  * [매트랩 코드](https://github.com/sjchoi86/RecurrentFlowNet)
* [Mobile-robot-simulator - Mobile robot simulator in MATLAB](https://github.com/sjchoi86/Mobile-robot-simulator)
  * [Navigation with Occupancy Flow](https://www.youtube.com/watch?v=Hffzo6-4w24&feature=youtu.be)
* [Where to Apply Dropout in Recurrent Neural Networks for Handwriting Recognition?](https://www.slideshare.net/LeeGyeonghoon/where-to-apply-dropout-in-recurrent-neural-networks-for-handwriting-recognition?ref=https%3A%2F%2Fwww.slideshare.net%2FLeeGyeonghoon%2Fslideshelf)
* [A friendly introduction to Recurrent Neural Networks](https://www.youtube.com/watch?v=UNmqTiOnRfg)

## RNN Recurrent Neural Net
* [Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN) Baby steps to your neural network's first memories.](https://iamtrask.github.io/2015/11/15/anyone-can-code-lstm/)
  * [(한글 번역) Anyone Can Learn To Code an LSTM-RNN in Python (Part 1: RNN)](http://jaejunyoo.blogspot.com/2017/06/anyone-can-learn-to-code-LSTM-RNN-Python.html)
* **[Awesome Recurrent Neural Networks - A curated list of resources dedicated to recurrent neural networks](https://github.com/kjw0612/awesome-rnn)**
* **[The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)**
* [opinion mining with deep recurrent nets](http://www.cs.cornell.edu/~oirsoy/drnt.htm)
* [Composing Music With Recurrent Neural Networks](http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/)
* [Composing Music with LSTM Recurrent Networks - Blues Improvisation](http://people.idsia.ch/~juergen/blues/)
* [Training a Recurrent Neural Network to Compose Music](https://maraoz.com/2016/02/02/abc-rnn/)
* [A Recurrent Neural Network Music Generation Tutorial](https://magenta.tensorflow.org/2016/06/10/recurrent-neural-network-generation-tutorial/)
* [How Generative Music Works](https://teropa.info/loop/#/title)
* [Composing Music With Recurrent Neural Networks](http://www.hexahedria.com/2015/08/03/composing-music-with-recurrent-neural-networks/)
* [Recurrent Neural Network, Fractal for Deep Learning](http://www.slideshare.net/uspace/recurrent-neural-network-fractal-for-deep-learning)
* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
* [Multi-layer Recurrent Neural Networks (LSTM, GRU, RNN) for character-level language models in Torch](https://github.com/karpathy/char-rnn)
* [mrchrisjohnson Recurrent Neural Shady](https://soundcloud.com/mrchrisjohnson/recurrent-neural-shady)
* [Recurrent neural network (depth=3) generates next 1,000 bytes of "Let It Go":](http://elnn.snucse.org/sandbox/music-rnn/)
* [recurrent neural network handwriting generation demo](http://www.cs.toronto.edu/~graves/handwriting.cgi?text=Recurrent+neural+nets+are+fucking+magical.&style=&bias=0.5&samples=3)
* [Teaching recurrent Neural Networks about Monet](http://blog.manugarri.com/teaching-recurrent-neural-networks-about-monet/)
* [Recurrent Neural Networks Tutorial, Part 1 – Introduction to RNNs](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
  * [Recurrent Neural Network (RNN) Tutorial - Part 1](http://aikorea.org/blog/rnn-tutorial-1/)
* [Recurrent Neural Networks Tutorial, Part 2 – Implementing a RNN with Python, Numpy and Theano](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)
  * [RNN Tutorial Part 2 - Python, NumPy와 Theano로 RNN 구현하기](http://aikorea.org/blog/rnn-tutorial-2/)
* [Recurrent Neural Networks Tutorial, Part 3 – Backpropagation Through Time and Vanishing Gradients](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)
  * [RNN Tutorial Part 3 - BPTT와 Vanishing Gradient 문제](http://aikorea.org/blog/rnn-tutorial-3/)
* [Vanishing Gradient Problem](https://brunch.co.kr/@chris-song/39)
* [Recurrent Neural Network Tutorial, Part 4 – Implementing a GRU/LSTM RNN with Python and Theano](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)
  * [RNN Tutorial Part 4 - GRU/LSTM RNN 구조를 Python과 Theano를 이용하여 구현하기](http://aikorea.org/blog/rnn-tutorial-4/)
* [Auto-Generating Clickbait With Recurrent Neural Networks](http://larseidnes.com/2015/10/13/auto-generating-clickbait-with-recurrent-neural-networks/)
* [Modeling Molecules with Recurrent Neural Networks](http://csvoss.github.io/projects/2015/10/08/rnns-and-chemistry.html)
* [char_rnn_ari 한글 Character RNN 구현](https://github.com/bluedisk/char_rnn_ari)
* [Introduction to Recurrent Networks in TensorFlow](http://www.kdnuggets.com/2016/05/intro-recurrent-networks-tensorflow.html)
* [Recurrent Flow Network for Occupancy Flow](https://github.com/sjchoi86/RecurrentFlowNet)
* [Large-Scale Item Categorization in e-Commerce Using Multiple Recurrent Neural Networks](http://www.kdd.org/kdd2016/subtopic/view/large-scale-item-categorization-in-e-commerce-using-multiple-recurrent-neur/)
* [RNNS IN TENSORFLOW, A PRACTICAL GUIDE AND UNDOCUMENTED FEATURES](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)
  * [RNNs in Tensorflow, A Practical Guide and Undocumented Features](https://tgjeon.github.io/post/rnns-in-tensorflow/)
* [Recurrent Neural Network tutorial (2nd)](http://www.slideshare.net/uspace/recurrent-neural-network-tutorial-2nd)
* [Attention and Augmented Recurrent Neural Networks](http://distill.pub/2016/augmented-rnns/)
* [Fast Weights RNN](https://tensorflowkorea.wordpress.com/2016/10/25/fast-weights-rnn/)
* [Recurrent Neural Networks for Beginners (Tutorial)](https://medium.com/@awjuliani/recurrent-neural-networks-for-beginners-24288e37ac91)
* [RNN(Recurrent Neural Network)과 Torch로 발라드곡 작사하기](http://www.popit.kr/rnnrecurrent-neural-network%EA%B3%BC-torch%EB%A1%9C-%EB%B0%9C%EB%9D%BC%EB%93%9C%EA%B3%A1-%EC%9E%91%EC%82%AC%ED%95%98%EA%B8%B0/)
* [Recurrent Neural Network Writes Music and Shakespeare Novels | Two Minute Papers](https://www.youtube.com/watch?v=Jkkjy7dVdaY)
* [Rohan & Lenny #3: Recurrent Neural Networks](https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b)
* [A Neural Representation of Sketch Drawings](https://arxiv.org/pdf/1704.03477.pdf)
  * [Sketch RNN](https://github.com/tensorflow/magenta/blob/master/magenta/models/sketch_rnn/sketch_rnn.ipynb)
  * [Teaching Machines to Draw](http://blog.otoro.net/2017/05/19/teaching-machines-to-draw/)
* [Implementing RNN for Spam Prediction](https://drive.google.com/file/d/0Byx2LlqPbfj2cHQ3RmJ2SXI4U3c/view)
* [#P.2. On Human Motion Prediction using RNNs (2017)](http://t-robotics.blogspot.com/2017/06/p2-on-human-motion-prediction-using.html#.WTavhRPyiZ0)
* [RNN-implementation-using-Numpy-binary-digit-addition](https://github.com/jaejun-yoo/RNN-implementation-using-Numpy-binary-digit-addition)
* [RNN 관련 글 모음](http://hamait.tistory.com/849)
* [Tips for Training Recurrent Neural Networks](http://danijar.com/tips-for-training-recurrent-neural-networks/)
* [Decoding the Enigma with Recurrent Neural Networks](https://greydanus.github.io/2017/01/07/enigma-rnn/)
  * [Crypto-RNN: Decoding Polyalphabetic Ciphers with Recurrent Neural Networks](https://github.com/greydanus/crypto-rnn)
* [RNN을 이용한 stock 종가 예측](http://mybeta.tistory.com/27)
* [RNNoise: Learning Noise Suppression](https://people.xiph.org/~jm/demo/rnnoise/)
* [CTRNN - Python package that implements Continuous Time Recurrent Neural Networks (CTRNNs)](https://github.com/madvn/CTRNN)
* [Using Genetic Algorithm for Optimizing Recurrent Neural Networks](https://www.kdnuggets.com/2018/01/genetic-algorithm-optimizing-recurrent-neural-network.html)
* [Only Numpy: NIPS 2017 - Implementing Dilated Recurrent Neural Networks with Interactive Code](https://towardsdatascience.com/only-numpy-nips-2017-implementing-dilated-recurrent-neural-networks-with-interactive-code-e83abe8c9b27)

# Neuroevolution
* [Neuroevolution: A different kind of deep learning](https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning)
* [Find the Right Version of NEAT for Your Needs](http://eplex.cs.ucf.edu/neat_software/)

# Library
* [프로그래밍 언어별 딥러닝 라이브러리 정리](http://aikorea.org/blog/dl-libraries/)
* [어떤 Deep Learning Library를 선택해야하나요?](http://tmmse.xyz/choosing-deep-learning-libraries/)
* [15 Deep Learning Libraries](http://www.datasciencecentral.com/profiles/blogs/here-are-15-libraries-in-various-languages-to-help-implement-your)
* [15 Deep Learning Tutorials](http://www.datasciencecentral.com/profiles/blogs/15-deep-learning-tutorials)
* [50 Deep Learning Software Tools and Platforms, Updated](http://www.kdnuggets.com/2015/12/deep-learning-tools.html)
* [A.I. Duet - A piano that responds to you](https://github.com/googlecreativelab/aiexperiments-ai-duet)
* [Computational Network Toolkit (CNTK)](https://cntk.codeplex.com)
* Caffe
  * [윈도우에서 Caffe 이용하기](https://github.com/jaeho-kang/deep-learning/blob/master/%EC%9C%88%EB%8F%84%EC%9A%B0%EC%97%90%EC%84%9C%20caffe%20%EC%9D%B4%EC%9A%A9%ED%95%98%EA%B8%B0.md)
  * [Setting Caffe on Windows with CUDA & Python](http://m.blog.naver.com/bsh0128/220733003127)
  * [A DSL for deep neural networks, supporting Caffe and Torch http://ajtulloch.github.io/dnngraph](https://github.com/ajtulloch/dnngraph)
  * [Deep Dreams (with Caffe)](https://github.com/google/deepdream/blob/master/dream.ipynb)
  * [Running Google’s Deep Dream on Windows (with or without CUDA) – The Easy Way](http://thirdeyesqueegee.com/deepdream/2015/07/19/running-googles-deep-dream-on-windows-with-or-without-cuda-the-easy-way/)
  * [Deep Learning and Caffe](http://whydsp.org/319)
  * [영상을 이용하기위한 Convolutional Neural Networks, CNN](http://jangjy.tistory.com/181)
  * [Modeling Images, Videos and Text Using the Caffe Deep Learning Library, part 1 (by Kate Saenko)](http://www.slideshare.net/ktoshik/kate-saenko-msr-russia-summer-school-modeling-images-video-text-caffe-dl-part1)
  * [Apply simple pruning on Caffemodel](https://github.com/garion9013/impl-pruning-caffemodel)
  * [Caffe to TensorFlow](https://github.com/ethereon/caffe-tensorflow)
  * [github.com/DeepLearningStudy/caffe/tree/master/examples](https://github.com/DeepLearningStudy/caffe/tree/master/examples)
  * [C++ Example 1. Hello Caffe](http://deeplearningstudy.github.io/doc_caffe_example_1hellocaffe.html)
    * [Caffe C++ API on Windows](http://blog.naver.com/atelierjpro/220835313030)
  * [SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd)
  * [Netscope CNN Analyzer - A web-based tool for visualizing and analyzing convolutional neural network ](https://dgschwend.github.io/netscope/quickstart.html)
    * CNN Model 분석을 도와줌
    * Model을 prototxt 형태로 넣어주면, 네트워크 구조와, 하단에 CNN Dimension, parameter 수 등의 세부 정보를 정리
  * [caffe-boo - My own caffe-windows with additional layers and features](https://github.com/seokhoonboo/caffe-boo)
  * [Caffe2 - A New Lightweight, Modular, and Scalable Deep Learning Framework](http://caffe2.ai/)
  * [Classifying ImageNet: using the C++ API](https://github.com/BVLC/caffe/tree/master/examples/cpp_classification)
  * [C++ Example 1. Hello Caffe](http://deeplearningstudy.github.io/doc_caffe_example_1hellocaffe.html)
  * [Deep learning tutorial on Caffe technology : basic commands, Python and C++ code](http://christopher5106.github.io/deep/learning/2015/09/04/Deep-learning-tutorial-on-Caffe-Technology.html)
  * [A Practical Introduction to Deep Learning with Caffe and Python](http://adilmoujahid.com/posts/2016/06/introduction-deep-learning-python-caffe/)
  * [Caffe 와 Python을 사용하여 딥러닝으로 개와 고양이 구분하기 1](http://kyubot.tistory.com/96)
  * [Caffe 와 Python을 사용하여 딥러닝으로 개와 고양이 구분하기 2](http://kyubot.tistory.com/97)
* Cloudera
  * [Deep Learning Frameworks on CDH and Cloudera Data Science Workbench](http://blog.cloudera.com/blog/2017/04/deep-learning-frameworks-on-cdh-and-cloudera-data-science-workbench/)
    * CDH & Cloudera Data Science Workbench 기반의 딥러닝 프레임워크 소개
* CoreML
  * [coreml-scikit-example - Apple CoreML example with scikit-learn](https://github.com/mfpierre/coreml-scikit-example)
  * [pypi.python.org/pypi/coremltools](https://pypi.python.org/pypi/coremltools)
  * [Keras Deep Learning with Apple’s CoreMLTools on iOS 11 – Part 1](https://amundtveit.com/2017/06/07/keras-deep-learning-with-apples-coremltools-on-ios-11-part-1/)
  * [WWDC 애플 코어 머신러닝 (Core ML) 발표 요약 1 ("Introduction to CoreML)](https://www.facebook.com/groups/TensorFlowKR/permalink/482610418746688/)
  * [CoreML and Vision-Create a basic example](https://www.youtube.com/watch?v=mVQB4YEkOKM)
  * [How I Shipped a Neural Network on iOS with CoreML, PyTorch, and React Native](https://attardi.org/pytorch-and-coreml/)
* [Deep Learning Model Convertors](https://github.com/ysh329/deep-learning-model-convertor)
* [deeplearn.js - a hardware-accelerated machine intelligence library for the web](https://pair-code.github.io/deeplearnjs/)
* [deepart.io](http://www.deepart.io/) - Generate images styled like your favorite artist
* [DeepOSM - Classify roads and features in satellite imagery, by training neural networks with OpenStreetMap (OSM) data](https://github.com/trailbehind/DeepOSM)
* [DL4J Deep Learning for Java](http://deeplearning4j.org/)
  * [DL4J Java자바를 위한 딥 러닝](http://deeplearning4j.org/kr-index.html)
  * [인공 신경망 및 심층 신경망 소개](http://deeplearning4j.org/kr-neuralnet-overview.html)
  * [A Beginner’s Guide to Recurrent Networks and LSTMs](http://deeplearning4j.org/lstm.html)
  * [Using Neural Networks With Regression](http://deeplearning4j.org/linear-regression.html)
  * [RBM with DL4J for Deep Learning](http://www.slideshare.net/uspace/rbm-with-dl4j-for-deep-learning-50955012)
  * [NN Models with DL4J for Deep Learning](http://www.slideshare.net/uspace/nn-models-with-dl4j-for-deep-learning)
  * [A Beginner’s Guide to Eigenvectors, PCA, Covariance and Entropy](http://deeplearning4j.org/eigenvector)
  * [“딥러닝, 게을러지려고 연구하죠”...아담 깁슨 DL4J 창시자](https://www.imaso.co.kr/news/article_view.php?article_idx=20150824223056)
  * [Exploring convolutional neural networks with DL4J](http://brooksandrew.github.io/simpleblog/articles/convolutional-neural-network-training-with-dl4j/)
  * [Deep Learning Using DL4J and Spark on HDP for Fun and Profit](https://www.youtube.com/watch?v=XCX0GsswDfM)
  * [rl4j - Reinforcement Learning for the JVM](https://github.com/deeplearning4j/rl4j)
  * [MLPClassifierLinear](https://www.youtube.com/watch?v=BN_g2t0ykxg) This is a screencast that shows building a Linear Classifier using a Neural Network
  * [Introduction to Deep Neural Networks](https://deeplearning4j.org/neuralnet-overview)
  * [Open Data for Deep Learning](https://deeplearning4j.org/opendata)
  * [제가 번역한 딥러닝 문서 목록 (deeplerarning4j.org)](http://keunwoochoi.blogspot.com/2016/03/deeplerarning4jorg.html)
  * [Getting started with Deeplearning4J and Scala](http://datasmarts.net/2017/11/11/getting-started-with-deeplearning4j-and-scala/)
* [Eesen - The official repository of the Eesen project](https://github.com/srvk/eesen)
* [Fabrik – Collaboratively build, visualize, and design neural nets in the browser http://fabrik.cloudcv.org](https://github.com/Cloud-CV/Fabrik)
* [gemmlowp: a small self-contained low-precision GEMM library](https://github.com/google/gemmlowp)
* [Gradient Boosting Interactive Playground](http://arogozhnikov.github.io/2016/07/05/gradient_boosting_playground.html)
* [Labellio: Scalable Cloud Architecture for Efficient Multi-GPU Deep Learning](http://devblogs.nvidia.com/parallelforall/labellio-scalable-cloud-architecture-efficient-multi-gpu-deep-learning/)
* Lasagne
  * [Lasagne-CTC](https://github.com/skaae/Lasagne-CTC)
* [Mind - Flexible neural networks in JavaScript](http://www.mindjs.net/)
* [Mindori - On-demand GPUs for neural networks](http://mindori.com/)
* [mxnet - Flexible and Efficient Library for Deep Learning](http://mxnet.io/)
  * **[An interactive book on deep learning. Much easy, so MXNet. Wow. http://gluon.mxnet.io](https://github.com/zackchase/mxnet-the-straight-dope)**
  * [mxnet - Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Scala, Go, Javascript and more http://mxnet.rtfd.org](https://github.com/dmlc/mxnet)
  * [MXNet - Deep Learning Framework of Choice at AWS](http://www.allthingsdistributed.com/2016/11/mxnet-default-framework-deep-learning-aws.html)
  * [Alex Smola at AI Frontiers: Scalable Deep Learning Using MXNet](https://www.slideshare.net/AIFrontiers/scalable-deep-learning-using-mxnet)
  * **[MXNet을 활용한 이미지 분류 앱 개발하기](http://www.popit.kr/mxnet%EC%9D%84-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%9D%B4%EB%AF%B8%EC%A7%80-%EB%B6%84%EB%A5%98-%EC%95%B1-%EA%B0%9C%EB%B0%9C%ED%95%98%EA%B8%B0/)**
  * [github.com/JONGGON/Mxnet_Tutorial](https://github.com/JONGGON/Mxnet_Tutorial)
  * [Apache MXNet에 대한 모든 것!](http://channy.creation.net/blog/1155)
  * [MXNet for Deep Learning](https://github.com/apache/incubator-mxnet)
  * [MXNet 기반 추천 오픈 소스 딥러닝 프로젝트 모음](http://blog.creation.net/apache-mxnet-deep-learning-project#.WYJws9Pygcw)
  * [MXNet 시작하기 (1) – NDArrays API](http://blog.creation.net/mxnet-part-1-ndarrays-api#.WYKHYtPygcw)
  * [CVPR 2017 Tutorial https://mli.github.io/cvpr17/](https://github.com/mli/cvpr17)
  * [클라우드에 딱 맞는 MXNet의 5가지 딥러닝 학습 기능](http://blog.creation.net/mxnet-deep-learning-features-aws-cloud)
  * [dl-twitch-series](https://github.com/sunilmallya/dl-twitch-series/blob/master/README.md)
  * [Apache MXNet으로 배워보는 딥러닝(Deep Learning) - 김무현 (AWS 솔루션즈아키텍트)](https://www.youtube.com/watch?v=H66GDuLsGl4)
  * [MXNet 시작하기 (1) – NDArrays API](http://blog.creation.net/mxnet-part-1-ndarrays-api)
  * [deepSpeech.mxnet: Rich Speech Example](https://github.com/apache/incubator-mxnet/tree/master/example/speech_recognition)
    * [deepSpeech.mxnet: Rich Speech Example](https://github.com/samsungsds-rnd/deepspeech.mxnet)
  * [CapsNet-MXNet](https://github.com/samsungsds-rnd/capsnet.mxnet)
  * [An introduction to the MXNet API — part 1](https://becominghuman.ai/an-introduction-to-the-mxnet-api-part-1-848febdcf8ab)
  * [전이학습(transfer learning)으로 모형 재사용하기 (Gluon 기반)](http://freesearch.pe.kr/archives/4701)
  * [Gluon을 이용한 Grad CAM](http://freesearch.pe.kr/archives/4695)
  * [seq2seq기반 덧셈 모형 빌드(with Gluon)](http://freesearch.pe.kr/archives/4710)
  * [MXNet 혹은 Gluon으로 모형을 개발할때 반드시 맞닥뜨릴 한가지 이슈](http://freesearch.pe.kr/archives/4737)
  * [Deep learning with Apache MXNet on Cloudera Data Science Workbench](https://blog.cloudera.com/blog/2017/10/deep-learning-with-apache-mxnet-on-cloudera-data-science-workbench/)
* [neural enhance - Super Resolution for images using deep learning](https://github.com/alexjc/neural-enhance)
* [Polyaxon - An enterprise-grade open source platform for building, training, and monitoring large scale deep learning applications](https://polyaxon.com/)
  * [Polyaxon: An open source Deep Learning / Machine Learning stack on Kubernetes](https://www.techleer.com/articles/485-polyaxon-an-open-source-deep-learning-machine-learning-stack-on-kubernetes/)
* [PyCNN - Image Processing in Cellular Neural Networks with Python](http://blog.ankitaggarwal.me/PyCNN/)
  * [Sequence To Sequence Attention Models In PyCNN](https://talbaumel.github.io/attention)
* [pylearn2-practice](https://github.com/zygmuntz/pylearn2-practice)
* [SINGA is a general distributed deep learning platform for training big deep learning models over large datasets](http://singa.apache.org/docs/overview.html)
* Sonnet
  * [DeepMind의 Neural Network를 위한 라이브러리 SONNET](https://www.nextobe.com/single-post/2017/05/11/DeepMind-SONNET)
* [VELES - Distributed platform for rapid Deep learning application development](https://velesnet.ml/)
* [webdnn - Fastest DNN Execution Framework on Web Browser https://mil-tokyo.github.io/webdnn](https://github.com/mil-tokyo/webdnn)
* [xcessiv - A web-based application for quick and scalable construction of massive machine learning ensembles](https://github.com/reiinakano/xcessiv)
* [YOLO DARKNET - 구성 및 설치, 사용방법](http://www.popit.kr/yolo-darknet-%EA%B5%AC%EC%84%B1-%EB%B0%8F-%EC%84%A4%EC%B9%98-%EC%82%AC%EC%9A%A9%EB%B0%A9%EB%B2%95/)
  * [github.com/AlexeyAB/darknet](https://github.com/AlexeyAB/darknet/)

# Medical
* [MOLIERE: Automatic Biomedical Hypothesis Generation System](https://www.youtube.com/watch?v=wA6OCix-4FU&t=7s)
* [Applying deep learning to medical data](https://www.slideshare.net/hyunseokmin/applying-deep-learning-to-medical-data)
* [의료 ai를 위해 세상에 없는 양질의 data 만드는 도구 제작하기](https://www.slideshare.net/deview/213-ai-data)
* [Cardiac MRI Segmentation](https://chuckyee.github.io/cardiac-segmentation/)

# Microsoft
* The Microsoft Cognitive Toolkit 마이크로소프트에서 개발한 딥러닝 프레임워크 CNTK
  * [blog](https://blogs.microsoft.com/next/2016/10/25/microsoft-releases-beta-microsoft-cognitive-toolkit-deep-learning-advances)
  * [website](https://www.microsoft.com/en-us/research/product/cognitive-toolkit/)
  * [github.com/Microsoft/CNTK](https://github.com/Microsoft/CNTK)
  * CNTK v1.x; 속도는 빠르지만 C++, C# API만 지원하고 실서비스 배포가 불편한 문제
  * CNTK v2.0; Python API 지원, 최적화된 분산 학습 가능
  * [1-bit SGD, Model sharing 등 최적화된 대용량 처리에 초점을 맞춰서 개발](https://www.microsoft.com/en-us/research/product/cognitive-toolkit/features/)
  * [Image, Speech, Text 분야의 다양한 학습 모델](https://www.microsoft.com/en-us/research/product/cognitive-toolkit/model-gallery/)
  * [학습된 모델을 Azure로 배포, 서비스 가능](https://github.com/Microsoft/CNTK/wiki/Evaluate-a-model-in-an-Azure-WebApi)
  * [Benchmarking CNTK on Keras: is it Better at Deep Learning than TensorFlow?](http://minimaxir.com/2017/06/keras-cntk/)
  * [Microsoft Congnitive Toolkit 알아보기!](http://digitalbourgeois.tistory.com/37)
* [Building Deep Neural Networks in the Cloud with Azure GPU VMs, MXNet and Microsoft R Server](https://blogs.technet.microsoft.com/machinelearning/2016/09/15/building-deep-neural-networks-in-the-cloud-with-azure-gpu-vms-mxnet-and-microsoft-r-server/)
* [AirSim - Open source simulator based on Unreal Engine for autonomous vehicles from Microsoft AI & Research](https://github.com/Microsoft/AirSim)

# Mooc
* [Deep Learning Courses](http://machinelearningmastery.com/deep-learning-courses/)
* [6.S094: Deep Learning for Self-Driving Cars](http://selfdrivingcars.mit.edu/)
  * [MIT 6.S094: Introduction to Deep Learning and Self-Driving Cars](https://www.youtube.com/watch?v=1L0TKZQcUtA&feature=youtu.be&list=PLrAXtmErZgOeiKm4sgNOknGvNjby9efdf)
* [DeepDriving: Learning Affordance for Direct Perception in Autonomous Driving](http://deepdriving.cs.princeton.edu/?platform=hootsuite)
* [How To Get The Best Deep Learning Education For Free](https://www.linkedin.com/pulse/how-get-best-deep-learning-education-forfree-mariya-yao)
* [Dive into Deep Learning with 15 free online courses](https://medium.freecodecamp.com/dive-into-deep-learning-with-these-23-online-courses-bf247d289cc0)
* [Podcast: The world needs AI researchers. Here’s how to become one](https://80000hours.org/2017/07/podcast-the-world-needs-ai-researchers-heres-how-to-become-one/)
* [CS 598 LAZ: Cutting-Edge Trends in Deep Learning and Recognition](http://slazebni.cs.illinois.edu/spring17/)
* **[Practical Deep Learning For Coders, Part 1](http://course.fast.ai/)**
* **[Practical Deep Learning For Coders, Part 2](http://course.fast.ai/part2.html)**
  * [Cutting Edge Deep Learning for Coders—Launching Deep Learning Part 2](http://www.fast.ai/2017/07/28/deep-learning-part-two-launch/)
* [github.com/shahariarrabby/deeplearning.ai](https://github.com/shahariarrabby/deeplearning.ai)

# Paper
* [Two Minute Papers](https://www.youtube.com/channel/UCbfYPyITQ-7l4upoX8nvctg)
* [Awesome - Most Cited Deep Learning Papers](https://github.com/terryum/awesome-deep-learning-papers)
* [Awesome Deep Learning: Most Cited Deep Learning Papers](http://www.kdnuggets.com/2017/04/awesome-deep-learning-most-cited-papers.html)
* [Deep Learning Papers Reading Roadmap](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap)
* [Five Hundred Deep Learning Papers, Graphviz and Python](http://dnlcrl.github.io/projects/2015/10/10/500-deep-learning-papers-graphviz-python.html?imm_mid=0dd0f3&cmp=em-data-na-na-newsltr_20151202)
* [deeplearning-papernotes](https://github.com/nolsigan/deeplearning-papernotes)
* [the morning paper](https://blog.acolyer.org/)
* [Summaries and notes on Deep Learning research papers](https://github.com/dennybritz/deeplearning-papernotes)
* [openreview.net](https://openreview.net/)
* [9 Key Deep Learning Papers, Explained](http://www.kdnuggets.com/2016/09/9-key-deep-learning-papers-explained.html/3)
  * 이름 / 이해 난이도 / 읽기 수월함 / 필수성 / 선행지식
  * AlexNet (2012) 하 / 쉬움 / 필수 / 콘볼루션 오퍼레이션 지식, 이미지넷 챌린지
  * ZF Net (2013) 하 / 쉬움 / 옵션 (Segmentation, Localization을 하겠다고 하면 필수) / AlexNet
  * VGG Net (2014) 하 / 쉬움 / 옵션 / AlexNet
  * GoogLeNet (2015) 상 / 어려움 / 옵션 / AlexNet, Hebb 법칙
  * Microsoft ResNet (2015) 중 / 쉬움 / 필수와 옵션의 중간 / AlexNet, VGG Net, NiN(Network in Network)
  * Region Based CNNs (R-CNN - 2013, Fast R-CNN - 2015, Faster R-CNN - 2015)
    * 하 (부분적 상) / 중간 / 옵션 (Segmentation, Localization을 하겠다고 하면 필수. Fast R-CNN을 중심으로 보는게 좋음) / PASCAL 챌린지
  * Generative Adversarial Networks (2014) ? / ? / 필수 / VGG Net
  * Generating Image Descriptions (2014) 상 / 쉬움 / 중간 (이미지 to 문장을 하겠다고 하면 필수) / LSTM, 캡셔닝 챌린지
  * Spatial Transformer Networks (2015) 중 / 어려움 / 옵션 (아직 불명) / 공간변환
* [Deep Learning Paper Implementations: Spatial Transformer Networks - Part I](https://kevinzakka.github.io/2017/01/10/stn-part1/)
* [Arxiv Sanity Preserver](http://www.arxiv-sanity.com/)
* [Computer Vision and Pattern Recognition (cs.CV)](https://scirate.com/arxiv/cs.CV) arXiv에 올라온, CV/PR 주제 논문의 초록만 모아 보여줌
* MNIST 숫자 인식기 Gaussian Bayesian 확률 모델로 구현
  * 목표
    * MNIST 데이터 특성 시각적으로 이해하기
    * Python, numpy, matplotlib 사용해 보기
    * Bayesian Theorem 이해하고 구현해 보기
    * Multivariate Gaussian Distribution 이해하고 구현해 보기
  * 실험 데이터
    * 학습 데이터: MNIST 기본 60,000개
    * 테스트 데이터: MNIST 기본 10,000개
  * 실험 결과
    * Bayesian 확률 모델만으로 분류 정확도가 대략 84% 정도 나오는 것을 확인
    * Multivariate Gaussian 적용하니까 분류 정확도가 대략 92% 정도까지 올라가는 것을 확인
  * 코드
    * [메인 프로그램](https://github.com/dgtgrade/HumanLearning/blob/master/2001a.py)
      * numpy, matplotlib 외에 본격 머신러닝 라이브러리는 전혀 사용하지 않았음
      * 머신러닝 관련 부분 대략 200줄 이하로 매우 짧음
      * 시각화 관련 코드 및 코멘트 등이 대략 300줄 정도임
    * [MNIST 데이터 파일](https://github.com/dgtgrade/HumanLearning/tree/master/data) MNIST 공식 홈페이지에서 받은 그대로
    * [MNIST 데이터 로딩 프로그램](https://github.com/dgtgrade/HumanLearning/blob/master/mnist2ndarray.py)
    * [Multivariate Gaussian 적용하지 않고 Bayesian 확률 모형만으로 돌아가는 코드: 위 2001a.py 옛날 버전](https://github.com/dgtgrade/HumanLearning/blob/8e57a2b3340da3b38956b83cf24433d3a9fbd11b/2001a.py)
  * 실험 동영상
    * 학습: 실험데이터 전체 60000개를 학습하는 과정을 보여줌
    * 테스트: 테스트 데이터 전체 10000개를 테스트 하는 과정을 보여줌
    * 테스트 과정에서 정답률은 1번 후보만으로 구했으나, 표시는 3번후보까지 하였음
* [A Review on a Deep Learning that Reveals the Importance of Big Data](https://fananymi.wordpress.com/)
* [Decoupled Neural Interfaces using Synthetic Gradients](https://arxiv.org/pdf/1608.05343.pdf) synthetic gradient - 뉴럴넷 업데이트 과정의 모듈간 강결합을 decouple
* [Deep Learning without Backpropagation Tutorial: DeepMind's Synthetic Gradients](https://iamtrask.github.io/2017/03/21/synthetic-gradients/)
* [Deep Learning Papers Reading Roadmap](https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap)
* [Deep Learning Research Review Week 1: Generative Adversarial Nets](https://adeshpande3.github.io/adeshpande3.github.io/Deep-Learning-Research-Review-Week-1-Generative-Adversarial-Nets)
* ["Distributed Training of Deep Neuronal Networks: Theoretical and Practical Limits of Parallel Scalability](http://arxiv.org/abs/1609.06870v1)
  * 여러 노드를 썼을 때, 네트웍 벤드위쓰와 전체 노드에서의 계산을 기다리면 어떻게 되는지
  * 싱글 노드에서 할 때 batch 사이즈를 달리하면 어느 layer 계산이 bottleneck인지
  * 이런 문제를 방지하기 위해 디자인을 바꿀 때 어디부터 보면 되는지
  * 계산량을 어떻게 계산하는지
* [Highway and Residual Networks learn Unrolled Iterative Estimation](https://arxiv.org/abs/1612.07771)
  * VGGNet, GoogLeNet, ResNet 등과 같은 매우 많은 계층을 가진 Deep Net 들이 뛰어난 이유를
  * 기존의 각각의 계층이 특정한 추상적인 feature를 대표하며 이를 계층적으로 계산하기 때문이라는 "representation view" 를 뒤집고
  * 각각의 블록 또는 단계마다 단계적인 feature의 변화가 반복적으로 일어난다는 "unrolled iterative estimation" 으로 설명
* [Solving Verbal Comprehension Questions in IQ Test by Knowledge-Powered Word Embedding](http://arxiv.org/pdf/1505.07909v1.pdf)
* [Stacked Approximated Regression Machine: A Simple Deep Learning Approach](https://arxiv.org/pdf/1608.04062v1.pdf)
  * SARM이라는 layer wise training 기법
  * Back propagation 없이 layer 단위로 학습을 시켜도 현재 state of the art DNN과 비슷하거나 더 나은 성능을 보인다는 주장
  * PCANet에 non linearity를 추가
* [The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition ECCV 2016](https://arxiv.org/pdf/1511.06789v3.pdf)
  * Fine-Grained Recognition을 할 때 Noisy Fine-Grained Data, 즉 Web에서 검색한 Noisy하지만 큰 데이터가 도움이 된다는 내용
  * Noisy Fine-Grained Data 구축
    * 새의 종을 구별하는 데이터베이스를 구축한다면, Wikipedia에서 종을 검색하여 그 키워드를 기반으로 구글링하여 이미지 구축
    * 여러 카테고리에 동시에 등장하는 그림을 지우는 등의 간단한 정제작업을 추가
    * 여전히 이 데이터베이스는 롱테일 문제도 있고 에러도 존재
  * 실험 결과, 퀄리티가 좋지만 작은 데이터보다 성능이 좋다
  * 큰 Noisy Fine-Grained Data로 학습한 후 좋은 데이터로 튜닝하면 더 좋다
  * [Factors in Finetuning Deep Model for Object Detection with Long-tail Distribution](http://www.ee.cuhk.edu.hk/~wlouy…/…/OuyangFactors_CVPR16.pdf)
    * Long-tail Distribution을 가진 DB의 문제점 지적
  * [Training Region-based Object Detectors with Online Hard Example Mining](https://arxiv.org/abs/1604.03540,
CVPR2016)
    * easy examples과 hard examples의 너무 큰 차이에 대해서 문제를 지적
  * Fine-Grained Recognition이라는 테스크에 한정된 실험, 분석, 수학적인 설명 부족한 논문
* [Learning to Remember Rare Events](http://www.gitxiv.com/posts/vnbEtdiH3qZEeKBGb/learning-to-remember-rare-events)
* [Best Practices for Applying Deep Learning to Novel Applications](https://github.com/TeamLab/seminar/blob/master/paper/2017/1704_01568.md)
* [Top 20 Recent Research Papers on Machine Learning and Deep Learning](http://www.kdnuggets.com/2017/04/top-20-papers-machine-learning.html)
* [Deep Learning Papers by task](https://github.com/sbrugman/deep-learning-papers)
* [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515) overfitting 방지
  * [SELUs (scaled exponential linear units) - Visualized and Histogramed Comparisons among ReLU and Leaky ReLU](https://github.com/shaohua0116/Activation-Visualization-Histogram)
  * [SelfNormalizingNetworks](https://github.com/bioinf-jku/SNNs)
  * [SELU_Keras_Tutorial](https://github.com/bigsnarfdude/SELU_Keras_Tutorial)
* [One Model To Learn Them All](http://xxx.lanl.gov/pdf/1706.05137v1)
  * [One Model To Learn Them All](http://xxx.lanl.gov/abs/1706.05137)
  * 이미지,음성, 텍스트 처리를 위한 하나의 모델
  * 딥러닝은 음성 인식, 이미지 분류에서부터 번역에 이르기까지 많은 분야에서 훌륭한 결과를 제공
  * 그러나 각 문제마다 깊이있는 모델을 잘 작동 시키려면 아키텍처 연구와 장기간의 튜닝이 필요
  * 여러 도메인에 걸쳐있는 여러 가지 문제에 대해 좋은 결과를 얻을 수있는 단일 모델을 제시
  * 특히 ImageNet, 다중 번역 작업, 이미지 캡션 (COCO 데이터 세트), 음성 인식 코퍼스 및 영어 구문 분석 작업에서 이
단일 모델을 동시에 학습
  * 우리의 모델 아키텍처는 여러 도메인의 빌딩 블록을 통합
  * convolutional layer, attention mechanism, sparsely-gated layer가 포함
  * 흥미롭게도 블록이 작업에 중요하지 않더라도 이 계산 블록 각각은 우리가 훈련하는 작업의 하위 집합에 결정적인 역할
  * 그것이 성능에 해를 입히지 않으며, 대부분의 경우 모든 작업에서 성능을 향상 시킨다는 것을 관찰
  * 또한 데이터가 적은 작업은 다른 작업과의 공동 교육을 통해 큰 효과를 얻는 반면 큰 작업의 성능은 전혀 저하되지 않음
* [초짜 대학원생 입장에서 이해하는 [CVPR 2017] Learning by Association - A versatile semi-supervised training method for neural networks](http://jaejunyoo.blogspot.com/2017/07/learning-by-association-versatile-semi-supervised-training.html)
* [Tutorial on Theory and Application of Generative Adversarial Networks](https://github.com/mingyuliutw/cvpr2017_gan_tutorial)
* [Domain Adaptation for Visual Applications: A Comprehensive Survey](https://arxiv.org/abs/1702.05374)
  * Transfer Learning 의 일종인 Domain Adaptation 에 대한 방대한 리뷰
* [TRANSFER LEARNING FOR SOUND CLASSIFICATION](http://tatalab.ca/2017/07/17/transfer-learning-for-sound-classification/)
* [Inception v3를 활용한 Transfer Learning](http://jsideas.net/python/2017/09/12/Inception_v3_transfer_learning.html)
* [Transfer Learning using differential learning rates](https://medium.com/@manikantayadunanda/transfer-learning-using-differential-learning-rates-638455797f00)
* [ACL 2017에서 Google 발표 논문](https://www.facebook.com/nextobe1/posts/355787631523905)
* [Layer Normalization](https://tensorflow.blog/2016/07/)
* [blog.lunit.io](https://blog.lunit.io/)
* [인공지능(AI) 관련 최신 공개된 주요 논문 발표 요약](http://www.aitimes.kr/news/articleView.html?idxno=10966)
* [MY FAVORITE DEEP LEARNING PAPERS OF 2017](http://cachestocaches.com/2017/12/favorite-deep-learning-2017/)

# Quickprop
* [Quickprop: an almost forgotten neural training algorithm](https://www.bonaccorso.eu/2017/09/15/quickprop-an-almost-forgotten-neural-training-algorithm/)

# Reinforcement Learning, RL
* [5 Ways to Get Started with Reinforcement Learning](https://buzzrobot.com/5-ways-to-get-started-with-reinforcement-learning-b96d1989c575)
* [Fundamental of Reinforcement Learning](https://dnddnjs.gitbooks.io/rl/content/)
* [Ujava.org reinforcement-learning](http://www.slideshare.net/uspace/ujavaorg-reinforcementlearning)
* [ujava.org Reinforcement Learning (2nd)](http://www.slideshare.net/uspace/ujavaorg-reinforcement-learning-2nd)
* [ujava.org workshop : Reinforcement Learning with Thompson Sampling](http://www.slideshare.net/uspace/ujavaorg-workshop-reinforcement-learning-with-thompson-sampling)
* [Reinforcement Learning and DQN, learning to play from pixels](https://rubenfiszel.github.io/posts/rl4j/2016-09-08-DQN-Learning-to-play-from-pixels-step-by-step.html)
* [DQN 3.0](https://github.com/deepmind/dqn)
* [github.com/LeeGyeongTak/DQN](https://github.com/LeeGyeongTak/DQN)
* [Frame Skipping and Pre-Processing for Deep Q-Networks on Atari 2600 Games](https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/)
* [github.com/LeeGyeongTak/Q_Network](https://github.com/LeeGyeongTak/Q_Network)
* [Guest Post (Part I): Demystifying Deep Reinforcement Learning](http://www.nervanasys.com/demystifying-deep-reinforcement-learning/)
  * [딥 강화학습 쉽게 이해하기](http://ddanggle.github.io/ml/ai/cs/2016/09/24/demystifyingDL.html)
* [Deep Learning in a Nutshell: Reinforcement Learning](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/)
* [Bayesian Programming and Learning for Multi-Player Video Games Application to RTS AI](http://emotion.inrialpes.fr/people/synnaeve/phdthesis/phdthesis.html)
  * [Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks](http://arxiv.org/abs/1609.02993)
  * 스타크래프트 같은 실시간 전략 (RTS) 게임은 체스나 바둑과는 다르게 제한된 자원(미네랄, 가스 등)과 불확실한 정보 (보이지 않는 상대방의 플레이 등) 속에서 의사결정을 해야하는 어려움이 존재
  * 이 논문에서는 “참/거짓”으로 표현되는 boolean logic이 아닌 베이지언 모델링으로 이런 정보의 불확실함(uncertainty)를 처리
* [한국어 Multiagent Bidirectional- Coordinated Nets for Learning to Play StarCraft Combat Games](https://www.slideshare.net/KihoSuh/multiagent-bidirectional-coordinated-nets-for-learning-to-play-starcraft-combat-games)
* [스타크래프트2 강화학습(StarCraft II Reinforcement Learning)](https://www.slideshare.net/sjhshy/2-starcraft-ii-reinforcement-learning-80779324)
* [gym-starcraft](https://github.com/alibaba/gym-starcraft/blob/master/README.md)
* [StarCraft II RL Tutorial 1 - Deepmind's StarCraft II RL Environment](http://chris-chris.ai/2017/08/30/pysc2-tutorial1/)
* [StarCraft II RL Tutorial 2 - Deepmind's pysc2: Observations](http://chris-chris.ai/2017/11/06/pysc2-tutorial2/)
* [SSCAIT Student StarCraft AI Tournament & Ladder](https://sscaitournament.com/)
* [Deep Learning in a Nutshell: Reinforcement Learning](https://devblogs.nvidia.com/parallelforall/deep-learning-nutshell-reinforcement-learning/)
* [강화 학습 기초 Reinforcement Learning an introduction](http://www.slideshare.net/carpedm20/reinforcement-learning-an-introduction-64037079)
* [async-rl-tensorflow - Asynchronous Methods for Deep Reinforcement Learning](https://github.com/devsisters/async-rl-tensorflow)
* [LEARNING REINFORCEMENT LEARNING (WITH CODE, EXERCISES AND SOLUTIONS)](http://www.wildml.com/2016/10/learning-reinforcement-learning/)
* [Reinforcement Learning 101 (in 15 minutes)](https://www.facebook.com/SKTBrain/posts/311444575893030)
* [멀티 암드 밴딧(Multi-Armed Bandits)](https://brunch.co.kr/@chris-song/62)
* [Bandit 101](https://www.facebook.com/SKTBrain/posts/313678162336338) Multi-Armed Bandit (MAB) 입문자료
* [톰슨 샘플링 for Bandits](https://brunch.co.kr/@chris-song/66)
* [Nathan Epstein - Reinforcement Learning in Python](https://www.youtube.com/watch?v=rTMa04TZ_MY)
* [Lecture 10 Reinforcement Learning I](https://www.youtube.com/watch?v=IXuHxkpO5E8)
* [Reinforcement learning with unsupervised auxiliary tasks](https://deepmind.com/bSandbox/reinforcement-learning-unsupervised-auxiliary-tasks/)
Sandbox [Designing Neural Network Architectures using Reinforcement Learning" (Under review as a conference paper at ICLR 2017)](https://arxiv.org/abs/1611.02167)
* [Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)
* [Simple Reinforcement Learning in Tensorflow: Part 1 - Two-armed Bandit](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149)
* [Simple Reinforcement Learning with Tensorflow Part 1.5: Contextual Bandits](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c)
* [Simple Reinforcement Learning with Tensorflow: Part 2 - Policy-based Agents](https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724)
* [Simple Reinforcement Learning with Tensorflow: Part 3 - Model-Based RL](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99)
* [Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df)
* [Simple Reinforcement Learning with Tensorflow Part 5: Visualizing an Agent’s Thoughts and Actions](https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a)
* [Simple Reinforcement Learning with Tensorflow Part 6: Partial Observability and Deep Recurrent Q-Networks](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc)
* [Simple Reinforcement Learning with Tensorflow Part 7: Action-Selection Strategies for Exploration](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf)
* [Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2)
* [Bridging Cognitive Science and Reinforcement Learning Part 1: Enactivism](https://medium.com/@awjuliani/bridging-cognitive-science-and-reinforcement-learning-part-1-enactivism-601af34ef122)
* [Deep Reinforcement Learning - author: David Silver, Department of Computer Science, University College London](http://videolectures.net/rldm2015_silver_reinforcement_learning/)
  * [Tutorial: Deep Reinforcement Learning - David Silver, Google DeepMind](http://icml.cc/2016/tutorials/deep_rl_tutorial.pdf)
* [Quantum Boltzman Machines for Deep Reinforcement Learning](https://theinformationageblog.wordpress.com/2017/01/20/quantum-boltzman-machines-for-deep-reinforcement-learning/)
* [모두를 위한 머신러닝/딥러닝 강의](http://hunkim.github.io/ml/)
  * [모두를위한RL강좌](https://www.youtube.com/playlist?list=PLlMkM4tgfjnKsCWav-Z2F-MMFRx-2gMGG)
  * [모두를 위한 딥러닝 - Deep Reinforcement Learning](https://www.inflearn.com/course/reinforcement-learning/)
  * [kimhun_rl_windows - I follow the lecture (https://hunkim.github.io/ml/) on windows version](https://github.com/nalsil/kimhun_rl_windows)
* [Reinforcement learning](https://www.youtube.com/playlist?list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT)
* [Tutorial: Introduction to Reinforcement Learning with Function Approximation](https://www.youtube.com/watch?v=ggqnxyjaKe4)
  * [Introduction to Reinforcement Learning with Function Approximation](http://media.nips.cc/Conferences/2015/tutorialslides/SuttonIntroRL-nips-2015-tutorial.pdf)
* [DeepHack.RL](http://rl.deephack.me/)
  * [DeepHack.RL (2017)](https://www.youtube.com/playlist?list=PLt1IfGj6-_-efXDATIw4JI92DjMrVed3P)
* [Building a deep learning DOOM bot](https://www.codelitt.com/blog/doom-ai/)
  * [ViZDoom is a Doom-based AI research platform for reinforcement learning from raw visual information](http://vizdoom.cs.put.edu.pl/)
    * [Tutorial](http://vizdoom.cs.put.edu.pl/tutorial)
  * [ViZDoom - Doom-based AI Research Platform for Reinforcement Learning from Raw Visual Information)](https://github.com/mwydmuch/ViZDoom)
  * [Windows에서 VizDoom 설치하기](http://ishuca.tistory.com/401)
* [ishuca.tistory.com/tag/강화학습](http://ishuca.tistory.com/tag/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5)
* [Deep Reinforcement Learning Pieter abbeel](http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/?q=Pieter+abbeel)
* [irelia - Korean Janggi AI using q learning](https://github.com/jireh-father/irelia)
* [Deep Reinforcement Learning: An Overview](https://arxiv.org/abs/1701.07274)
* [강화학습 튜토리알 - 인공 신경망으로 '퐁' 게임을 학습시키자 (Andrej Karpathy 포스트 번역)](http://keunwoochoi.blogspot.com/2016/06/andrej-karpathy.html)
* [Playing Atari with Deep Reinforcement Learning](https://speakerdeck.com/jacksongl/playing-atari-with-deep-reinforcement-learning)
* [Solved atari games](https://entropicai.blogspot.com/)
* [Minimal and Clean Reinforcement Learning Examples](https://github.com/rlcode/reinforcement-learning)
* [한국어 Safe Multi-Agent Reinforcement Learning for Autonomous Driving](https://www.slideshare.net/KihoSuh/safe-multiagent-reinforcement-learning-for-autonomouse-driving)
* [Reinforcement  Learning – Policy](http://web.stanford.edu/class/cs234/slides/cs234_guest_lecture_policy_gradients.pdf)
* [UCL Course on RL](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)
* [Jessica Forde An Introduction to Reinforcement Learning PyCon 2017](https://www.youtube.com/watch?v=k1UuTyW2uFc)
* [PLE: A Reinforcement Learning Environment](http://pygame-learning-environment.readthedocs.io)
* [async_deep_reinforce - Asynchronous deep reinforcement learning](https://github.com/miyosuda/async_deep_reinforce)
* [카카오AI리포트 프리뷰 무적 알파고를 만든 비결은?](https://brunch.co.kr/@kakao-it/68)
* [NEURAL ARCHITECTURE SEARCH WITH REINFORCEMENT LEARNING](https://openreview.net/pdf?id=r1Ue8Hcxg)
  * [한국어 Neural Architecture Search with Reinforcement Learning](https://www.slideshare.net/KihoSuh/neural-architecture-search-with-reinforcement-learning-76883153)
  * [review](https://www.facebook.com/groups/modulabs/permalink/1361230430608803)
* [IGC 엔씨소프트 이경종 - 강화 학습을 이용한 NPC AI 구현](https://www.slideshare.net/ssuser052dd11/igc-npc-ai)
* [카카오AI리포트 알파고를 탄생시킨 강화학습의 비밀](https://brunch.co.kr/@kakao-it/73)
* [Tic-Tac-Toe-Machine-Leaning-Using-Reinforcement-Learning](https://github.com/jamesq9/Tic-Tac-Toe-Machine-Learning-Using-Reinforcement-Learning)
* [김정주: 파이썬으로 나만의 강화학습 환경 만들기](https://www.youtube.com/watch?v=chVLag1NIAQ)
  * [파이썬으로 나만의 강화학습 만들기](https://www.slideshare.net/ssuser163469/ss-78685946)
* [알아두면 쓸데있는 신기한 강화학습 NAVER 2017](https://www.slideshare.net/carpedm20/naver-2017)
  * [알아두면 쓸데있는 신기한 강화학습](https://www.youtube.com/watch?v=NGGO0zdzhVQ&feature=youtu.be)
* [스타2 강화학습 튜토리얼 - 1편](https://brunch.co.kr/@chris-song/44)
* [스타2 강화학습 튜토리얼 - 2편](https://brunch.co.kr/@chris-song/48)
* [Reinforcement learning environments with musculoskeletal models http://opensim.stanford.edu/](https://github.com/stanfordnmbl/osim-rl)
* [Jessica Forde An Introduction to Reinforcement Learning PyCon 2017](https://www.youtube.com/watch?v=k1Uos.stat('.')
* [DeepRLHacks](https://github.com/williamFalcon/DeepRLHacks/blob/master/README.md) 고 깊게 이해하기](https://www.youtube.com/w
* [RLCode와 A3C 쉽고 깊게 이해하기](https://www.slideshare.net/WoongwonLee/rlcode-a3c)
* [5 Ways to Get Started Reinforcement Learning](https://buzzrobot.com/5-ways-to-get-started-with-reinforcement-learning-b96d1989c57HA with 5)
* [Deep RL Bootcamp](https://self.google.com/view/deep-rl-bootcamp/lectures)
* [rllab - a framework for developing and evaluating reinforcement learning algorithms](https://github.com/rll/rllab)
* [강화학습 퍼마리오 part 1selfhttps://brunch.co.kr/@kakao-it/144)
* [강화학습으로 풀어보는 슈퍼마리오 part 2.](https://brunch.co.kr/@kakao-it/161)
* [Reinforce-2 Dynamic Programming, Policy Iteration, Value Iteration](https://drive.google.com/file/d/0B9Eqem6No7GvOG5iSVN1MzdndEU/view)
* [Neural Architecture Search with Reinforcement Learning](https://www.slideshare.net/KihoSuh/neural-architecture-search-with-reinforcement-learning-76883153) 한국어
* [Evolving Stable Strategies](http://blog.otoro.net/2017/11/12/evolving-stable-strategies/)
  * [ESTool](https://github.com/hardmaru/estool/blob/master/README.md)
* [Doing Deep Reinforcement learning with PPO](https://www.slideshare.net/ssuser517c25/doing-deep-reinforcement-learning-with-ppo-82483715)
* [The pycolab game engine](https://github.com/deepmind/pycolab)
* [Neural Map - Structured Memory for Deep RL](http://www.cs.cmu.edu/~rsalakhu/NIPS2017_StructureMemoryForDeepRL.pdf)
* [확률적 바람이 부는 격자 세계로 익히는 강화학습](https://blog.naver.com/kwonpub/221167111861)
* [강화학습 좀 더 이해해보자](https://blog.naver.com/kwonpub/221167878734)
* [Thinking out loud: hierarchical and interpretable multi-task reinforcement learning](https://einstein.ai/research/hierarchical-reinforcement-learning) hierarchical policy network
* [Learning to Compose Skills](https://himanshusahni.github.io/2017/12/26/reusability-in-ai.html)
* [논문 요약 - Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural networks for reinforcement learning](http://keunwoochoi.blogspot.com/2017/12/deep-neuroevolution-genetic-algorithms.html)
* [MM framework for RL](https://www.slideshare.net/sungyubkim75/mm-framework-for-rl)
* [Guided policy search](https://www.slideshare.net/jaehyeonBahk/guided-policy-search)
* [Practical_RL: Reinforcement learning for seq2seq (pytorch, tensorflow, theano)](https://www.techleer.com/articles/460-practical_rl-reinforcement-learning-for-seq2seq-pytorch-tensorflow-theano/)
* [Intuitive RL: Intro to Advantage-Actor-Critic (A2C)](https://medium.com/@rudygilman/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752)
* [Psychlab](https://github.com/deepmind/lab/tree/master/game_scripts/levels/contributed/psychlab)
  * [Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents](https://arxiv.org/abs/1801.08116)
  * [Open-sourcing Psychlab](https://deepmind.com/blog/open-sourcing-psychlab/)
  * [Psychlab 'visual search' task in DeepMind Lab](https://www.youtube.com/watch?v=54AS3a6niPo&feature=youtu.be)
* [Learning to Optimize with Reinforcement Learning](http://bair.berkeley.edu/blog/2017/09/12/learning-to-optimize-with-rl/)
* [가깝고도 먼 Trpo](https://www.slideshare.net/WoongwonLee/trpo-87165690)
* [Introduction to Learning to Trade with Reinforcement Learning](http://www.wildml.com/2018/02/introduction-to-learning-to-trade-with-reinforcement-learning/)
* [Deep Reinforcement Learning Doesn't Work Yet](https://www.alexirpan.com/2018/02/14/rl-hard.html)
* [Inverse Reinforcement Learning pt. I](https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html)
* [Introduction to Various Reinforcement Learning Algorithms. Part I (Q-Learning, SARSA, DQN, DDPG)](https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287)
* [강화학습과 가상화폐](https://wonseokjung.github.io//crypto_rl/update/cryptoandRL_1/)
* [DQN을 알아보자 - Playing Atari with Deep Reinforcement Learning](https://wonseokjung.github.io//rl_paper/update/RL-PP-DQN/)

## RL MOOC, Lectures
* [CS294: Deep RL Start!](https://tensorflow.blog/2017/01/23/cs294-deep-rl-start/)
  * [CS294-112 Deep Reinforcement Learning Sp17](https://www.youtube.com/playlist?list=PLkFD6_40KJIwTmSbCv9OVJB3YaO4sFwkX)
* [Berkeley CS 294: Deep Reinforcement Learning, Spring 2017](http://rll.berkeley.edu/deeprlcourse/)
  * [CS 294: Deep Reinforcement Learning —(1) Introduction and course overview](https://medium.com/@peteryun/rl-cs-294-deep-reinforcement-learning-introduction-and-course-overview-c7d9cb550ced)
* [MIT 6.S094: Deep Learning for Self-Driving Cars (Lecture 2), 2017](http://selfdrivingcars.mit.edu/)
* [UCL, David Silver, Reinforcement Learning, 2015](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html)
* [Stanford Andrew Ng CS229 Lecture 16, 2008](https://www.youtube.com/watch?v=RtxI449ZjSc)
* [Deep Reinforcement Learning](http://videolectures.net/deeplearning2016_abbeel_deep_reinforcement/)
* [Implementation of Reinforcement Learning Algorithms. Python, OpenAI Gym, Tensorflow. Exercises and Solutions to accompany Sutton's Book and David Silver's course](https://github.com/dennybritz/reinforcement-learning)
* [Reinforcement Learning: An Introduction](https://webdocs.cs.ualberta.ca/~sutton/book/the-book-2nd.html)
* [Deep Reinforcement Learning and Control](https://katefvision.github.io/)
* Temporal-Difference Learning
	* [1](https://wonseokjung.github.io/reinforcementlearning/update/RL-TD1/)
  * [2](https://wonseokjung.github.io/reinforcementlearning/update/RL-TD2/)

# Semantic Sementation
* [A 2017 Guide to Semantic Segmentation with Deep Learning](http://blog.qure.ai/notes/semantic-segmentation-deep-learning-review)
* [LinkNet - Feature Forwarding: Exploiting Encoder Representations for Efficient Semantic Segmentation](https://codeac29.github.io/projects/linknet/index.html)
* [Semantic Segmentation using Fully Convolutional Networks over the years](https://meetshah1995.github.io/semantic-segmentation/deep-learning/pytorch/visdom/2017/06/01/semantic-segmentation-over-the-years.html)

# Spark
* [DeepSpark: Spark-Based Deep Learning Supporting Asynchronous Updates and Caffe Compatibility](http://hgpu.org/?p=15511)
* [yahoo/CaffeOnSpark](https://github.com/yahoo/CaffeOnSpark)
* [SparkNet: Training Deep Networks in Spark](http://arxiv.org/abs/1511.06051)
* [spark-summit.org/2016/schedule](https://spark-summit.org/2016/schedule/)
  * Large-Scale Deep Learning with TensorFlow (Jeff Dean)
    * [slide](http://www.slideshare.net/JenAman/large-scale-deep-learning-with-tensorflow)
    * [video](https://youtu.be/XYwIDn00PAo)
  * [AI: The New Electricity (Andrew Ng)](https://youtu.be/4eJhcxfYR4I)
  * Large Scale Multimedia Data Intelligence And Analysis On Spark (Baidu)
    * [slide](http://www.slideshare.net/JenAman/large-scale-multimedia-data-intelligence-and-analysis-on-spark)
    * [video](https://youtu.be/LrtdyCWphvs)
  * Scaling Machine Learning To Billions Of Parameters (Yahoo)
    * [slide](http://www.slideshare.net/JenAman/scaling-machine-learning-to-billions-of-parameters)
    * [video](https://youtu.be/l_1S7W_l2cI)
  * CaffeOnSpark: Deep Learning On Spark Cluster (Yahoo)
    * [slide](http://www.slideshare.net/JenAman/caffeonspark-deep-learning-on-spark-cluster)
    * [video](https://youtu.be/Mn7QEdUFSnQ)
  * Scalable Deep Learning in Baidu
    * [slide](http://www.slideshare.net/JenAman/scalable-deep-learning-platform-on-spark-in-baidu)
    * [video](https://youtu.be/n9yZNmC20pc)

# VAE
* [What is variational autoencoder?](http://nolsigan.com/blog/what-is-variational-autoencoder/) VAE
* [On manifolds and autoencoders](http://videolectures.net/deeplearning2015_vincent_autoencoders/?q=vincent%20autoencoder)
* [KL divergence와 VAE](http://blog.naver.com/atelierjpro/220981354861)
* [1. 오토인코더(Sparse Autoencoder) 1 – AutoEncoders & Sparsity](http://solarisailab.com/archives/113?ckattempt=1)
* [A Gentle Autoencoder Tutorial (with keras)](http://www.birving.com/presentations/autoencoders/index.html#/)
* [Variational Auto-Encoder for MNIST](https://github.com/hwalsuklee/tensorflow-mnist-VAE)
* [초짜 대학원생의 입장에서 이해하는 Auto-Encoding Variational Bayes (VAE) (1)](http://jaejunyoo.blogspot.com/2017/04/auto-encoding-variational-bayes-vae-1.html)
* [초짜 대학원생의 입장에서 이해하는 Auto-Encoding Variational Bayes (VAE) (2)](http://jaejunyoo.blogspot.com/2017/04/auto-encoding-variational-bayes-vae-2.html)
* [초짜 대학원생의 입장에서 이해하는 Auto-Encoding Variational Bayes (VAE) (3)](http://jaejunyoo.blogspot.com/2017/05/auto-encoding-variational-bayes-vae-3.html)
* [NDC2017 딥러닝으로 게임 콘텐츠 제작하기 - VAE를 이용한 콘텐츠 생성 기법 연구 사례](https://www.slideshare.net/HwanheeKim2/ndc2017-vae-75419284)
* [Variational Coin Toss](http://www.openias.org/variational-coin-toss)
* [Variational Autoencoder를 여러 가지 각도에서 이해하기 (Understanding Variational Autoencoder from Various Perspectives)](https://www.slideshare.net/haezoom/variational-autoencoder-understanding-variational-autoencoder-from-various-perspectives)
